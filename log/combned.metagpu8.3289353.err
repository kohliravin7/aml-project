/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/rkohli/aml-project/src/train_search.py:138: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), args['grad_clip'])
Traceback (most recent call last):
  File "main.py", line 298, in <module>
    # darts(exp_name, darts_args)
  File "/home/rkohli/aml-project/src/train_search.py", line 101, in darts
    utils.save_genotype(genotype, os.path.join(args['save'], 'genotype.json'))
  File "/home/rkohli/aml-project/src/utils.py", line 124, in save_genotype
    json.dumps(genotype._asdict(), write)
TypeError: dumps() takes 1 positional argument but 2 were given
