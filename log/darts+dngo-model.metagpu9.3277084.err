INFO:solver.bo_hb:Evaluate: [1.63165364e+01 1.01117284e-01 6.17002909e+00 8.44182613e-02
 3.42684647e-01 5.12115129e+00 2.76532707e+00 1.10043109e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 in 0.000636 seconds
INFO:solver.bo_hb:Evaluate: [2.34717370e+01 3.60494032e-01 4.97391459e+00 4.68649725e-02
 8.70531088e-01 5.24568228e+00 2.56564033e+00 9.22773200e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 in 0.000541 seconds
INFO:solver.bo_hb:Evaluate: [2.71887734e+01 1.65679211e-01 5.19101413e+00 3.65475064e-02
 1.37857891e+00 5.56816589e+00 1.22946726e+00 8.59388811e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.06627168429902346, 'grad_clip_value': 8, 'initial_lr': 1.5231319634833736e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adad', 'weight_decay': 1.0039654734954814e-05}
INFO:root:param size = 0.255930MB
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
INFO:root:epoch 0 lr 1.377686e-06
/home/rkohli/aml-project/src/train.py:131: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), grad_clip)
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351185e+00 7.322304 48.897059
INFO:root:train 100 2.352851e+00 7.193688 47.617574
INFO:root:train 150 2.353499e+00 6.943295 47.299255
INFO:root:train 200 2.355030e+00 6.864117 46.944963
INFO:root:train 250 2.353526e+00 6.940986 47.292082
INFO:root:train 300 2.352452e+00 7.013081 47.606935
INFO:root:train 350 2.351747e+00 7.015670 47.845442
INFO:root:train 400 2.351232e+00 7.021509 47.973815
INFO:root:train 450 2.350841e+00 7.074557 48.108370
INFO:root:train 500 2.350800e+00 7.060878 48.156811
INFO:root:train 550 2.350480e+00 7.029832 48.077359
INFO:root:train 600 2.350160e+00 7.061148 48.055324
INFO:root:train 650 2.350663e+00 7.025250 48.043875
INFO:root:train 700 2.350626e+00 6.992243 48.036287
INFO:root:train_acc 7.029167
INFO:root:valid 000 2.387055e+00 4.687500 35.937500
INFO:root:valid 050 2.350655e+00 7.475490 47.303922
INFO:root:valid 100 2.352217e+00 7.209158 47.648515
INFO:root:valid 150 2.349538e+00 7.367550 48.209851
INFO:root:valid_acc 7.283333
INFO:root:epoch 1 lr 9.969028e-07
INFO:root:train 000 2.336573e+00 7.812500 50.000000
INFO:root:train 050 2.348708e+00 7.536765 48.958333
INFO:root:train 100 2.348202e+00 7.348391 49.226485
INFO:root:train 150 2.350597e+00 7.243377 48.313328
INFO:root:train 200 2.351039e+00 7.268346 48.173197
INFO:root:train 250 2.350816e+00 7.326942 48.194721
INFO:root:train 300 2.350443e+00 7.257060 48.297342
INFO:root:train 350 2.350122e+00 7.305021 48.303953
INFO:root:train 400 2.350185e+00 7.302057 48.343984
INFO:root:train 450 2.350656e+00 7.292822 48.375139
INFO:root:train 500 2.350802e+00 7.241766 48.387600
INFO:root:train 550 2.351368e+00 7.188634 48.318398
INFO:root:train 600 2.351407e+00 7.165141 48.182716
INFO:root:train 650 2.351247e+00 7.171659 48.247888
INFO:root:train 700 2.351266e+00 7.177247 48.310449
INFO:root:train_acc 7.131250
INFO:root:valid 000 2.360027e+00 4.687500 40.625000
INFO:root:valid 050 2.337688e+00 7.843137 48.682598
INFO:root:valid 100 2.341932e+00 7.425743 47.988861
INFO:root:valid 150 2.341308e+00 7.264073 47.930464
INFO:root:valid_acc 7.150000
INFO:root:epoch 2 lr 5.262292e-07
INFO:root:train 000 2.338935e+00 7.812500 46.875000
INFO:root:train 050 2.357207e+00 7.383578 47.426471
INFO:root:train 100 2.355262e+00 7.271040 48.004332
INFO:root:train 150 2.352969e+00 7.450331 48.592715
INFO:root:train 200 2.353108e+00 7.423818 48.725124
INFO:root:train 250 2.352823e+00 7.339392 48.630478
INFO:root:train 300 2.352785e+00 7.386836 48.639950
INFO:root:train 350 2.354171e+00 7.233796 48.499822
INFO:root:train 400 2.354651e+00 7.274782 48.476465
INFO:root:train 450 2.355233e+00 7.289357 48.285061
INFO:root:train 500 2.355434e+00 7.282310 48.163049
INFO:root:train 550 2.355286e+00 7.202813 48.219147
INFO:root:train 600 2.355271e+00 7.219738 48.180116
INFO:root:train 650 2.355253e+00 7.310868 48.137481
INFO:root:train 700 2.355107e+00 7.337732 48.129904
INFO:root:train_acc 7.310417
INFO:root:valid 000 2.340350e+00 9.375000 42.187500
INFO:root:valid 050 2.347840e+00 6.832108 44.454657
INFO:root:valid 100 2.339672e+00 7.178218 46.318069
INFO:root:valid 150 2.337202e+00 7.471026 47.081954
INFO:root:valid_acc 7.441667
INFO:root:epoch 3 lr 1.454462e-07
INFO:root:train 000 2.320118e+00 15.625000 54.687500
INFO:root:train 050 2.356312e+00 8.057598 48.039216
INFO:root:train 100 2.357131e+00 7.889851 47.865099
INFO:root:train 150 2.355702e+00 7.802152 48.023593
INFO:root:train 200 2.354033e+00 7.773632 48.266480
INFO:root:train 250 2.354304e+00 7.700448 48.337898
INFO:root:train 300 2.354784e+00 7.630814 48.375208
INFO:root:train 350 2.354974e+00 7.643340 48.384081
INFO:root:train 400 2.355464e+00 7.594296 48.223192
INFO:root:train 450 2.356033e+00 7.563054 48.118764
INFO:root:train 500 2.356237e+00 7.538049 47.979042
INFO:root:train 550 2.356026e+00 7.517582 48.014973
INFO:root:train 600 2.355864e+00 7.575915 48.115121
INFO:root:train 650 2.355730e+00 7.618088 48.178283
INFO:root:train 700 2.355568e+00 7.649786 48.323823
INFO:root:train_acc 7.637500
INFO:root:valid 000 2.333236e+00 7.812500 39.062500
INFO:root:valid 050 2.331933e+00 8.363971 47.702206
INFO:root:valid 100 2.332225e+00 8.462252 47.540223
INFO:root:valid 150 2.334106e+00 8.226407 47.278560
INFO:root:valid_acc 8.375000
INFO:root:epoch 4 lr 0.000000e+00
INFO:root:train 000 2.375695e+00 0.000000 50.000000
INFO:root:train 050 2.354864e+00 8.180147 48.835784
INFO:root:train 100 2.357049e+00 8.060025 48.917079
INFO:root:train 150 2.355007e+00 8.009106 48.923841
INFO:root:train 200 2.354684e+00 8.076803 48.997201
INFO:root:train 250 2.355098e+00 7.974353 48.935508
INFO:root:train 300 2.354602e+00 7.942276 49.216154
INFO:root:train 350 2.355287e+00 7.928241 49.047365
INFO:root:train 400 2.355216e+00 7.925499 49.099906
INFO:root:train 450 2.355752e+00 7.975333 49.036863
INFO:root:train 500 2.355533e+00 8.055763 49.030065
INFO:root:train 550 2.356269e+00 8.028017 48.933757
INFO:root:train 600 2.356532e+00 8.023087 48.874272
INFO:root:train 650 2.356462e+00 8.023714 48.838326
INFO:root:train 700 2.356951e+00 7.964069 48.823110
INFO:root:train_acc 7.972917
INFO:root:valid 000 2.314716e+00 7.812500 45.312500
INFO:root:valid 050 2.327353e+00 9.834559 48.376225
INFO:root:valid 100 2.331284e+00 9.637995 47.261757
INFO:root:valid 150 2.333170e+00 9.406043 46.926738
INFO:root:valid_acc 9.283333
INFO:solver.bo_hb:Configuration achieved a performance of 2.333062 in 1292.627203 seconds
INFO:solver.bo_hb:Start iteration 3 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 44.769956
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.862776
INFO:solver.bo_hb:Next candidate [3.11111111e+01 6.66666667e-02 4.22222222e+00 1.66675000e-02
 5.55555556e-01 4.83333333e+00 5.00000000e-01 9.45000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.026666666666666672, 'grad_clip_value': 8, 'initial_lr': 1.211539282207375e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0043613690324998e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 6.057696e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.350327e+00 7.352941 48.927696
INFO:root:train 100 2.351107e+00 7.301980 47.834158
INFO:root:train 150 2.350858e+00 7.129553 47.682119
INFO:root:train 200 2.351433e+00 7.112873 47.434701
INFO:root:train 250 2.349043e+00 7.189990 47.914592
INFO:root:train 300 2.347080e+00 7.308970 48.344061
INFO:root:train 350 2.345525e+00 7.385150 48.686788
INFO:root:train 400 2.344204e+00 7.485193 48.912874
INFO:root:train 450 2.342974e+00 7.604629 49.133869
INFO:root:train 500 2.342019e+00 7.662799 49.267091
INFO:root:train 550 2.340865e+00 7.764292 49.344941
INFO:root:train 600 2.339757e+00 7.872296 49.420237
INFO:root:train 650 2.339347e+00 7.879704 49.491167
INFO:root:train 700 2.338506e+00 7.917261 49.583185
INFO:root:train_acc 8.035417
INFO:root:valid 000 2.357530e+00 4.687500 37.500000
INFO:root:valid 050 2.324837e+00 9.528186 50.765931
INFO:root:valid 100 2.325929e+00 9.436881 50.788985
INFO:root:valid 150 2.323139e+00 9.602649 51.241722
INFO:root:valid_acc 9.608333
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.313802e+00 12.500000 53.125000
INFO:root:train 050 2.322959e+00 9.620098 51.439951
INFO:root:train 100 2.322616e+00 9.405941 51.980198
INFO:root:train 150 2.325412e+00 9.219785 51.014073
INFO:root:train 200 2.325639e+00 9.273943 51.002799
INFO:root:train 250 2.325522e+00 9.287849 50.983566
INFO:root:train 300 2.325159e+00 9.328281 51.053779
INFO:root:train 350 2.324830e+00 9.348291 51.046118
INFO:root:train 400 2.324794e+00 9.375000 51.106608
INFO:root:train 450 2.325132e+00 9.385394 51.094789
INFO:root:train 500 2.325321e+00 9.290793 51.047904
INFO:root:train 550 2.325860e+00 9.233212 50.966992
INFO:root:train 600 2.325896e+00 9.211210 50.896943
INFO:root:train 650 2.325733e+00 9.214190 50.967262
INFO:root:train 700 2.325807e+00 9.212286 51.003031
INFO:root:train_acc 9.120833
INFO:root:valid 000 2.341382e+00 6.250000 51.562500
INFO:root:valid 050 2.316426e+00 9.681373 52.022059
INFO:root:valid 100 2.319663e+00 9.576114 51.253094
INFO:root:valid 150 2.319247e+00 9.416391 51.003725
INFO:root:valid_acc 9.383333
INFO:solver.bo_hb:Configuration achieved a performance of 2.320068 
INFO:solver.bo_hb:Evaluation of this configuration took 446.754131 seconds
INFO:solver.bo_hb:Current incumbent [3.11111111e+01 6.66666667e-02 4.22222222e+00 1.66675000e-02
 1.00000000e+00 4.83333333e+00 0.00000000e+00 9.45000000e-04] with estimated performance 2.320068
INFO:solver.bo_hb:Start iteration 4 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 43.491488
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.480067
INFO:solver.bo_hb:Next candidate [2.40000000e+01 1.55555556e-01 4.22222222e+00 5.55650000e-03
 1.11111111e-01 3.16666667e+00 2.83333333e+00 3.95000000e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000619 seconds
INFO:solver.bo_hb:Current incumbent [3.11111111e+01 6.66666667e-02 4.22222222e+00 1.66675000e-02
 1.00000000e+00 4.83333333e+00 0.00000000e+00 9.45000000e-04] with estimated performance 2.320068
INFO:solver.bo_hb:Start iteration 5 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 37.829273
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.491774
INFO:solver.bo_hb:Next candidate [2.93333333e+01 6.66666667e-02 4.22222222e+00 8.33335000e-02
 1.66666667e+00 3.50000000e+00 5.00000000e-01 5.05000000e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000714 seconds
INFO:solver.bo_hb:Current incumbent [3.11111111e+01 6.66666667e-02 4.22222222e+00 1.66675000e-02
 1.00000000e+00 4.83333333e+00 0.00000000e+00 9.45000000e-04] with estimated performance 2.320068
INFO:solver.bo_hb:Start iteration 6 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 37.384050
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.459253
INFO:solver.bo_hb:Next candidate [2.40000000e+01 3.33333333e-01 7.77777778e+00 5.00005000e-02
 1.88888889e+00 5.50000000e+00 2.16666667e+00 9.45000000e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000614 seconds
INFO:solver.bo_hb:Current incumbent [3.11111111e+01 6.66666667e-02 4.22222222e+00 1.66675000e-02
 1.00000000e+00 4.83333333e+00 0.00000000e+00 9.45000000e-04] with estimated performance 2.320068
INFO:solver.bo_hb:Start iteration 7 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/rkohli/anaconda3/lib/python3.7/site-packages/scipy/optimize/optimize.py:596: RuntimeWarning: invalid value encountered in subtract
  numpy.max(numpy.abs(fsim[0] - fsim[1:])) <= fatol):
INFO:solver.bo_hb:Time to train the model: 37.757995
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.570622
INFO:solver.bo_hb:Next candidate [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.07407407e+00 4.50000000e+00 1.85185185e-02 8.71666667e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0040222409305598e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.349928e+00 7.383578 49.111520
INFO:root:train 100 2.350304e+00 7.317450 48.050743
INFO:root:train 150 2.349642e+00 7.253725 47.920116
INFO:root:train 200 2.349778e+00 7.213930 47.675684
INFO:root:train 250 2.346990e+00 7.339392 48.188496
INFO:root:train 300 2.344629e+00 7.490656 48.624377
INFO:root:train 350 2.342698e+00 7.629986 48.985043
INFO:root:train 400 2.341020e+00 7.773535 49.232388
INFO:root:train 450 2.339421e+00 7.912971 49.452605
INFO:root:train 500 2.338066e+00 7.990269 49.660055
INFO:root:train 550 2.336556e+00 8.104583 49.784483
INFO:root:train 600 2.335105e+00 8.262271 49.901206
INFO:root:train 650 2.334305e+00 8.347734 49.956797
INFO:root:train 700 2.333123e+00 8.438837 50.104761
INFO:root:train_acc 8.618750
INFO:root:valid 000 2.344596e+00 4.687500 37.500000
INFO:root:valid 050 2.313710e+00 11.274510 51.562500
INFO:root:valid 100 2.314601e+00 10.860149 51.624381
INFO:root:valid 150 2.311757e+00 11.061672 52.214404
INFO:root:valid_acc 11.133333
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.323395e+00 12.500000 54.687500
INFO:root:train 050 2.317382e+00 10.998775 52.665441
INFO:root:train 100 2.318492e+00 10.751856 52.459777
INFO:root:train 150 2.319641e+00 10.627070 51.759106
INFO:root:train 200 2.319098e+00 10.611007 51.865672
INFO:root:train 250 2.318664e+00 10.557769 51.867530
INFO:root:train 300 2.318817e+00 10.589701 51.842816
INFO:root:train 350 2.318460e+00 10.639245 51.896368
INFO:root:train 400 2.318251e+00 10.672537 52.033978
INFO:root:train 450 2.318507e+00 10.663803 52.064856
INFO:root:train 500 2.318596e+00 10.578842 52.282934
INFO:root:train 550 2.318817e+00 10.540495 52.228902
INFO:root:train 600 2.318410e+00 10.581323 52.332051
INFO:root:train 650 2.318235e+00 10.589478 52.359351
INFO:root:train 700 2.318216e+00 10.629904 52.364925
INFO:root:train_acc 10.641667
INFO:root:valid 000 2.334433e+00 1.562500 48.437500
INFO:root:valid 050 2.306138e+00 10.049020 52.849265
INFO:root:valid 100 2.310075e+00 9.746287 52.042079
INFO:root:valid 150 2.309810e+00 9.757864 52.079884
INFO:root:valid_acc 9.733333
INFO:solver.bo_hb:Configuration achieved a performance of 2.310242 
INFO:solver.bo_hb:Evaluation of this configuration took 446.540208 seconds
INFO:solver.bo_hb:Current incumbent [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.00000000e+00 4.50000000e+00 0.00000000e+00 8.71666667e-04] with estimated performance 2.310242
INFO:solver.bo_hb:Start iteration 8 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 45.964198
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.530013
INFO:solver.bo_hb:Next candidate [3.11111111e+01 2.88888889e-01 4.22222222e+00 5.00005000e-02
 1.88888889e+00 5.83333333e+00 1.50000000e+00 9.45000000e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000611 seconds
INFO:solver.bo_hb:Current incumbent [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.00000000e+00 4.50000000e+00 0.00000000e+00 8.71666667e-04] with estimated performance 2.310242
INFO:solver.bo_hb:Start iteration 9 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 57.326163
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.524440
INFO:solver.bo_hb:Next candidate [3.11111111e+01 5.18518519e-02 6.00000000e+00 6.11115000e-02
 1.88888889e+00 4.50000000e+00 5.00000000e-01 3.95000000e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000616 seconds
INFO:solver.bo_hb:Current incumbent [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.00000000e+00 4.50000000e+00 0.00000000e+00 8.71666667e-04] with estimated performance 2.310242
INFO:solver.bo_hb:Start iteration 10 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 57.100063
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.598869
INFO:solver.bo_hb:Next candidate [2.93333333e+01 2.00000000e-01 4.22222222e+00 8.33335000e-02
 1.07407407e+00 4.50000000e+00 2.16666667e+00 6.15000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 2.6101622241115896e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0028361940741568e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 1.305081e-06
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351002e+00 7.383578 48.988971
INFO:root:train 100 2.352437e+00 7.271040 47.694926
INFO:root:train 150 2.352883e+00 7.046772 47.309603
INFO:root:train 200 2.354162e+00 6.949627 46.913868
INFO:root:train 250 2.352486e+00 6.997012 47.254731
INFO:root:train 300 2.351189e+00 7.090947 47.622508
INFO:root:train 350 2.350256e+00 7.109152 47.894409
INFO:root:train 400 2.349545e+00 7.095542 48.024470
INFO:root:train 450 2.348917e+00 7.126524 48.163803
INFO:root:train 500 2.348645e+00 7.079591 48.219187
INFO:root:train 550 2.348147e+00 7.080876 48.176611
INFO:root:train 600 2.347660e+00 7.131344 48.125520
INFO:root:train 650 2.347918e+00 7.092454 48.111079
INFO:root:train 700 2.347672e+00 7.081402 48.136591
INFO:root:train_acc 7.122917
INFO:root:valid 000 2.377011e+00 6.250000 32.812500
INFO:root:valid 050 2.344280e+00 7.659314 47.794118
INFO:root:valid 100 2.345571e+00 7.301980 47.942450
INFO:root:valid 150 2.343014e+00 7.471026 48.551325
INFO:root:valid_acc 7.433333
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.355853e+00 10.937500 51.562500
INFO:root:train 050 2.347697e+00 7.965686 49.295343
INFO:root:train 100 2.348555e+00 7.858911 49.133663
INFO:root:train 150 2.348749e+00 7.895281 48.582368
INFO:root:train 200 2.348570e+00 7.921331 48.686256
INFO:root:train 250 2.348109e+00 8.011703 48.586902
INFO:root:train 300 2.348176e+00 8.082434 48.691860
INFO:root:train 350 2.347811e+00 8.119658 48.673433
INFO:root:train 400 2.347553e+00 8.182668 48.733635
INFO:root:train 450 2.347886e+00 8.152023 48.711197
INFO:root:train 500 2.347974e+00 8.083832 48.867889
INFO:root:train 550 2.348166e+00 8.062046 48.769283
INFO:root:train 600 2.347740e+00 8.145279 48.897671
INFO:root:train 650 2.347594e+00 8.124520 48.907930
INFO:root:train 700 2.347515e+00 8.124554 48.943474
INFO:root:train_acc 8.102083
INFO:root:valid 000 2.345186e+00 6.250000 45.312500
INFO:root:valid 050 2.324814e+00 9.589461 49.019608
INFO:root:valid 100 2.330823e+00 8.926361 47.617574
INFO:root:valid 150 2.330094e+00 9.002483 47.764901
INFO:root:valid_acc 8.816667
INFO:solver.bo_hb:Configuration achieved a performance of 2.330805 
INFO:solver.bo_hb:Evaluation of this configuration took 394.343808 seconds
INFO:solver.bo_hb:Current incumbent [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.00000000e+00 4.50000000e+00 0.00000000e+00 8.71666667e-04] with estimated performance 2.310242
INFO:solver.bo_hb:Start iteration 11 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 47.979222
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.490063
INFO:solver.bo_hb:Next candidate [2.93333333e+01 2.88888889e-01 4.22222222e+00 8.33335000e-02
 1.88888889e+00 5.83333333e+00 5.00000000e-01 9.45000000e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000610 seconds
INFO:solver.bo_hb:Current incumbent [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.00000000e+00 4.50000000e+00 0.00000000e+00 8.71666667e-04] with estimated performance 2.310242
INFO:solver.bo_hb:Start iteration 12 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 39.581183
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.698016
INFO:solver.bo_hb:Next candidate [2.93333333e+01 2.00000000e-01 6.00000000e+00 1.66675000e-02
 1.66666667e+00 5.50000000e+00 8.33333333e-01 8.35000000e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000618 seconds
INFO:solver.bo_hb:Current incumbent [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.00000000e+00 4.50000000e+00 0.00000000e+00 8.71666667e-04] with estimated performance 2.310242
INFO:solver.bo_hb:Start iteration 13 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 39.743116
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.495854
INFO:solver.bo_hb:Next candidate [2.93333333e+01 1.55555556e-01 4.22222222e+00 5.55650000e-03
 1.00000000e+00 5.83333333e+00 5.00000000e-01 9.45000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.06222222222222223, 'grad_clip_value': 8, 'initial_lr': 1.0660620905544188e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0043613690324998e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 5.330310e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.350431e+00 7.352941 48.927696
INFO:root:train 100 2.351316e+00 7.301980 47.818688
INFO:root:train 150 2.351174e+00 7.119205 47.609685
INFO:root:train 200 2.351863e+00 7.081779 47.349192
INFO:root:train 250 2.349576e+00 7.165090 47.827440
INFO:root:train 300 2.347717e+00 7.262251 48.240241
INFO:root:train 350 2.346262e+00 7.331731 48.571047
INFO:root:train 400 2.345034e+00 7.426746 48.811565
INFO:root:train 450 2.343903e+00 7.514551 49.026469
INFO:root:train 500 2.343053e+00 7.556761 49.129865
INFO:root:train 550 2.341993e+00 7.653698 49.186139
INFO:root:train 600 2.340975e+00 7.763103 49.248648
INFO:root:train 650 2.340669e+00 7.750096 49.315956
INFO:root:train 700 2.339919e+00 7.783524 49.404868
INFO:root:train_acc 7.895833
INFO:root:valid 000 2.360698e+00 4.687500 37.500000
INFO:root:valid 050 2.327787e+00 9.313725 50.428922
INFO:root:valid 100 2.328931e+00 9.235767 50.448639
INFO:root:valid 150 2.326151e+00 9.292219 50.900248
INFO:root:valid_acc 9.291667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.338541e+00 7.812500 53.125000
INFO:root:train 050 2.331643e+00 9.589461 51.378676
INFO:root:train 100 2.331295e+00 9.297649 51.113861
INFO:root:train 150 2.332313e+00 9.178394 50.372517
INFO:root:train 200 2.332385e+00 9.157338 50.676306
INFO:root:train 250 2.331816e+00 9.219373 50.522908
INFO:root:train 300 2.331670e+00 9.307517 50.425664
INFO:root:train 350 2.331257e+00 9.415064 50.413996
INFO:root:train 400 2.330816e+00 9.476309 50.580580
INFO:root:train 450 2.331060e+00 9.475471 50.588969
INFO:root:train 500 2.331358e+00 9.421781 50.645584
INFO:root:train 550 2.331728e+00 9.406193 50.564315
INFO:root:train 600 2.331433e+00 9.445196 50.678557
INFO:root:train 650 2.331129e+00 9.447005 50.775250
INFO:root:train 700 2.331061e+00 9.439640 50.815799
INFO:root:train_acc 9.358333
INFO:root:valid 000 2.339947e+00 3.125000 46.875000
INFO:root:valid 050 2.315071e+00 9.037990 50.888480
INFO:root:valid 100 2.319406e+00 8.771658 50.556931
INFO:root:valid 150 2.318967e+00 8.836921 50.455298
INFO:root:valid_acc 8.791667
INFO:solver.bo_hb:Configuration achieved a performance of 2.319510 
INFO:solver.bo_hb:Evaluation of this configuration took 446.517482 seconds
INFO:solver.bo_hb:Current incumbent [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.00000000e+00 4.50000000e+00 0.00000000e+00 8.71666667e-04] with estimated performance 2.310242
INFO:solver.bo_hb:Start iteration 14 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 84.925420
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.716020
INFO:solver.bo_hb:Next candidate [2.93333333e+01 3.70370370e-02 7.48148148e+00 1.66675000e-02
 1.00000000e+00 4.50000000e+00 1.50000000e+00 5.41666667e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.01481481481481482, 'grad_clip_value': 8, 'initial_lr': 1.211539282207375e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0024975809558717e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 6.057696e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351103e+00 7.322304 48.927696
INFO:root:train 100 2.352657e+00 7.224629 47.679455
INFO:root:train 150 2.353213e+00 7.005381 47.330298
INFO:root:train 200 2.354634e+00 6.902985 46.944963
INFO:root:train 250 2.353043e+00 6.959661 47.285857
INFO:root:train 300 2.351868e+00 7.044228 47.612126
INFO:root:train 350 2.351057e+00 7.037927 47.876603
INFO:root:train 400 2.350457e+00 7.037095 48.008884
INFO:root:train 450 2.349959e+00 7.074557 48.122228
INFO:root:train 500 2.349806e+00 7.057759 48.153693
INFO:root:train 550 2.349405e+00 7.035504 48.117060
INFO:root:train 600 2.349010e+00 7.079347 48.089122
INFO:root:train 650 2.349399e+00 7.044451 48.075077
INFO:root:train 700 2.349261e+00 7.025678 48.083096
INFO:root:train_acc 7.064583
INFO:root:valid 000 2.382508e+00 6.250000 32.812500
INFO:root:valid 050 2.347711e+00 7.536765 47.610294
INFO:root:valid 100 2.349162e+00 7.178218 47.741337
INFO:root:valid 150 2.346505e+00 7.326159 48.354719
INFO:root:valid_acc 7.283333
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.340238e+00 7.812500 46.875000
INFO:root:train 050 2.346213e+00 8.026961 48.835784
INFO:root:train 100 2.346069e+00 7.332921 48.808787
INFO:root:train 150 2.348266e+00 7.253725 47.816639
INFO:root:train 200 2.348550e+00 7.066231 47.963308
INFO:root:train 250 2.348192e+00 7.090388 47.995518
INFO:root:train 300 2.347771e+00 7.075374 48.261005
INFO:root:train 350 2.347437e+00 7.055734 48.361823
INFO:root:train 400 2.347714e+00 7.087749 48.312812
INFO:root:train 450 2.348169e+00 7.067627 48.361280
INFO:root:train 500 2.348356e+00 7.010978 48.325225
INFO:root:train 550 2.348924e+00 6.939088 48.256012
INFO:root:train 600 2.348857e+00 6.959755 48.146319
INFO:root:train 650 2.348625e+00 6.972446 48.209485
INFO:root:train 700 2.348683e+00 6.983327 48.194544
INFO:root:train_acc 6.968750
INFO:root:valid 000 2.361143e+00 4.687500 43.750000
INFO:root:valid 050 2.338430e+00 7.781863 48.682598
INFO:root:valid 100 2.342136e+00 7.456683 48.066213
INFO:root:valid 150 2.341633e+00 7.471026 48.096026
INFO:root:valid_acc 7.408333
INFO:solver.bo_hb:Configuration achieved a performance of 2.342627 
INFO:solver.bo_hb:Evaluation of this configuration took 395.511827 seconds
INFO:solver.bo_hb:Current incumbent [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.00000000e+00 4.50000000e+00 0.00000000e+00 8.71666667e-04] with estimated performance 2.310242
INFO:solver.bo_hb:Start iteration 15 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 110.953611
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.458205
INFO:solver.bo_hb:Next candidate [2.22222222e+01 2.00000000e-01 4.66666667e+00 1.66675000e-02
 1.00000000e+00 4.50000000e+00 2.68518519e+00 5.05000000e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000620 seconds
INFO:solver.bo_hb:Current incumbent [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.00000000e+00 4.50000000e+00 0.00000000e+00 8.71666667e-04] with estimated performance 2.310242
INFO:solver.bo_hb:Start iteration 16 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 39.850467
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.645149
INFO:solver.bo_hb:Next candidate [2.40000000e+01 6.66666667e-02 4.66666667e+00 8.33335000e-02
 3.33333333e-01 5.50000000e+00 1.50000000e+00 8.35000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.026666666666666672, 'grad_clip_value': 8, 'initial_lr': 2.6101622241115896e-06, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0038527198226775e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 2.610162e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351152e+00 7.322304 48.927696
INFO:root:train 100 2.352771e+00 7.209158 47.648515
INFO:root:train 150 2.353381e+00 6.984685 47.288907
INFO:root:train 200 2.354869e+00 6.887438 46.937189
INFO:root:train 250 2.353329e+00 6.953436 47.298307
INFO:root:train 300 2.352214e+00 7.023463 47.622508
INFO:root:train 350 2.351466e+00 7.011218 47.863248
INFO:root:train 400 2.350919e+00 7.009819 47.993298
INFO:root:train 450 2.350486e+00 7.050305 48.118764
INFO:root:train 500 2.350399e+00 7.045284 48.153693
INFO:root:train 550 2.350046e+00 7.018489 48.083031
INFO:root:train 600 2.349698e+00 7.055948 48.065724
INFO:root:train 650 2.350154e+00 7.018049 48.060676
INFO:root:train 700 2.350076e+00 6.990014 48.065264
INFO:root:train_acc 7.022917
INFO:root:valid 000 2.384781e+00 6.250000 35.937500
INFO:root:valid 050 2.349433e+00 7.352941 47.518382
INFO:root:valid 100 2.350960e+00 7.193688 47.818688
INFO:root:valid 150 2.348299e+00 7.274421 48.396109
INFO:root:valid_acc 7.225000
INFO:root:epoch 1 lr 2.610162e-08
INFO:root:train 000 2.335911e+00 9.375000 50.000000
INFO:root:train 050 2.347393e+00 7.628676 49.111520
INFO:root:train 100 2.346981e+00 7.518564 49.319307
INFO:root:train 150 2.349271e+00 7.398593 48.385762
INFO:root:train 200 2.349724e+00 7.400498 48.274254
INFO:root:train 250 2.349505e+00 7.463894 48.331673
INFO:root:train 300 2.349185e+00 7.376453 48.416736
INFO:root:train 350 2.348858e+00 7.407407 48.441952
INFO:root:train 400 2.348907e+00 7.395574 48.484258
INFO:root:train 450 2.349366e+00 7.379435 48.486003
INFO:root:train 500 2.349536e+00 7.322854 48.487400
INFO:root:train 550 2.350095e+00 7.259528 48.394964
INFO:root:train 600 2.350161e+00 7.237937 48.258111
INFO:root:train 650 2.350000e+00 7.241263 48.319892
INFO:root:train 700 2.350019e+00 7.241887 48.379547
INFO:root:train_acc 7.197917
INFO:root:valid 000 2.358320e+00 4.687500 42.187500
INFO:root:valid 050 2.336573e+00 7.781863 48.560049
INFO:root:valid 100 2.340847e+00 7.441213 48.035272
INFO:root:valid 150 2.340243e+00 7.357202 47.982202
INFO:root:valid_acc 7.241667
INFO:solver.bo_hb:Configuration achieved a performance of 2.341226 
INFO:solver.bo_hb:Evaluation of this configuration took 394.770008 seconds
INFO:solver.bo_hb:Current incumbent [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.00000000e+00 4.50000000e+00 0.00000000e+00 8.71666667e-04] with estimated performance 2.310242
INFO:solver.bo_hb:Start iteration 17 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 42.304545
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.757761
INFO:solver.bo_hb:Next candidate [2.40000000e+01 6.66666667e-02 7.33333333e+00 5.00005000e-02
 3.33333333e-01 5.50000000e+00 5.00000000e-01 5.05000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.026666666666666672, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 1.778290e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.350936e+00 7.322304 48.958333
INFO:root:train 100 2.352342e+00 7.224629 47.710396
INFO:root:train 150 2.352726e+00 7.026076 47.433775
INFO:root:train 200 2.353976e+00 6.941853 47.115983
INFO:root:train 250 2.352209e+00 7.021912 47.541086
INFO:root:train 300 2.350871e+00 7.090947 47.861296
INFO:root:train 350 2.349912e+00 7.122507 48.116987
INFO:root:train 400 2.349157e+00 7.153990 48.289433
INFO:root:train 450 2.348517e+00 7.202744 48.454823
INFO:root:train 500 2.348200e+00 7.204341 48.534182
INFO:root:train 550 2.347627e+00 7.216992 48.502722
INFO:root:train 600 2.347067e+00 7.261335 48.502496
INFO:root:train 650 2.347291e+00 7.226863 48.504704
INFO:root:train 700 2.347010e+00 7.224055 48.533345
INFO:root:train_acc 7.293750
INFO:root:valid 000 2.377827e+00 6.250000 37.500000
INFO:root:valid 050 2.342828e+00 7.812500 48.253676
INFO:root:valid 100 2.344241e+00 7.766089 48.638614
INFO:root:valid 150 2.341533e+00 7.936672 49.120447
INFO:root:valid_acc 7.850000
INFO:root:epoch 1 lr 1.778290e-08
INFO:root:train 000 2.329670e+00 9.375000 51.562500
INFO:root:train 050 2.340805e+00 8.272059 49.754902
INFO:root:train 100 2.340357e+00 7.998144 50.046411
INFO:root:train 150 2.342810e+00 7.822848 49.151490
INFO:root:train 200 2.343178e+00 7.804726 49.074938
INFO:root:train 250 2.342944e+00 7.843625 49.134711
INFO:root:train 300 2.342591e+00 7.781354 49.231728
INFO:root:train 350 2.342240e+00 7.830306 49.243234
INFO:root:train 400 2.342251e+00 7.812500 49.322007
INFO:root:train 450 2.342652e+00 7.798642 49.334812
INFO:root:train 500 2.342803e+00 7.737650 49.373129
INFO:root:train 550 2.343335e+00 7.701906 49.271211
INFO:root:train 600 2.343371e+00 7.692908 49.165453
INFO:root:train 650 2.343190e+00 7.702093 49.251152
INFO:root:train 700 2.343199e+00 7.721113 49.282275
INFO:root:train_acc 7.654167
INFO:root:valid 000 2.353694e+00 4.687500 43.750000
INFO:root:valid 050 2.330663e+00 8.149510 49.877451
INFO:root:valid 100 2.334592e+00 7.936262 49.226485
INFO:root:valid 150 2.334042e+00 7.915977 49.110099
INFO:root:valid_acc 7.858333
INFO:solver.bo_hb:Configuration achieved a performance of 2.334972 
INFO:solver.bo_hb:Evaluation of this configuration took 448.085420 seconds
INFO:solver.bo_hb:Current incumbent [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.00000000e+00 4.50000000e+00 0.00000000e+00 8.71666667e-04] with estimated performance 2.310242
INFO:solver.bo_hb:Start iteration 18 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 42.460255
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.676172
INFO:solver.bo_hb:Next candidate [2.93333333e+01 1.55555556e-01 7.33333333e+00 9.44445000e-02
 1.11111111e-01 4.83333333e+00 8.33333333e-01 9.45000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.06222222222222223, 'grad_clip_value': 8, 'initial_lr': 2.966350736475734e-06, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'adad', 'weight_decay': 1.0043613690324998e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 2.966351e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351188e+00 7.322304 48.897059
INFO:root:train 100 2.352858e+00 7.193688 47.617574
INFO:root:train 150 2.353510e+00 6.932947 47.299255
INFO:root:train 200 2.355044e+00 6.856343 46.937189
INFO:root:train 250 2.353545e+00 6.928536 47.285857
INFO:root:train 300 2.352474e+00 7.002699 47.601744
INFO:root:train 350 2.351772e+00 7.006766 47.840990
INFO:root:train 400 2.351261e+00 7.013716 47.969919
INFO:root:train 450 2.350874e+00 7.067627 48.104906
INFO:root:train 500 2.350836e+00 7.054641 48.153693
INFO:root:train 550 2.350520e+00 7.024161 48.074524
INFO:root:train 600 2.350203e+00 7.053349 48.052725
INFO:root:train 650 2.350710e+00 7.018049 48.036674
INFO:root:train 700 2.350677e+00 6.985556 48.025143
INFO:root:train_acc 7.022917
INFO:root:valid 000 2.387195e+00 4.687500 35.937500
INFO:root:valid 050 2.350767e+00 7.475490 47.334559
INFO:root:valid 100 2.352332e+00 7.209158 47.648515
INFO:root:valid 150 2.349653e+00 7.357202 48.209851
INFO:root:valid_acc 7.275000
INFO:root:epoch 1 lr 2.966351e-08
INFO:root:train 000 2.360402e+00 6.250000 51.562500
INFO:root:train 050 2.354789e+00 7.843137 49.019608
INFO:root:train 100 2.354125e+00 7.812500 48.452970
INFO:root:train 150 2.354688e+00 7.771109 47.826987
INFO:root:train 200 2.354988e+00 7.719216 48.087687
INFO:root:train 250 2.354299e+00 7.756474 47.958167
INFO:root:train 300 2.354075e+00 7.843646 47.928779
INFO:root:train 350 2.353638e+00 7.905983 47.965634
INFO:root:train 400 2.353263e+00 7.952774 48.172537
INFO:root:train 450 2.353579e+00 7.971868 48.163803
INFO:root:train 500 2.353858e+00 7.918538 48.191118
INFO:root:train 550 2.354218e+00 7.877722 48.100045
INFO:root:train 600 2.353954e+00 7.945092 48.156718
INFO:root:train 650 2.353659e+00 7.920507 48.298291
INFO:root:train 700 2.353540e+00 7.910574 48.350571
INFO:root:train_acc 7.835417
INFO:root:valid 000 2.352312e+00 4.687500 46.875000
INFO:root:valid 050 2.330241e+00 8.363971 48.253676
INFO:root:valid 100 2.335680e+00 7.998144 47.246287
INFO:root:valid 150 2.335025e+00 8.153974 47.144040
INFO:root:valid_acc 8.025000
INFO:solver.bo_hb:Configuration achieved a performance of 2.335697 
INFO:solver.bo_hb:Evaluation of this configuration took 510.349310 seconds
INFO:solver.bo_hb:Current incumbent [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.00000000e+00 4.50000000e+00 0.00000000e+00 8.71666667e-04] with estimated performance 2.310242
INFO:solver.bo_hb:Start iteration 19 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([19])) that is different to the input size (torch.Size([19, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 40.140436
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.752146
INFO:solver.bo_hb:Next candidate [2.93333333e+01 6.66666667e-02 6.00000000e+00 5.55650000e-03
 1.66666667e+00 5.50000000e+00 1.50000000e+00 5.05000000e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000598 seconds
INFO:solver.bo_hb:Current incumbent [2.93333333e+01 2.00000000e-01 6.00000000e+00 5.00005000e-02
 1.00000000e+00 4.50000000e+00 0.00000000e+00 8.71666667e-04] with estimated performance 2.310242
INFO:solver.bo_hb:Return [29.333333333333332, 0.2, 6.0, 0.0500005, 1.0, 4.5, 0.0, 0.0008716666666666665] as incumbent with error 2.310242 
