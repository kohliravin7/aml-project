INFO:solver.bo_hb:Evaluate: [1.11165193e-01 5.21629886e+00 8.84883241e-02 4.34823313e-01
 4.92439912e+00 2.91465311e+00 2.39687820e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 in 0.000220 seconds
INFO:solver.bo_hb:Evaluate: [3.45841639e-02 5.59253467e+00 7.49960013e-02 8.83202922e-01
 4.41049184e+00 2.89225791e+00 1.54542869e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 in 0.000158 seconds
INFO:solver.bo_hb:Evaluate: [1.82202307e-01 4.31768181e+00 8.33510938e-02 1.64974045e+00
 3.22950966e+00 1.67553786e+00 9.72605894e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 in 0.000152 seconds
INFO:solver.bo_hb:Start iteration 3 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/aml-project/src/models/DNGO.py:158: RuntimeWarning: invalid value encountered in true_divide
  y_nom=(y-self._my)/self._sy
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/rkohli/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py:2093: RuntimeWarning: invalid value encountered in det
  r = _umath_linalg.det(a, signature=signature)
INFO:solver.bo_hb:Time to train the model: 14.844802
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.564368
INFO:solver.bo_hb:Next candidate [2.00e-01 6.00e+00 5.05e-02 1.00e+00 4.50e+00 1.50e+00 5.05e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
INFO:root:epoch 0 lr 6.309138e-04
/home/rkohli/aml-project/src/train.py:131: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), grad_clip)
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.304052e+00 14.920343 59.834559
INFO:root:train 100 2.192448e+00 22.277228 69.647277
INFO:root:train 150 2.106916e+00 26.479719 74.596440
INFO:root:train 200 2.028890e+00 30.472637 77.751866
INFO:root:train 250 1.957219e+00 34.132221 80.048556
INFO:root:train 300 1.884605e+00 37.302741 82.039037
INFO:root:train 350 1.817105e+00 39.975071 83.542557
INFO:root:train 400 1.752013e+00 42.651185 84.834788
INFO:root:train 450 1.691422e+00 45.111558 85.875139
INFO:root:train 500 1.629927e+00 47.598553 86.785803
INFO:root:train 550 1.571227e+00 49.824183 87.607759
INFO:root:train 600 1.513579e+00 51.864081 88.376144
INFO:root:train 650 1.458484e+00 53.871448 89.072101
INFO:root:train 700 1.407460e+00 55.668242 89.677693
INFO:root:train_acc 57.227083
INFO:root:valid 000 5.966894e-01 81.250000 100.000000
INFO:root:valid 050 6.365464e-01 82.138480 97.855392
INFO:root:valid 100 6.365928e-01 82.286510 97.880569
INFO:root:valid 150 6.407184e-01 81.995033 98.002897
INFO:root:valid_acc 82.116667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 1.077636e+00 67.187500 98.437500
INFO:root:train 050 1.195947e+00 62.898284 93.075980
INFO:root:train 100 1.197284e+00 62.747525 93.285891
INFO:root:train 150 1.186214e+00 63.410596 93.512003
INFO:root:train 200 1.184767e+00 63.425062 93.555659
INFO:root:train 250 1.185868e+00 63.365289 93.538347
INFO:root:train 300 1.185523e+00 63.418812 93.500831
INFO:root:train 350 1.190918e+00 63.118768 93.442842
INFO:root:train 400 1.196237e+00 63.018236 93.360349
INFO:root:train 450 1.193014e+00 63.040466 93.396619
INFO:root:train 500 1.193826e+00 63.055140 93.360155
INFO:root:train 550 1.194626e+00 63.010436 93.358666
INFO:root:train 600 1.194599e+00 63.095362 93.323627
INFO:root:train 650 1.193881e+00 63.124040 93.296371
INFO:root:train 700 1.194688e+00 63.055011 93.319811
INFO:root:train_acc 63.145833
INFO:root:valid 000 6.451895e-01 85.937500 96.875000
INFO:root:valid 050 7.061439e-01 81.495098 97.732843
INFO:root:valid 100 7.146821e-01 80.847772 97.942450
INFO:root:valid 150 7.190098e-01 80.691225 97.868377
INFO:root:valid_acc 80.683333
INFO:solver.bo_hb:Configuration achieved a performance of 0.721007 
INFO:solver.bo_hb:Evaluation of this configuration took 98.499897 seconds
INFO:solver.bo_hb:Current incumbent [2.00e-01 6.00e+00 5.05e-02 1.00e+00 4.50e+00 2.00e+00 5.05e-04] with estimated performance 0.721007
INFO:solver.bo_hb:Start iteration 4 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 17.274554
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.209768
INFO:solver.bo_hb:Next candidate [3.33333333e-01 4.07407407e+00 3.95000000e-02 1.88888889e+00
 5.83333333e+00 1.66666667e-01 6.50000000e-05]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000196 seconds
INFO:solver.bo_hb:Current incumbent [2.00e-01 6.00e+00 5.05e-02 1.00e+00 4.50e+00 2.00e+00 5.05e-04] with estimated performance 0.721007
INFO:solver.bo_hb:Start iteration 5 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.385091
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.221321
INFO:solver.bo_hb:Next candidate [3.77777778e-01 6.00000000e+00 5.05000000e-02 1.00000000e+00
 3.05555556e+00 1.83333333e+00 5.05000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.1511111111111111, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 6.309138e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.304055e+00 14.920343 59.803922
INFO:root:train 100 2.192532e+00 22.153465 69.585396
INFO:root:train 150 2.107107e+00 26.355546 74.503311
INFO:root:train 200 2.029243e+00 30.278296 77.689677
INFO:root:train 250 1.957551e+00 33.989044 80.023655
INFO:root:train 300 1.885086e+00 37.219684 82.013081
INFO:root:train 350 1.817809e+00 39.930556 83.502493
INFO:root:train 400 1.752909e+00 42.565461 84.807512
INFO:root:train 450 1.692456e+00 45.049196 85.854351
INFO:root:train 500 1.631219e+00 47.514346 86.770210
INFO:root:train 550 1.572723e+00 49.733439 87.590744
INFO:root:train 600 1.515091e+00 51.773087 88.363145
INFO:root:train 650 1.460082e+00 53.777842 89.057700
INFO:root:train 700 1.409061e+00 55.530046 89.659861
INFO:root:train_acc 57.122917
INFO:root:valid 000 6.091717e-01 78.125000 100.000000
INFO:root:valid 050 6.535463e-01 81.556373 97.886029
INFO:root:valid 100 6.548345e-01 81.868812 97.926980
INFO:root:valid 150 6.587376e-01 81.498344 98.023593
INFO:root:valid_acc 81.641667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 1.618900e+00 43.750000 90.625000
INFO:root:train 050 1.616196e+00 51.102941 89.215686
INFO:root:train 100 1.643118e+00 50.201114 88.954208
INFO:root:train 150 1.644888e+00 50.372517 88.917632
INFO:root:train 200 1.650257e+00 50.450871 88.712687
INFO:root:train 250 1.646394e+00 50.753237 88.713894
INFO:root:train 300 1.642833e+00 50.872093 88.875623
INFO:root:train 350 1.646585e+00 50.734509 88.808761
INFO:root:train 400 1.653543e+00 50.607855 88.727400
INFO:root:train 450 1.647540e+00 50.758731 88.806125
INFO:root:train 500 1.647618e+00 50.785928 88.797405
INFO:root:train 550 1.650652e+00 50.663566 88.781760
INFO:root:train 600 1.651791e+00 50.668157 88.732321
INFO:root:train 650 1.651663e+00 50.691244 88.702477
INFO:root:train 700 1.648676e+00 50.735556 88.728156
INFO:root:train_acc 50.900000
INFO:root:valid 000 8.124493e-01 79.687500 96.875000
INFO:root:valid 050 9.072625e-01 72.487745 96.875000
INFO:root:valid 100 9.137714e-01 71.751238 97.261757
INFO:root:valid 150 9.195577e-01 71.564570 97.071606
INFO:root:valid_acc 71.516667
INFO:solver.bo_hb:Configuration achieved a performance of 0.921697 
INFO:solver.bo_hb:Evaluation of this configuration took 76.638153 seconds
INFO:solver.bo_hb:Current incumbent [2.00e-01 6.00e+00 5.05e-02 1.00e+00 4.50e+00 2.00e+00 5.05e-04] with estimated performance 0.721007
INFO:solver.bo_hb:Start iteration 6 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 17.451209
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.223112
INFO:solver.bo_hb:Next candidate [2.44444444e-01 5.11111111e+00 6.51666667e-02 1.22222222e+00
 3.05555556e+00 2.50000000e+00 9.81666667e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.0977777777777778, 'grad_clip_value': 8, 'initial_lr': 0.0013499986473050239, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.004530976036227e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 6.749993e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.297038e+00 15.165441 60.753676
INFO:root:train 100 2.181267e+00 22.741337 70.513614
INFO:root:train 150 2.092307e+00 27.162666 75.341474
INFO:root:train 200 2.011489e+00 31.273321 78.435945
INFO:root:train 250 1.936388e+00 35.009960 80.739542
INFO:root:train 300 1.859989e+00 38.366902 82.687915
INFO:root:train 350 1.789533e+00 41.038996 84.134615
INFO:root:train 400 1.721578e+00 43.785069 85.415368
INFO:root:train 450 1.658670e+00 46.327605 86.425998
INFO:root:train 500 1.595306e+00 48.855414 87.328468
INFO:root:train 550 1.535287e+00 51.154152 88.109687
INFO:root:train 600 1.476476e+00 53.247192 88.859713
INFO:root:train 650 1.420501e+00 55.261137 89.528130
INFO:root:train 700 1.369235e+00 57.005617 90.127942
INFO:root:train_acc 58.568750
INFO:root:valid 000 5.771605e-01 79.687500 100.000000
INFO:root:valid 050 6.107577e-01 82.598039 98.161765
INFO:root:valid 100 6.108264e-01 82.781559 98.097153
INFO:root:valid 150 6.145392e-01 82.533113 98.189156
INFO:root:valid_acc 82.733333
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 1.210280e+00 64.062500 95.312500
INFO:root:train 050 1.289589e+00 61.182598 92.677696
INFO:root:train 100 1.296199e+00 60.519802 92.713490
INFO:root:train 150 1.284789e+00 60.740894 92.818709
INFO:root:train 200 1.283436e+00 60.937500 92.801617
INFO:root:train 250 1.282364e+00 60.725847 92.797560
INFO:root:train 300 1.282310e+00 60.823297 92.748131
INFO:root:train 350 1.290375e+00 60.478989 92.632657
INFO:root:train 400 1.297046e+00 60.411471 92.557668
INFO:root:train 450 1.295872e+00 60.410892 92.610172
INFO:root:train 500 1.295802e+00 60.485279 92.571108
INFO:root:train 550 1.296706e+00 60.412886 92.558984
INFO:root:train 600 1.296967e+00 60.446131 92.535878
INFO:root:train 650 1.295266e+00 60.534274 92.528322
INFO:root:train 700 1.294357e+00 60.536287 92.517386
INFO:root:train_acc 60.581250
INFO:root:valid 000 6.265735e-01 84.375000 96.875000
INFO:root:valid 050 7.062263e-01 80.974265 97.824755
INFO:root:valid 100 7.138257e-01 80.352723 98.050743
INFO:root:valid 150 7.190970e-01 80.028974 97.909768
INFO:root:valid_acc 79.908333
INFO:solver.bo_hb:Configuration achieved a performance of 0.721394 
INFO:solver.bo_hb:Evaluation of this configuration took 75.322376 seconds
INFO:solver.bo_hb:Current incumbent [2.00e-01 6.00e+00 5.05e-02 1.00e+00 4.50e+00 2.00e+00 5.05e-04] with estimated performance 0.721007
INFO:solver.bo_hb:Start iteration 7 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 13.852874
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.217493
INFO:solver.bo_hb:Next candidate [2.22222222e-02 4.66666667e+00 6.15000000e-02 1.22222222e+00
 3.50000000e+00 2.50000000e+00 9.45000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.008888888888888894, 'grad_clip_value': 8, 'initial_lr': 0.00132739445772974, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0043613690324998e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 6.636972e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.298604e+00 15.134804 60.447304
INFO:root:train 100 2.183612e+00 22.679455 70.312500
INFO:root:train 150 2.095522e+00 26.986755 75.289735
INFO:root:train 200 2.015264e+00 31.102301 78.412624
INFO:root:train 250 1.941105e+00 34.860558 80.664841
INFO:root:train 300 1.865430e+00 38.112542 82.594477
INFO:root:train 350 1.795460e+00 40.785256 84.018875
INFO:root:train 400 1.727938e+00 43.512313 85.317955
INFO:root:train 450 1.664754e+00 46.112805 86.335920
INFO:root:train 500 1.601004e+00 48.649576 87.247380
INFO:root:train 550 1.540717e+00 50.921620 88.041629
INFO:root:train 600 1.481574e+00 53.054804 88.812916
INFO:root:train 650 1.425400e+00 55.078725 89.489727
INFO:root:train 700 1.373842e+00 56.842903 90.083363
INFO:root:train_acc 58.412500
INFO:root:valid 000 5.673450e-01 79.687500 100.000000
INFO:root:valid 050 6.046415e-01 82.904412 98.039216
INFO:root:valid 100 6.071830e-01 82.735149 98.050743
INFO:root:valid 150 6.102041e-01 82.615894 98.240894
INFO:root:valid_acc 82.858333
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 7.647346e-01 76.562500 98.437500
INFO:root:train 050 6.889244e-01 79.871324 97.824755
INFO:root:train 100 6.954644e-01 79.238861 97.818688
INFO:root:train 150 6.882278e-01 79.728891 97.785596
INFO:root:train 200 6.825021e-01 80.091729 97.792289
INFO:root:train 250 6.848465e-01 80.117032 97.615787
INFO:root:train 300 6.818739e-01 80.315615 97.580980
INFO:root:train 350 6.871555e-01 79.994658 97.591702
INFO:root:train 400 6.876711e-01 79.921291 97.572475
INFO:root:train 450 6.850760e-01 79.940410 97.651053
INFO:root:train 500 6.842190e-01 79.961951 97.670284
INFO:root:train 550 6.843428e-01 79.931375 97.660504
INFO:root:train 600 6.847852e-01 79.916285 97.639351
INFO:root:train 650 6.824463e-01 80.021121 97.679051
INFO:root:train 700 6.825137e-01 79.977265 97.699715
INFO:root:train_acc 80.020833
INFO:root:valid 000 5.420325e-01 85.937500 96.875000
INFO:root:valid 050 5.826981e-01 83.547794 98.498775
INFO:root:valid 100 5.943663e-01 83.493193 98.530322
INFO:root:valid 150 5.980905e-01 83.412666 98.416805
INFO:root:valid_acc 83.483333
INFO:solver.bo_hb:Configuration achieved a performance of 0.600589 
INFO:solver.bo_hb:Evaluation of this configuration took 74.606146 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 4.66666667e+00 6.15000000e-02 1.00000000e+00
 3.50000000e+00 2.00000000e+00 9.45000000e-04] with estimated performance 0.600589
INFO:solver.bo_hb:Start iteration 8 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 13.783039
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.210866
INFO:solver.bo_hb:Next candidate [3.77777778e-01 6.00000000e+00 5.05000000e-02 1.74074074e+00
 5.61111111e+00 2.61111111e+00 5.41666667e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000192 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 4.66666667e+00 6.15000000e-02 1.00000000e+00
 3.50000000e+00 2.00000000e+00 9.45000000e-04] with estimated performance 0.600589
INFO:solver.bo_hb:Start iteration 9 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 13.910248
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.296633
INFO:solver.bo_hb:Next candidate [6.66666667e-02 4.22222222e+00 5.05000000e-02 3.33333333e-01
 3.50000000e+00 5.00000000e-01 8.35000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.026666666666666672, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0038527198226775e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 1.261828e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.303475e+00 11.734069 61.611520
INFO:root:train 100 2.151538e+00 19.941213 71.379950
INFO:root:train 150 2.020095e+00 28.073262 76.872930
INFO:root:train 200 1.898765e+00 34.841418 80.425995
INFO:root:train 250 1.781629e+00 40.357321 83.117530
INFO:root:train 300 1.669645e+00 45.208679 85.205565
INFO:root:train 350 1.573452e+00 48.900463 86.769943
INFO:root:train 400 1.484978e+00 52.291147 88.080580
INFO:root:train 450 1.406499e+00 55.248753 89.111003
INFO:root:train 500 1.335476e+00 57.740768 89.970060
INFO:root:train 550 1.272198e+00 59.936479 90.701565
INFO:root:train 600 1.213308e+00 61.850042 91.358153
INFO:root:train 650 1.159579e+00 63.716878 91.906682
INFO:root:train 700 1.111746e+00 65.330777 92.394793
INFO:root:train_acc 66.714583
INFO:root:valid 000 4.584858e-01 85.937500 100.000000
INFO:root:valid 050 4.286292e-01 87.990196 98.958333
INFO:root:valid 100 4.290635e-01 88.134282 98.901609
INFO:root:valid 150 4.270859e-01 88.131209 98.934189
INFO:root:valid_acc 88.300000
INFO:root:epoch 1 lr 1.261828e-05
INFO:root:train 000 5.899552e-01 85.937500 100.000000
INFO:root:train 050 5.961143e-01 82.015931 98.100490
INFO:root:train 100 5.948369e-01 82.147277 97.957921
INFO:root:train 150 5.787420e-01 82.895281 98.002897
INFO:root:train 200 5.761592e-01 83.061256 97.994403
INFO:root:train 250 5.729960e-01 83.067729 98.007968
INFO:root:train 300 5.662178e-01 83.352367 98.089701
INFO:root:train 350 5.668930e-01 83.315527 98.068020
INFO:root:train 400 5.638407e-01 83.330736 98.079021
INFO:root:train 450 5.583795e-01 83.543514 98.143016
INFO:root:train 500 5.553402e-01 83.551647 98.187999
INFO:root:train 550 5.535445e-01 83.609347 98.216311
INFO:root:train 600 5.514240e-01 83.680844 98.221714
INFO:root:train 650 5.476611e-01 83.750960 98.252688
INFO:root:train 700 5.450754e-01 83.802158 98.277015
INFO:root:train_acc 83.872917
INFO:root:valid 000 3.721827e-01 89.062500 96.875000
INFO:root:valid 050 3.790882e-01 89.093137 99.203431
INFO:root:valid 100 3.849531e-01 89.263614 99.211015
INFO:root:valid 150 3.919173e-01 89.155629 99.089404
INFO:root:valid_acc 89.266667
INFO:solver.bo_hb:Configuration achieved a performance of 0.392190 
INFO:solver.bo_hb:Evaluation of this configuration took 83.724016 seconds
INFO:solver.bo_hb:Current incumbent [6.66666667e-02 4.22222222e+00 5.05000000e-02 0.00000000e+00
 3.50000000e+00 0.00000000e+00 8.35000000e-04] with estimated performance 0.392190
INFO:solver.bo_hb:Start iteration 10 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 13.918908
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.206441
INFO:solver.bo_hb:Next candidate [2.00000000e-01 6.59259259e+00 6.15000000e-02 3.70370370e-02
 3.16666667e+00 2.77777778e-01 2.83333333e-05]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 0.00132739445772974, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0001304883347975e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 1.327394e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.295357e+00 12.162990 62.009804
INFO:root:train 100 2.137386e+00 21.039604 71.905941
INFO:root:train 150 2.000980e+00 29.263245 77.411010
INFO:root:train 200 1.875413e+00 35.999689 80.931281
INFO:root:train 250 1.754665e+00 41.496514 83.603088
INFO:root:train 300 1.640270e+00 46.350706 85.662375
INFO:root:train 350 1.542816e+00 49.964387 87.175036
INFO:root:train 400 1.453478e+00 53.327618 88.450748
INFO:root:train 450 1.374743e+00 56.239606 89.433204
INFO:root:train 500 1.304007e+00 58.691991 90.266342
INFO:root:train 550 1.241339e+00 60.877949 90.982305
INFO:root:train 600 1.183005e+00 62.783382 91.623336
INFO:root:train 650 1.129836e+00 64.590534 92.163498
INFO:root:train 700 1.082660e+00 66.155492 92.633292
INFO:root:train_acc 67.520833
INFO:root:valid 000 4.501973e-01 84.375000 100.000000
INFO:root:valid 050 4.105696e-01 88.725490 99.019608
INFO:root:valid 100 4.113561e-01 88.768564 98.978960
INFO:root:valid 150 4.095353e-01 88.700331 99.037666
INFO:root:valid_acc 88.858333
INFO:root:epoch 1 lr 1.327394e-05
INFO:root:train 000 8.047407e-01 79.687500 98.437500
INFO:root:train 050 8.814248e-01 72.640931 95.863971
INFO:root:train 100 8.589406e-01 73.236386 96.055074
INFO:root:train 150 8.248582e-01 74.203228 96.347268
INFO:root:train 200 8.085068e-01 74.844527 96.455224
INFO:root:train 250 7.957395e-01 75.049801 96.545070
INFO:root:train 300 7.844073e-01 75.508721 96.605066
INFO:root:train 350 7.765339e-01 75.761218 96.688034
INFO:root:train 400 7.682043e-01 75.970231 96.734726
INFO:root:train 450 7.578132e-01 76.326912 96.791851
INFO:root:train 500 7.498917e-01 76.528194 96.940494
INFO:root:train 550 7.457027e-01 76.670259 96.954401
INFO:root:train 600 7.395555e-01 76.903078 97.002392
INFO:root:train 650 7.324179e-01 77.179339 97.057412
INFO:root:train 700 7.271809e-01 77.331491 97.088980
INFO:root:train_acc 77.535417
INFO:root:valid 000 3.885735e-01 89.062500 96.875000
INFO:root:valid 050 4.009965e-01 88.878676 99.019608
INFO:root:valid 100 4.058130e-01 89.031559 99.040842
INFO:root:valid 150 4.121210e-01 88.700331 98.934189
INFO:root:valid_acc 88.800000
INFO:solver.bo_hb:Configuration achieved a performance of 0.412838 
INFO:solver.bo_hb:Evaluation of this configuration took 83.563630 seconds
INFO:solver.bo_hb:Current incumbent [6.66666667e-02 4.22222222e+00 5.05000000e-02 0.00000000e+00
 3.50000000e+00 0.00000000e+00 8.35000000e-04] with estimated performance 0.392190
INFO:solver.bo_hb:Start iteration 11 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 13.997653
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.218960
INFO:solver.bo_hb:Next candidate [2.00000000e-01 4.66666667e+00 5.05000000e-02 1.14814815e+00
 3.61111111e+00 2.50000000e+00 2.85000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0013133351732858e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 6.309138e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.304012e+00 14.920343 59.926471
INFO:root:train 100 2.192366e+00 22.122525 69.678218
INFO:root:train 150 2.106850e+00 26.386589 74.596440
INFO:root:train 200 2.028792e+00 30.348259 77.798507
INFO:root:train 250 1.957206e+00 34.007719 80.104582
INFO:root:train 300 1.884508e+00 37.256022 82.096138
INFO:root:train 350 1.817055e+00 39.983974 83.551460
INFO:root:train 400 1.751971e+00 42.662874 84.854271
INFO:root:train 450 1.691178e+00 45.160061 85.888997
INFO:root:train 500 1.629419e+00 47.626622 86.798278
INFO:root:train 550 1.570469e+00 49.849705 87.616266
INFO:root:train 600 1.512558e+00 51.900478 88.394343
INFO:root:train 650 1.457281e+00 53.900250 89.084101
INFO:root:train 700 1.406159e+00 55.659326 89.686608
INFO:root:train_acc 57.245833
INFO:root:valid 000 6.179330e-01 76.562500 100.000000
INFO:root:valid 050 6.440190e-01 81.862745 97.977941
INFO:root:valid 100 6.452213e-01 81.899752 97.942450
INFO:root:valid 150 6.487966e-01 81.705298 98.064983
INFO:root:valid_acc 81.800000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 1.101383e+00 62.500000 98.437500
INFO:root:train 050 1.199854e+00 62.530637 93.290441
INFO:root:train 100 1.203223e+00 62.407178 93.270421
INFO:root:train 150 1.191276e+00 63.048427 93.501656
INFO:root:train 200 1.190756e+00 63.145211 93.586754
INFO:root:train 250 1.191176e+00 63.060259 93.519671
INFO:root:train 300 1.190730e+00 63.154070 93.474875
INFO:root:train 350 1.196066e+00 62.833868 93.425036
INFO:root:train 400 1.201072e+00 62.737687 93.364246
INFO:root:train 450 1.197457e+00 62.773697 93.382761
INFO:root:train 500 1.198066e+00 62.818114 93.341442
INFO:root:train 550 1.199113e+00 62.775068 93.352995
INFO:root:train 600 1.199429e+00 62.866577 93.323627
INFO:root:train 650 1.198513e+00 62.936828 93.289171
INFO:root:train 700 1.199275e+00 62.881152 93.299750
INFO:root:train_acc 62.972917
INFO:root:valid 000 6.394394e-01 85.937500 96.875000
INFO:root:valid 050 7.060798e-01 81.556373 97.671569
INFO:root:valid 100 7.143656e-01 81.017946 97.942450
INFO:root:valid 150 7.189907e-01 80.639487 97.878725
INFO:root:valid_acc 80.491667
INFO:solver.bo_hb:Configuration achieved a performance of 0.721120 
INFO:solver.bo_hb:Evaluation of this configuration took 74.644328 seconds
INFO:solver.bo_hb:Current incumbent [6.66666667e-02 4.22222222e+00 5.05000000e-02 0.00000000e+00
 3.50000000e+00 0.00000000e+00 8.35000000e-04] with estimated performance 0.392190
INFO:solver.bo_hb:Start iteration 12 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 13.982124
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.264025
INFO:solver.bo_hb:Next candidate [1.55555556e-01 4.22222222e+00 5.05000000e-02 1.96296296e+00
 3.83333333e+00 1.38888889e+00 9.81666667e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000193 seconds
INFO:solver.bo_hb:Current incumbent [6.66666667e-02 4.22222222e+00 5.05000000e-02 0.00000000e+00
 3.50000000e+00 0.00000000e+00 8.35000000e-04] with estimated performance 0.392190
INFO:solver.bo_hb:Start iteration 13 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 13.980411
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.284924
INFO:solver.bo_hb:Next candidate [2.22222222e-02 7.33333333e+00 5.05000000e-02 7.77777778e-01
 3.16666667e+00 1.66666667e-01 6.50000000e-05]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.008888888888888894, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0002993808675978e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 6.309138e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 1.847352e+00 37.959559 81.801471
INFO:root:train 100 1.460630e+00 53.171411 88.149752
INFO:root:train 150 1.199837e+00 62.034354 91.380381
INFO:root:train 200 1.026111e+00 67.700560 93.221393
INFO:root:train 250 8.995494e-01 71.850100 94.397410
INFO:root:train 300 8.012872e-01 74.958472 95.229444
INFO:root:train 350 7.278102e-01 77.368234 95.793269
INFO:root:train 400 6.653880e-01 79.403055 96.243766
INFO:root:train 450 6.158045e-01 80.931264 96.615161
INFO:root:train 500 5.735041e-01 82.319736 96.903069
INFO:root:train 550 5.389165e-01 83.376815 97.150068
INFO:root:train 600 5.082609e-01 84.304804 97.368968
INFO:root:train 650 4.813286e-01 85.133449 97.554243
INFO:root:train 700 4.589056e-01 85.834968 97.708631
INFO:root:train_acc 86.462500
INFO:root:valid 000 1.277625e-01 96.875000 100.000000
INFO:root:valid 050 1.480585e-01 94.914216 99.877451
INFO:root:valid 100 1.484154e-01 95.235149 99.798886
INFO:root:valid 150 1.497474e-01 95.405629 99.730960
INFO:root:valid_acc 95.558333
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 1.634917e-01 93.750000 100.000000
INFO:root:train 050 1.731087e-01 94.822304 99.601716
INFO:root:train 100 1.679722e-01 95.126856 99.582302
INFO:root:train 150 1.679591e-01 94.960679 99.658526
INFO:root:train 200 1.654139e-01 95.009328 99.681281
INFO:root:train 250 1.718337e-01 94.808267 99.638944
INFO:root:train 300 1.720195e-01 94.855689 99.631437
INFO:root:train 350 1.742009e-01 94.729345 99.648326
INFO:root:train 400 1.737357e-01 94.844919 99.653211
INFO:root:train 450 1.729755e-01 94.830931 99.670870
INFO:root:train 500 1.721754e-01 94.878992 99.653817
INFO:root:train 550 1.726416e-01 94.887137 99.654038
INFO:root:train 600 1.734921e-01 94.852329 99.656822
INFO:root:train 650 1.736509e-01 94.822869 99.668779
INFO:root:train 700 1.740092e-01 94.804297 99.672343
INFO:root:train_acc 94.785417
INFO:root:valid 000 1.030784e-01 96.875000 100.000000
INFO:root:valid 050 1.312300e-01 96.017157 99.724265
INFO:root:valid 100 1.327872e-01 95.993193 99.737005
INFO:root:valid 150 1.345389e-01 96.026490 99.751656
INFO:root:valid_acc 96.050000
INFO:solver.bo_hb:Configuration achieved a performance of 0.134926 
INFO:solver.bo_hb:Evaluation of this configuration took 83.811201 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 14 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.089155
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.205348
INFO:solver.bo_hb:Next candidate [3.77777778e-01 4.66666667e+00 5.05000000e-02 5.55555556e-01
 3.05555556e+00 1.50000000e+00 8.35000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.1511111111111111, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0038527198226775e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 6.309138e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.304005e+00 14.828431 59.865196
INFO:root:train 100 2.192252e+00 22.230817 69.647277
INFO:root:train 150 2.107021e+00 26.459023 74.544702
INFO:root:train 200 2.029150e+00 30.418221 77.736318
INFO:root:train 250 1.957644e+00 34.069970 80.073456
INFO:root:train 300 1.885122e+00 37.287168 82.054610
INFO:root:train 350 1.817861e+00 39.957265 83.538105
INFO:root:train 400 1.753015e+00 42.592737 84.838685
INFO:root:train 450 1.692490e+00 45.087306 85.868210
INFO:root:train 500 1.631014e+00 47.548653 86.798278
INFO:root:train 550 1.572419e+00 49.781647 87.619102
INFO:root:train 600 1.514787e+00 51.858881 88.391743
INFO:root:train 650 1.459619e+00 53.864247 89.084101
INFO:root:train 700 1.408608e+00 55.630350 89.686608
INFO:root:train_acc 57.208333
INFO:root:valid 000 6.123160e-01 78.125000 100.000000
INFO:root:valid 050 6.488285e-01 81.893382 97.977941
INFO:root:valid 100 6.502811e-01 81.899752 97.942450
INFO:root:valid 150 6.542152e-01 81.519040 98.044288
INFO:root:valid_acc 81.691667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 1.575281e+00 45.312500 90.625000
INFO:root:train 050 1.601049e+00 51.011029 89.246324
INFO:root:train 100 1.629357e+00 50.386757 89.000619
INFO:root:train 150 1.630692e+00 50.527732 89.021109
INFO:root:train 200 1.635254e+00 50.699627 88.743781
INFO:root:train 250 1.630969e+00 51.045817 88.751245
INFO:root:train 300 1.627546e+00 51.188746 88.943106
INFO:root:train 350 1.631217e+00 51.059473 88.853276
INFO:root:train 400 1.638018e+00 50.950748 88.754676
INFO:root:train 450 1.632316e+00 51.112112 88.833841
INFO:root:train 500 1.632258e+00 51.125873 88.837949
INFO:root:train 550 1.635711e+00 50.978335 88.832804
INFO:root:train 600 1.637081e+00 50.993136 88.792117
INFO:root:train 650 1.636711e+00 50.993664 88.755280
INFO:root:train 700 1.633925e+00 51.043153 88.777193
INFO:root:train_acc 51.177083
INFO:root:valid 000 8.181542e-01 81.250000 96.875000
INFO:root:valid 050 9.011802e-01 73.069853 97.028186
INFO:root:valid 100 9.080798e-01 72.308168 97.230817
INFO:root:valid 150 9.136022e-01 72.019868 97.009520
INFO:root:valid_acc 71.916667
INFO:solver.bo_hb:Configuration achieved a performance of 0.915387 
INFO:solver.bo_hb:Evaluation of this configuration took 74.694420 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 15 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.088777
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.281986
INFO:solver.bo_hb:Next candidate [1.11111111e-01 6.00000000e+00 2.83333333e-03 1.00000000e+00
 4.16666667e+00 5.55555556e-02 4.68333333e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.04444444444444445, 'grad_clip_value': 8, 'initial_lr': 0.0010131334785603088, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0021590821721555e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 5.065667e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 1.931666e+00 34.037990 79.564951
INFO:root:train 100 1.573243e+00 49.288366 86.571782
INFO:root:train 150 1.312437e+00 58.402318 90.138659
INFO:root:train 200 1.129032e+00 64.404540 92.179726
INFO:root:train 250 9.944912e-01 68.781125 93.507221
INFO:root:train 300 8.876301e-01 72.321429 94.461171
INFO:root:train 350 8.073991e-01 74.924323 95.125534
INFO:root:train 400 7.384197e-01 77.135287 95.674875
INFO:root:train 450 6.847085e-01 78.876802 96.098947
INFO:root:train 500 6.380822e-01 80.407934 96.425898
INFO:root:train 550 5.999111e-01 81.584619 96.704855
INFO:root:train 600 5.656241e-01 82.625312 96.960795
INFO:root:train 650 5.353074e-01 83.568548 97.177419
INFO:root:train 700 5.098074e-01 84.359397 97.360913
INFO:root:train_acc 85.031250
INFO:root:valid 000 1.523236e-01 93.750000 100.000000
INFO:root:valid 050 1.639848e-01 94.546569 99.816176
INFO:root:valid 100 1.623539e-01 95.003094 99.737005
INFO:root:valid 150 1.643213e-01 95.012417 99.679222
INFO:root:valid_acc 95.175000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.766045e-01 92.187500 100.000000
INFO:root:train 050 4.079390e-01 88.082108 98.468137
INFO:root:train 100 4.062117e-01 88.242574 98.638614
INFO:root:train 150 3.954883e-01 88.524421 98.716887
INFO:root:train 200 3.996281e-01 88.339552 98.725124
INFO:root:train 250 4.079454e-01 88.116285 98.692729
INFO:root:train 300 4.068742e-01 88.216362 98.723007
INFO:root:train 350 4.106202e-01 88.092058 98.762464
INFO:root:train 400 4.129047e-01 88.049408 98.757014
INFO:root:train 450 4.102893e-01 88.071646 98.773559
INFO:root:train 500 4.083966e-01 88.198603 98.789920
INFO:root:train 550 4.089156e-01 88.169238 98.777790
INFO:root:train 600 4.083631e-01 88.199355 98.780678
INFO:root:train 650 4.094860e-01 88.172043 98.787922
INFO:root:train 700 4.103523e-01 88.155314 98.780760
INFO:root:train_acc 88.127083
INFO:root:valid 000 1.370248e-01 93.750000 100.000000
INFO:root:valid 050 1.527892e-01 95.404412 99.693627
INFO:root:valid 100 1.549453e-01 95.343441 99.721535
INFO:root:valid 150 1.563934e-01 95.250414 99.710265
INFO:root:valid_acc 95.241667
INFO:solver.bo_hb:Configuration achieved a performance of 0.157059 
INFO:solver.bo_hb:Evaluation of this configuration took 83.745015 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 16 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.109030
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.292349
INFO:solver.bo_hb:Next candidate [1.11111111e-01 6.00000000e+00 1.75000000e-02 5.55555556e-01
 3.16666667e+00 1.66666667e-01 9.45000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.04444444444444445, 'grad_clip_value': 8, 'initial_lr': 0.0010839269140212034, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0043613690324998e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 5.419635e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 1.907913e+00 35.355392 80.269608
INFO:root:train 100 1.540093e+00 50.479579 87.066832
INFO:root:train 150 1.278779e+00 59.581954 90.511175
INFO:root:train 200 1.097857e+00 65.477301 92.498445
INFO:root:train 250 9.651059e-01 69.889193 93.750000
INFO:root:train 300 8.605413e-01 73.209095 94.668812
INFO:root:train 350 7.824658e-01 75.765670 95.299145
INFO:root:train 400 7.164519e-01 77.828865 95.815150
INFO:root:train 450 6.644619e-01 79.476164 96.227134
INFO:root:train 500 6.192721e-01 80.981786 96.553767
INFO:root:train 550 5.815016e-01 82.177291 96.838135
INFO:root:train 600 5.479953e-01 83.189476 97.080387
INFO:root:train 650 5.189335e-01 84.070180 97.287826
INFO:root:train 700 4.946143e-01 84.802960 97.470132
INFO:root:train_acc 85.443750
INFO:root:valid 000 1.366833e-01 96.875000 100.000000
INFO:root:valid 050 1.636129e-01 94.669118 99.846814
INFO:root:valid 100 1.597232e-01 95.235149 99.783416
INFO:root:valid 150 1.588502e-01 95.260762 99.710265
INFO:root:valid_acc 95.450000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.544313e-01 90.625000 100.000000
INFO:root:train 050 4.054772e-01 88.265931 98.406863
INFO:root:train 100 3.960644e-01 88.242574 98.623144
INFO:root:train 150 3.868354e-01 88.627897 98.747930
INFO:root:train 200 3.913571e-01 88.557214 98.740672
INFO:root:train 250 3.996453e-01 88.315488 98.667829
INFO:root:train 300 3.981737e-01 88.366902 98.717816
INFO:root:train 350 4.020744e-01 88.314637 98.726852
INFO:root:train 400 4.042290e-01 88.252026 98.749221
INFO:root:train 450 4.022300e-01 88.272589 98.745843
INFO:root:train 500 4.004608e-01 88.345185 98.758733
INFO:root:train 550 4.014204e-01 88.291175 98.738090
INFO:root:train 600 4.006118e-01 88.324147 98.749480
INFO:root:train 650 4.016963e-01 88.323253 98.756720
INFO:root:train 700 4.022710e-01 88.318028 98.747325
INFO:root:train_acc 88.331250
INFO:root:valid 000 1.521667e-01 93.750000 100.000000
INFO:root:valid 050 1.475944e-01 95.863971 99.785539
INFO:root:valid 100 1.495395e-01 95.575495 99.737005
INFO:root:valid 150 1.500811e-01 95.540149 99.751656
INFO:root:valid_acc 95.583333
INFO:solver.bo_hb:Configuration achieved a performance of 0.149754 
INFO:solver.bo_hb:Evaluation of this configuration took 86.306929 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 17 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.728294
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.302322
INFO:solver.bo_hb:Next candidate [1.95061728e-01 5.95061728e+00 5.05000000e-02 7.77777778e-01
 3.01851852e+00 1.50000000e+00 5.66111111e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.0780246913580247, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0026104392895522e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 6.309138e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.303991e+00 14.920343 59.773284
INFO:root:train 100 2.192218e+00 22.215347 69.585396
INFO:root:train 150 2.106816e+00 26.438328 74.482616
INFO:root:train 200 2.028795e+00 30.356032 77.689677
INFO:root:train 250 1.957320e+00 33.964143 80.048556
INFO:root:train 300 1.884683e+00 37.204111 82.039037
INFO:root:train 350 1.817480e+00 39.930556 83.533654
INFO:root:train 400 1.752473e+00 42.545979 84.834788
INFO:root:train 450 1.691821e+00 45.035338 85.875139
INFO:root:train 500 1.630577e+00 47.470684 86.760853
INFO:root:train 550 1.572162e+00 49.707917 87.590744
INFO:root:train 600 1.514621e+00 51.767887 88.370944
INFO:root:train 650 1.459543e+00 53.775442 89.067300
INFO:root:train 700 1.408435e+00 55.541191 89.662090
INFO:root:train_acc 57.108333
INFO:root:valid 000 5.929795e-01 79.687500 100.000000
INFO:root:valid 050 6.365715e-01 81.832108 97.916667
INFO:root:valid 100 6.392628e-01 81.961634 97.896040
INFO:root:valid 150 6.427207e-01 81.715646 98.054636
INFO:root:valid_acc 81.925000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 1.057093e+00 64.062500 98.437500
INFO:root:train 050 1.195921e+00 63.112745 93.321078
INFO:root:train 100 1.196436e+00 62.964109 93.316832
INFO:root:train 150 1.184864e+00 63.296772 93.522351
INFO:root:train 200 1.182807e+00 63.355100 93.633396
INFO:root:train 250 1.183895e+00 63.284363 93.538347
INFO:root:train 300 1.183584e+00 63.382475 93.474875
INFO:root:train 350 1.189263e+00 63.109865 93.398326
INFO:root:train 400 1.194604e+00 63.049408 93.340867
INFO:root:train 450 1.191600e+00 63.088969 93.393154
INFO:root:train 500 1.192580e+00 63.123752 93.381986
INFO:root:train 550 1.193890e+00 63.067151 93.358666
INFO:root:train 600 1.193883e+00 63.178557 93.318428
INFO:root:train 650 1.193100e+00 63.224846 93.284370
INFO:root:train 700 1.193707e+00 63.148627 93.306437
INFO:root:train_acc 63.210417
INFO:root:valid 000 6.491376e-01 85.937500 96.875000
INFO:root:valid 050 7.050865e-01 81.188725 97.794118
INFO:root:valid 100 7.125301e-01 80.956064 98.066213
INFO:root:valid 150 7.177644e-01 80.784354 97.961507
INFO:root:valid_acc 80.641667
INFO:solver.bo_hb:Configuration achieved a performance of 0.719656 
INFO:solver.bo_hb:Evaluation of this configuration took 76.896565 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 18 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.886945
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.304392
INFO:solver.bo_hb:Next candidate [1.25925926e-01 6.00000000e+00 5.05000000e-02 1.11111111e-01
 4.50000000e+00 2.94444444e+00 6.50000000e-05]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000196 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 19 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([19])) that is different to the input size (torch.Size([19, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.833451
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.295192
INFO:solver.bo_hb:Next candidate [2.00000000e-01 7.77777778e+00 5.05000000e-02 1.66666667e+00
 4.16666667e+00 1.50000000e+00 9.45000000e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000196 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 20 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.875547
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.308593
INFO:solver.bo_hb:Next candidate [2.44444444e-01 7.33333333e+00 5.05000000e-02 1.11111111e-01
 4.50000000e+00 8.33333333e-01 1.75000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.0977777777777778, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'adad', 'weight_decay': 1.0008062296110608e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 1.261828e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.485907e+00 9.466912 49.846814
INFO:root:train 100 2.477301e+00 10.009282 49.737005
INFO:root:train 150 2.478119e+00 10.037252 49.679222
INFO:root:train 200 2.479974e+00 9.880286 49.292600
INFO:root:train 250 2.479217e+00 9.804532 49.377490
INFO:root:train 300 2.478270e+00 9.831811 49.304402
INFO:root:train 350 2.478438e+00 9.815705 49.301104
INFO:root:train 400 2.477753e+00 9.799719 49.392145
INFO:root:train 450 2.475104e+00 9.846175 49.553076
INFO:root:train 500 2.473891e+00 9.836577 49.628867
INFO:root:train 550 2.472415e+00 9.828721 49.773140
INFO:root:train 600 2.473571e+00 9.793573 49.701019
INFO:root:train 650 2.474431e+00 9.706221 49.596774
INFO:root:train 700 2.474214e+00 9.675909 49.598787
INFO:root:train_acc 9.656250
INFO:root:valid 000 2.366658e+00 14.062500 54.687500
INFO:root:valid 050 2.462010e+00 9.313725 50.091912
INFO:root:valid 100 2.457902e+00 9.653465 50.417698
INFO:root:valid 150 2.459656e+00 9.261175 50.320778
INFO:root:valid_acc 9.333333
INFO:root:epoch 1 lr 1.261828e-05
INFO:root:train 000 2.512466e+00 9.375000 50.000000
INFO:root:train 050 2.490174e+00 10.049020 48.958333
INFO:root:train 100 2.486366e+00 9.483292 49.953589
INFO:root:train 150 2.487467e+00 9.519868 50.031043
INFO:root:train 200 2.488517e+00 9.437189 49.828980
INFO:root:train 250 2.484963e+00 9.387450 50.242779
INFO:root:train 300 2.484974e+00 9.437292 50.119394
INFO:root:train 350 2.484368e+00 9.419516 50.133547
INFO:root:train 400 2.482429e+00 9.519171 50.292238
INFO:root:train 450 2.482571e+00 9.444290 50.408814
INFO:root:train 500 2.484288e+00 9.431138 50.283807
INFO:root:train 550 2.482095e+00 9.553652 50.371484
INFO:root:train 600 2.480395e+00 9.634983 50.475770
INFO:root:train 650 2.479422e+00 9.682220 50.549635
INFO:root:train 700 2.479255e+00 9.660307 50.550553
INFO:root:train_acc 9.662500
INFO:root:valid 000 2.538476e+00 6.250000 45.312500
INFO:root:valid 050 2.472513e+00 9.160539 50.459559
INFO:root:valid 100 2.467311e+00 9.498762 50.897277
INFO:root:valid 150 2.468294e+00 9.426738 51.014073
INFO:root:valid_acc 9.708333
INFO:solver.bo_hb:Configuration achieved a performance of 2.467302 
INFO:solver.bo_hb:Evaluation of this configuration took 95.981921 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 21 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([21])) that is different to the input size (torch.Size([21, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.883640
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.301610
INFO:solver.bo_hb:Next candidate [3.33333333e-01 6.00000000e+00 5.05000000e-02 1.11111111e-01
 3.50000000e+00 1.83333333e+00 6.15000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.13333333333333333, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0028361940741568e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 1.261828e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.427893e+00 9.558824 51.164216
INFO:root:train 100 2.370863e+00 10.659035 54.084158
INFO:root:train 150 2.335401e+00 11.516970 56.301738
INFO:root:train 200 2.305099e+00 13.331779 58.729789
INFO:root:train 250 2.277790e+00 15.344871 61.510209
INFO:root:train 300 2.253065e+00 17.441860 63.995017
INFO:root:train 350 2.229448e+00 19.266382 66.288283
INFO:root:train 400 2.208153e+00 20.846322 68.270729
INFO:root:train 450 2.188699e+00 22.048226 69.966048
INFO:root:train 500 2.168405e+00 23.253493 71.407186
INFO:root:train 550 2.148975e+00 24.319419 72.657668
INFO:root:train 600 2.128991e+00 25.288582 73.767679
INFO:root:train 650 2.110123e+00 26.231279 74.759985
INFO:root:train 700 2.092055e+00 27.168777 75.655314
INFO:root:train_acc 28.006250
INFO:root:valid 000 1.801071e+00 48.437500 85.937500
INFO:root:valid 050 1.796832e+00 43.688725 87.683824
INFO:root:valid 100 1.805053e+00 42.806312 88.118812
INFO:root:valid 150 1.809360e+00 42.435844 87.903560
INFO:root:valid_acc 42.433333
INFO:root:epoch 1 lr 1.261828e-05
INFO:root:train 000 2.028967e+00 28.125000 78.125000
INFO:root:train 050 1.994191e+00 29.993873 78.094363
INFO:root:train 100 1.995859e+00 29.594678 78.496287
INFO:root:train 150 1.995986e+00 29.770281 78.725166
INFO:root:train 200 1.998179e+00 29.601990 78.606965
INFO:root:train 250 1.999991e+00 29.569223 78.554532
INFO:root:train 300 1.999066e+00 29.812085 78.519518
INFO:root:train 350 1.999001e+00 29.874466 78.485577
INFO:root:train 400 1.999900e+00 29.901808 78.428928
INFO:root:train 450 1.995935e+00 30.172533 78.585782
INFO:root:train 500 1.993824e+00 30.326846 78.639596
INFO:root:train 550 1.994249e+00 30.285844 78.612750
INFO:root:train 600 1.992712e+00 30.376456 78.696963
INFO:root:train 650 1.990665e+00 30.546755 78.811444
INFO:root:train 700 1.989965e+00 30.621434 78.815977
INFO:root:train_acc 30.797917
INFO:root:valid 000 1.835952e+00 45.312500 84.375000
INFO:root:valid 050 1.888903e+00 38.296569 86.458333
INFO:root:valid 100 1.897474e+00 37.035891 86.092203
INFO:root:valid 150 1.897866e+00 37.117136 85.947848
INFO:root:valid_acc 37.016667
INFO:solver.bo_hb:Configuration achieved a performance of 1.899078 
INFO:solver.bo_hb:Evaluation of this configuration took 76.890805 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 22 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([22])) that is different to the input size (torch.Size([22, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.937222
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.296522
INFO:solver.bo_hb:Next candidate [2.00000000e-01 6.00000000e+00 5.05000000e-02 1.11111111e-01
 4.83333333e+00 1.72222222e+00 9.45000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0043613690324998e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 1.261828e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.427893e+00 9.558824 51.164216
INFO:root:train 100 2.370862e+00 10.659035 54.084158
INFO:root:train 150 2.335405e+00 11.516970 56.312086
INFO:root:train 200 2.305092e+00 13.339552 58.753109
INFO:root:train 250 2.277774e+00 15.344871 61.535110
INFO:root:train 300 2.253022e+00 17.431478 64.026163
INFO:root:train 350 2.229391e+00 19.230769 66.306090
INFO:root:train 400 2.208090e+00 20.811253 68.286315
INFO:root:train 450 2.188642e+00 22.017045 69.976441
INFO:root:train 500 2.168346e+00 23.237899 71.397829
INFO:root:train 550 2.148925e+00 24.296733 72.657668
INFO:root:train 600 2.128942e+00 25.272983 73.759879
INFO:root:train 650 2.110083e+00 26.224078 74.750384
INFO:root:train 700 2.092027e+00 27.164319 75.646398
INFO:root:train_acc 28.008333
INFO:root:valid 000 1.800106e+00 48.437500 85.937500
INFO:root:valid 050 1.796759e+00 43.596814 87.500000
INFO:root:valid 100 1.804976e+00 42.636139 88.072401
INFO:root:valid 150 1.809342e+00 42.290977 87.820778
INFO:root:valid_acc 42.316667
INFO:root:epoch 1 lr 1.261828e-05
INFO:root:train 000 1.930560e+00 34.375000 81.250000
INFO:root:train 050 1.925039e+00 33.639706 81.556373
INFO:root:train 100 1.928363e+00 33.740718 81.543936
INFO:root:train 150 1.928099e+00 33.557533 81.581126
INFO:root:train 200 1.927760e+00 33.550995 81.654229
INFO:root:train 250 1.929752e+00 33.503486 81.579930
INFO:root:train 300 1.928938e+00 33.783223 81.608181
INFO:root:train 350 1.929631e+00 33.649395 81.588319
INFO:root:train 400 1.931235e+00 33.642456 81.561721
INFO:root:train 450 1.927653e+00 33.907289 81.683065
INFO:root:train 500 1.925801e+00 33.991392 81.802021
INFO:root:train 550 1.926809e+00 33.932623 81.819986
INFO:root:train 600 1.925097e+00 34.109817 81.936356
INFO:root:train 650 1.923387e+00 34.274194 82.058852
INFO:root:train 700 1.922963e+00 34.348252 82.074715
INFO:root:train_acc 34.462500
INFO:root:valid 000 1.779408e+00 50.000000 85.937500
INFO:root:valid 050 1.831097e+00 42.892157 88.143382
INFO:root:valid 100 1.841684e+00 41.321163 87.917698
INFO:root:valid 150 1.842168e+00 41.287252 87.737997
INFO:root:valid_acc 41.416667
INFO:solver.bo_hb:Configuration achieved a performance of 1.843799 
INFO:solver.bo_hb:Evaluation of this configuration took 76.703561 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 23 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([23])) that is different to the input size (torch.Size([23, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.017155
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.310282
INFO:solver.bo_hb:Next candidate [2.00000000e-01 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.50000000e+00 1.50000000e+00 6.15000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0028361940741568e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 6.309138e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.303955e+00 14.828431 59.957108
INFO:root:train 100 2.192057e+00 22.184406 69.755569
INFO:root:train 150 2.106565e+00 26.541805 74.596440
INFO:root:train 200 2.028482e+00 30.534826 77.767413
INFO:root:train 250 1.956786e+00 34.169572 80.117032
INFO:root:train 300 1.884040e+00 37.390988 82.106520
INFO:root:train 350 1.816483e+00 40.121973 83.595976
INFO:root:train 400 1.751236e+00 42.744701 84.904925
INFO:root:train 450 1.690093e+00 45.250139 85.940965
INFO:root:train 500 1.628328e+00 47.723303 86.851297
INFO:root:train 550 1.569390e+00 49.943285 87.667309
INFO:root:train 600 1.511480e+00 51.973274 88.433340
INFO:root:train 650 1.456247e+00 53.967454 89.127304
INFO:root:train 700 1.405207e+00 55.723966 89.728959
INFO:root:train_acc 57.264583
INFO:root:valid 000 5.888231e-01 79.687500 100.000000
INFO:root:valid 050 6.356401e-01 82.138480 97.947304
INFO:root:valid 100 6.386679e-01 82.038985 97.865099
INFO:root:valid 150 6.426020e-01 81.891556 98.023593
INFO:root:valid_acc 82.083333
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 1.097130e+00 65.625000 98.437500
INFO:root:train 050 1.197086e+00 62.683824 93.075980
INFO:root:train 100 1.200472e+00 62.685644 93.146658
INFO:root:train 150 1.188342e+00 63.286424 93.408526
INFO:root:train 200 1.186741e+00 63.409515 93.477923
INFO:root:train 250 1.187236e+00 63.271912 93.420070
INFO:root:train 300 1.186526e+00 63.418812 93.428156
INFO:root:train 350 1.191986e+00 63.078704 93.384972
INFO:root:train 400 1.197385e+00 62.955892 93.333074
INFO:root:train 450 1.193873e+00 62.939994 93.379296
INFO:root:train 500 1.194751e+00 62.999002 93.350798
INFO:root:train 550 1.195824e+00 62.922527 93.355830
INFO:root:train 600 1.196268e+00 63.006968 93.305428
INFO:root:train 650 1.195408e+00 63.068836 93.267569
INFO:root:train 700 1.196034e+00 63.010432 93.275232
INFO:root:train_acc 63.093750
INFO:root:valid 000 6.395097e-01 85.937500 96.875000
INFO:root:valid 050 7.040702e-01 81.740196 97.763480
INFO:root:valid 100 7.118564e-01 81.079827 97.973391
INFO:root:valid 150 7.166215e-01 80.846440 97.899421
INFO:root:valid_acc 80.800000
INFO:solver.bo_hb:Configuration achieved a performance of 0.719083 
INFO:solver.bo_hb:Evaluation of this configuration took 76.639322 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 24 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.038499
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.308362
INFO:solver.bo_hb:Next candidate [3.33333333e-01 4.22222222e+00 1.75000000e-02 5.55555556e-01
 4.50000000e+00 1.83333333e+00 1.75000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.13333333333333333, 'grad_clip_value': 8, 'initial_lr': 0.0010839269140212034, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0008062296110608e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 5.419635e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.319273e+00 13.602941 58.149510
INFO:root:train 100 2.215624e+00 20.683787 67.899134
INFO:root:train 150 2.138123e+00 24.989652 72.826987
INFO:root:train 200 2.067144e+00 28.498134 76.220460
INFO:root:train 250 2.002754e+00 31.804034 78.635458
INFO:root:train 300 1.938874e+00 34.743563 80.694560
INFO:root:train 350 1.879316e+00 37.295228 82.202635
INFO:root:train 400 1.822181e+00 39.748286 83.545044
INFO:root:train 450 1.769158e+00 42.003880 84.551691
INFO:root:train 500 1.714510e+00 44.220933 85.497754
INFO:root:train 550 1.661465e+00 46.353221 86.340177
INFO:root:train 600 1.608541e+00 48.323107 87.115225
INFO:root:train 650 1.557647e+00 50.266417 87.826421
INFO:root:train 700 1.509233e+00 52.001605 88.442850
INFO:root:train_acc 53.570833
INFO:root:valid 000 7.092021e-01 78.125000 98.437500
INFO:root:valid 050 7.631430e-01 78.860294 97.395833
INFO:root:valid 100 7.652236e-01 78.975866 97.168936
INFO:root:valid 150 7.711231e-01 78.435430 97.330298
INFO:root:valid_acc 78.500000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 1.630215e+00 45.312500 90.625000
INFO:root:train 050 1.542282e+00 51.256127 89.399510
INFO:root:train 100 1.553789e+00 51.160272 89.433787
INFO:root:train 150 1.557218e+00 51.076159 89.197020
INFO:root:train 200 1.562856e+00 51.127177 89.031405
INFO:root:train 250 1.564349e+00 51.095618 88.975349
INFO:root:train 300 1.561866e+00 51.328904 89.046927
INFO:root:train 350 1.566112e+00 51.144053 88.964566
INFO:root:train 400 1.572814e+00 51.013092 88.797537
INFO:root:train 450 1.570257e+00 51.115576 88.830377
INFO:root:train 500 1.569935e+00 51.100923 88.897206
INFO:root:train 550 1.572447e+00 50.995349 88.878176
INFO:root:train 600 1.572718e+00 51.016535 88.815516
INFO:root:train 650 1.571609e+00 51.082469 88.824885
INFO:root:train 700 1.570986e+00 51.085503 88.857436
INFO:root:train_acc 51.208333
INFO:root:valid 000 8.797604e-01 79.687500 95.312500
INFO:root:valid 050 9.692729e-01 71.599265 96.476716
INFO:root:valid 100 9.767412e-01 71.287129 96.720297
INFO:root:valid 150 9.811937e-01 71.181705 96.533526
INFO:root:valid_acc 71.050000
INFO:solver.bo_hb:Configuration achieved a performance of 0.982595 
INFO:solver.bo_hb:Evaluation of this configuration took 76.685380 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 25 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([25])) that is different to the input size (torch.Size([25, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.955788
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.341938
INFO:solver.bo_hb:Next candidate [2.00000000e-01 6.44444444e+00 2.85000000e-02 1.11111111e-01
 3.50000000e+00 1.50000000e+00 6.15000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 0.001140249787561169, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0028361940741568e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 1.140250e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.432675e+00 9.405637 51.011029
INFO:root:train 100 2.378410e+00 10.457921 53.743812
INFO:root:train 150 2.344643e+00 11.020281 55.815397
INFO:root:train 200 2.315779e+00 12.461132 57.843595
INFO:root:train 250 2.289682e+00 14.361305 60.240289
INFO:root:train 300 2.266276e+00 16.408846 62.660922
INFO:root:train 350 2.244220e+00 18.206909 64.890491
INFO:root:train 400 2.224343e+00 19.774782 66.809539
INFO:root:train 450 2.206099e+00 21.026192 68.528271
INFO:root:train 500 2.187069e+00 22.227420 69.991267
INFO:root:train 550 2.168841e+00 23.227654 71.307849
INFO:root:train 600 2.150152e+00 24.191452 72.470362
INFO:root:train 650 2.132432e+00 25.067204 73.533506
INFO:root:train 700 2.115407e+00 25.947307 74.451676
INFO:root:train_acc 26.718750
INFO:root:valid 000 1.853723e+00 43.750000 85.937500
INFO:root:valid 050 1.835740e+00 41.299020 87.101716
INFO:root:valid 100 1.844359e+00 40.671411 87.530941
INFO:root:valid 150 1.848717e+00 40.428394 87.324089
INFO:root:valid_acc 40.391667
INFO:root:epoch 1 lr 1.140250e-05
INFO:root:train 000 1.955045e+00 29.687500 81.250000
INFO:root:train 050 1.953146e+00 32.414216 80.821078
INFO:root:train 100 1.956387e+00 32.255569 81.048886
INFO:root:train 150 1.956254e+00 32.243377 81.074089
INFO:root:train 200 1.956208e+00 32.252799 81.102301
INFO:root:train 250 1.958286e+00 32.246016 80.969871
INFO:root:train 300 1.957660e+00 32.537375 80.985257
INFO:root:train 350 1.958280e+00 32.362892 80.982906
INFO:root:train 400 1.959719e+00 32.317643 80.918797
INFO:root:train 450 1.956559e+00 32.601164 81.052522
INFO:root:train 500 1.954947e+00 32.675274 81.137725
INFO:root:train 550 1.955936e+00 32.585640 81.096869
INFO:root:train 600 1.954384e+00 32.685108 81.239601
INFO:root:train 650 1.952790e+00 32.802899 81.334005
INFO:root:train 700 1.952464e+00 32.859308 81.339158
INFO:root:train_acc 32.929167
INFO:root:valid 000 1.824227e+00 48.437500 84.375000
INFO:root:valid 050 1.869860e+00 40.533088 87.622549
INFO:root:valid 100 1.879390e+00 39.263614 87.128713
INFO:root:valid 150 1.879618e+00 39.331540 86.961921
INFO:root:valid_acc 39.225000
INFO:solver.bo_hb:Configuration achieved a performance of 1.881170 
INFO:solver.bo_hb:Evaluation of this configuration took 76.918304 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 26 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([26])) that is different to the input size (torch.Size([26, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.001373
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.345232
INFO:solver.bo_hb:Next candidate [3.77777778e-01 5.55555556e+00 3.95000000e-02 7.77777778e-01
 3.50000000e+00 5.00000000e-01 3.95000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.1511111111111111, 'grad_clip_value': 8, 'initial_lr': 0.0011994993031493794, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0018206976844024e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 5.997497e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 1.868034e+00 36.703431 81.433824
INFO:root:train 100 1.488945e+00 52.150371 87.747525
INFO:root:train 150 1.228058e+00 61.154801 91.080298
INFO:root:train 200 1.052690e+00 66.783271 92.964863
INFO:root:train 250 9.244974e-01 71.015936 94.142181
INFO:root:train 300 8.239029e-01 74.247301 95.021802
INFO:root:train 350 7.491579e-01 76.647080 95.624110
INFO:root:train 400 6.852452e-01 78.705580 96.099595
INFO:root:train 450 6.347428e-01 80.262611 96.483509
INFO:root:train 500 5.913104e-01 81.720933 96.787675
INFO:root:train 550 5.557046e-01 82.860708 97.056488
INFO:root:train 600 5.238406e-01 83.844634 97.288374
INFO:root:train 650 4.962204e-01 84.699021 97.467838
INFO:root:train 700 4.728910e-01 85.440442 97.626159
INFO:root:train_acc 86.081250
INFO:root:valid 000 1.535052e-01 95.312500 100.000000
INFO:root:valid 050 1.519252e-01 94.975490 99.908088
INFO:root:valid 100 1.521246e-01 95.389851 99.783416
INFO:root:valid 150 1.525131e-01 95.436672 99.782699
INFO:root:valid_acc 95.525000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 9.865302e-01 64.062500 96.875000
INFO:root:train 050 1.076798e+00 71.721814 95.098039
INFO:root:train 100 1.102094e+00 71.147896 95.204208
INFO:root:train 150 1.096123e+00 71.367964 95.343543
INFO:root:train 200 1.095783e+00 71.323072 95.374689
INFO:root:train 250 1.094897e+00 71.240040 95.362301
INFO:root:train 300 1.096219e+00 71.169020 95.369601
INFO:root:train 350 1.091951e+00 71.220620 95.383725
INFO:root:train 400 1.098898e+00 71.025561 95.293017
INFO:root:train 450 1.098141e+00 71.071231 95.295177
INFO:root:train 500 1.095538e+00 71.213822 95.306262
INFO:root:train 550 1.098272e+00 71.137704 95.292650
INFO:root:train 600 1.102385e+00 71.110649 95.244904
INFO:root:train 650 1.107405e+00 71.078149 95.233295
INFO:root:train 700 1.105786e+00 71.112696 95.216655
INFO:root:train_acc 71.081250
INFO:root:valid 000 1.575707e-01 93.750000 100.000000
INFO:root:valid 050 1.713258e-01 95.128676 99.632353
INFO:root:valid 100 1.722236e-01 95.142327 99.675124
INFO:root:valid 150 1.729029e-01 94.981374 99.699917
INFO:root:valid_acc 94.991667
INFO:solver.bo_hb:Configuration achieved a performance of 0.172760 
INFO:solver.bo_hb:Evaluation of this configuration took 86.289966 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 27 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.103230
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.319220
INFO:solver.bo_hb:Next candidate [3.77777778e-01 7.33333333e+00 1.75000000e-02 1.66666667e+00
 5.50000000e+00 1.50000000e+00 5.05000000e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000206 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 28 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.034396
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.297771
INFO:solver.bo_hb:Next candidate [6.66666667e-02 6.00000000e+00 5.05000000e-02 3.33333333e-01
 4.50000000e+00 5.00000000e-01 8.35000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.026666666666666672, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0038527198226775e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 1.261828e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.303464e+00 11.734069 61.550245
INFO:root:train 100 2.151462e+00 19.972153 71.364480
INFO:root:train 150 2.019977e+00 28.114652 76.883278
INFO:root:train 200 1.898652e+00 34.895833 80.418221
INFO:root:train 250 1.781408e+00 40.394671 83.117530
INFO:root:train 300 1.669264e+00 45.219061 85.195183
INFO:root:train 350 1.573063e+00 48.900463 86.761040
INFO:root:train 400 1.484580e+00 52.310630 88.068890
INFO:root:train 450 1.406078e+00 55.255682 89.097145
INFO:root:train 500 1.335061e+00 57.731412 89.960704
INFO:root:train 550 1.271837e+00 59.939315 90.693058
INFO:root:train 600 1.213018e+00 61.863041 91.350354
INFO:root:train 650 1.159340e+00 63.721678 91.897081
INFO:root:train 700 1.111514e+00 65.333006 92.385877
INFO:root:train_acc 66.722917
INFO:root:valid 000 4.583443e-01 85.937500 100.000000
INFO:root:valid 050 4.300074e-01 87.867647 98.927696
INFO:root:valid 100 4.299189e-01 88.072401 98.917079
INFO:root:valid 150 4.276659e-01 88.069123 98.954884
INFO:root:valid_acc 88.258333
INFO:root:epoch 1 lr 1.261828e-05
INFO:root:train 000 5.877010e-01 85.937500 100.000000
INFO:root:train 050 5.951636e-01 82.199755 98.069853
INFO:root:train 100 5.938053e-01 82.271040 97.973391
INFO:root:train 150 5.776705e-01 82.988411 98.002897
INFO:root:train 200 5.750122e-01 83.100124 97.986629
INFO:root:train 250 5.719879e-01 83.098855 97.995518
INFO:root:train 300 5.652168e-01 83.404277 98.074128
INFO:root:train 350 5.659771e-01 83.364494 98.036859
INFO:root:train 400 5.629932e-01 83.393080 98.043953
INFO:root:train 450 5.575366e-01 83.605876 98.115299
INFO:root:train 500 5.545563e-01 83.614022 98.159930
INFO:root:train 550 5.527848e-01 83.683076 98.190789
INFO:root:train 600 5.506717e-01 83.764039 98.198315
INFO:root:train 650 5.468959e-01 83.839766 98.233487
INFO:root:train 700 5.443450e-01 83.895774 98.259183
INFO:root:train_acc 83.966667
INFO:root:valid 000 3.716944e-01 89.062500 96.875000
INFO:root:valid 050 3.788172e-01 89.185049 99.234069
INFO:root:valid 100 3.848661e-01 89.232673 99.211015
INFO:root:valid 150 3.917941e-01 89.114238 99.099752
INFO:root:valid_acc 89.266667
INFO:solver.bo_hb:Configuration achieved a performance of 0.392068 
INFO:solver.bo_hb:Evaluation of this configuration took 85.823867 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 29 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.000734
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.207702
INFO:solver.bo_hb:Next candidate [3.33333333e-01 6.00000000e+00 5.05000000e-02 3.33333333e-01
 5.50000000e+00 8.33333333e-01 8.35000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.13333333333333333, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'adad', 'weight_decay': 1.0038527198226775e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 1.261828e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.485907e+00 9.466912 49.846814
INFO:root:train 100 2.477301e+00 10.009282 49.737005
INFO:root:train 150 2.478119e+00 10.037252 49.679222
INFO:root:train 200 2.479974e+00 9.880286 49.292600
INFO:root:train 250 2.479217e+00 9.804532 49.377490
INFO:root:train 300 2.478269e+00 9.831811 49.304402
INFO:root:train 350 2.478438e+00 9.815705 49.301104
INFO:root:train 400 2.477753e+00 9.799719 49.392145
INFO:root:train 450 2.475104e+00 9.846175 49.553076
INFO:root:train 500 2.473891e+00 9.836577 49.628867
INFO:root:train 550 2.472415e+00 9.828721 49.773140
INFO:root:train 600 2.473571e+00 9.793573 49.701019
INFO:root:train 650 2.474431e+00 9.706221 49.596774
INFO:root:train 700 2.474214e+00 9.675909 49.598787
INFO:root:train_acc 9.656250
INFO:root:valid 000 2.366658e+00 14.062500 54.687500
INFO:root:valid 050 2.462010e+00 9.313725 50.091912
INFO:root:valid 100 2.457902e+00 9.653465 50.417698
INFO:root:valid 150 2.459656e+00 9.261175 50.320778
INFO:root:valid_acc 9.333333
INFO:root:epoch 1 lr 1.261828e-05
INFO:root:train 000 2.546455e+00 7.812500 50.000000
INFO:root:train 050 2.498028e+00 9.926471 48.253676
INFO:root:train 100 2.492781e+00 9.545173 49.474010
INFO:root:train 150 2.494397e+00 9.468129 49.751656
INFO:root:train 200 2.495038e+00 9.398321 49.797886
INFO:root:train 250 2.491930e+00 9.356325 50.161853
INFO:root:train 300 2.491761e+00 9.421719 50.015573
INFO:root:train 350 2.490673e+00 9.455128 50.053419
INFO:root:train 400 2.488913e+00 9.538653 50.218204
INFO:root:train 450 2.489165e+00 9.496258 50.287555
INFO:root:train 500 2.490231e+00 9.459207 50.193363
INFO:root:train 550 2.487711e+00 9.604696 50.334619
INFO:root:train 600 2.486143e+00 9.671381 50.434172
INFO:root:train 650 2.485527e+00 9.720622 50.412826
INFO:root:train 700 2.485376e+00 9.673680 50.432418
INFO:root:train_acc 9.668750
INFO:root:valid 000 2.548680e+00 6.250000 43.750000
INFO:root:valid 050 2.477722e+00 9.068627 50.949755
INFO:root:valid 100 2.472380e+00 9.421411 51.345916
INFO:root:valid 150 2.473158e+00 9.406043 51.324503
INFO:root:valid_acc 9.708333
INFO:solver.bo_hb:Configuration achieved a performance of 2.472084 
INFO:solver.bo_hb:Evaluation of this configuration took 96.315450 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 30 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.989200
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.289007
INFO:solver.bo_hb:Next candidate [2.00000000e-01 4.66666667e+00 5.05000000e-02 1.00000000e+00
 4.50000000e+00 1.66666667e-01 6.15000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0028361940741568e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 6.309138e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 1.846875e+00 37.928922 81.924020
INFO:root:train 100 1.458905e+00 53.233292 88.165223
INFO:root:train 150 1.198858e+00 62.086093 91.401076
INFO:root:train 200 1.025465e+00 67.638371 93.244714
INFO:root:train 250 8.992565e-01 71.750498 94.416086
INFO:root:train 300 8.009805e-01 74.859842 95.239826
INFO:root:train 350 7.274154e-01 77.225783 95.811075
INFO:root:train 400 6.646936e-01 79.231608 96.267145
INFO:root:train 450 6.154531e-01 80.820399 96.635948
INFO:root:train 500 5.730959e-01 82.213698 96.928019
INFO:root:train 550 5.383783e-01 83.314428 97.178426
INFO:root:train 600 5.076827e-01 84.252808 97.397567
INFO:root:train 650 4.807995e-01 85.066244 97.587846
INFO:root:train 700 4.587183e-01 85.759183 97.737607
INFO:root:train_acc 86.410417
INFO:root:valid 000 1.558574e-01 93.750000 100.000000
INFO:root:valid 050 1.595985e-01 94.638480 99.846814
INFO:root:valid 100 1.561991e-01 94.972153 99.752475
INFO:root:valid 150 1.576359e-01 95.084851 99.710265
INFO:root:valid_acc 95.225000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 4.582807e-01 90.625000 100.000000
INFO:root:train 050 5.818455e-01 83.792892 97.610294
INFO:root:train 100 5.899262e-01 83.524134 97.694926
INFO:root:train 150 5.733616e-01 83.702401 97.764901
INFO:root:train 200 5.679649e-01 83.861940 97.792289
INFO:root:train 250 5.731926e-01 83.740040 97.783865
INFO:root:train 300 5.763362e-01 83.772841 97.752284
INFO:root:train 350 5.786530e-01 83.707265 97.774217
INFO:root:train 400 5.817677e-01 83.638560 97.775094
INFO:root:train 450 5.814453e-01 83.623198 97.772312
INFO:root:train 500 5.795955e-01 83.732535 97.788797
INFO:root:train 550 5.808823e-01 83.660390 97.793784
INFO:root:train 600 5.811896e-01 83.639247 97.808340
INFO:root:train 650 5.828454e-01 83.630952 97.796659
INFO:root:train 700 5.840718e-01 83.597093 97.762126
INFO:root:train_acc 83.583333
INFO:root:valid 000 1.362979e-01 95.312500 100.000000
INFO:root:valid 050 1.421369e-01 95.741422 99.724265
INFO:root:valid 100 1.434932e-01 95.792079 99.752475
INFO:root:valid 150 1.439126e-01 95.747103 99.772351
INFO:root:valid_acc 95.725000
INFO:solver.bo_hb:Configuration achieved a performance of 0.144447 
INFO:solver.bo_hb:Evaluation of this configuration took 85.733103 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 31 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([31])) that is different to the input size (torch.Size([31, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.163476
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.300976
INFO:solver.bo_hb:Next candidate [1.55555556e-01 5.11111111e+00 5.05000000e-02 1.22222222e+00
 4.50000000e+00 5.00000000e-01 6.50000000e-05]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.06222222222222223, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0002993808675978e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 6.309138e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 1.846872e+00 37.837010 81.985294
INFO:root:train 100 1.459238e+00 53.202351 88.118812
INFO:root:train 150 1.198629e+00 62.199917 91.349338
INFO:root:train 200 1.025011e+00 67.817164 93.190299
INFO:root:train 250 8.990902e-01 71.899900 94.366285
INFO:root:train 300 8.003210e-01 75.072674 95.208679
INFO:root:train 350 7.268198e-01 77.506232 95.779915
INFO:root:train 400 6.642850e-01 79.496571 96.235973
INFO:root:train 450 6.154536e-01 81.014412 96.604767
INFO:root:train 500 5.730384e-01 82.425773 96.903069
INFO:root:train 550 5.382358e-01 83.504424 97.158575
INFO:root:train 600 5.070497e-01 84.416597 97.374168
INFO:root:train 650 4.802132e-01 85.227055 97.556644
INFO:root:train 700 4.581759e-01 85.906295 97.710859
INFO:root:train_acc 86.541667
INFO:root:valid 000 1.269484e-01 95.312500 100.000000
INFO:root:valid 050 1.537039e-01 94.577206 99.846814
INFO:root:valid 100 1.514653e-01 95.095916 99.737005
INFO:root:valid 150 1.539434e-01 95.177980 99.699917
INFO:root:valid_acc 95.366667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.181496e-01 93.750000 100.000000
INFO:root:train 050 4.870768e-01 86.274510 98.039216
INFO:root:train 100 4.932818e-01 86.215965 98.189975
INFO:root:train 150 4.762159e-01 86.413493 98.375414
INFO:root:train 200 4.770763e-01 86.435012 98.421953
INFO:root:train 250 4.838801e-01 86.230080 98.381474
INFO:root:train 300 4.842628e-01 86.248962 98.395972
INFO:root:train 350 4.863907e-01 86.137821 98.419694
INFO:root:train 400 4.886705e-01 86.069981 98.414121
INFO:root:train 450 4.888177e-01 86.062223 98.406319
INFO:root:train 500 4.866137e-01 86.187001 98.400075
INFO:root:train 550 4.879244e-01 86.175703 98.403471
INFO:root:train 600 4.880898e-01 86.166285 98.408902
INFO:root:train 650 4.909549e-01 86.074309 98.401498
INFO:root:train 700 4.914867e-01 86.089069 98.375089
INFO:root:train_acc 86.050000
INFO:root:valid 000 1.320171e-01 95.312500 100.000000
INFO:root:valid 050 1.405942e-01 95.741422 99.724265
INFO:root:valid 100 1.429956e-01 95.668317 99.737005
INFO:root:valid 150 1.442645e-01 95.633278 99.762003
INFO:root:valid_acc 95.691667
INFO:solver.bo_hb:Configuration achieved a performance of 0.144262 
INFO:solver.bo_hb:Evaluation of this configuration took 85.824618 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 32 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.173268
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.300770
INFO:solver.bo_hb:Next candidate [2.22222222e-02 6.00000000e+00 1.75000000e-02 1.88888889e+00
 4.50000000e+00 1.50000000e+00 6.50000000e-05]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000199 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 33 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([33])) that is different to the input size (torch.Size([33, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.288498
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.219708
INFO:solver.bo_hb:Next candidate [6.66666667e-02 6.00000000e+00 5.05000000e-02 3.33333333e-01
 5.50000000e+00 1.50000000e+00 8.35000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.026666666666666672, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0038527198226775e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 1.261828e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.427894e+00 9.558824 51.133578
INFO:root:train 100 2.370858e+00 10.659035 54.068688
INFO:root:train 150 2.335391e+00 11.527318 56.291391
INFO:root:train 200 2.305062e+00 13.355100 58.760883
INFO:root:train 250 2.277735e+00 15.369771 61.522659
INFO:root:train 300 2.252993e+00 17.457434 64.010590
INFO:root:train 350 2.229365e+00 19.275285 66.306090
INFO:root:train 400 2.208073e+00 20.854115 68.290212
INFO:root:train 450 2.188622e+00 22.072478 69.983370
INFO:root:train 500 2.168324e+00 23.284681 71.429017
INFO:root:train 550 2.148895e+00 24.342105 72.671847
INFO:root:train 600 2.128912e+00 25.311980 73.783278
INFO:root:train 650 2.110053e+00 26.257680 74.771985
INFO:root:train 700 2.091994e+00 27.202211 75.666459
INFO:root:train_acc 28.043750
INFO:root:valid 000 1.799748e+00 48.437500 85.937500
INFO:root:valid 050 1.796646e+00 43.627451 87.591912
INFO:root:valid 100 1.804848e+00 42.713490 88.134282
INFO:root:valid 150 1.809223e+00 42.311672 87.893212
INFO:root:valid_acc 42.375000
INFO:root:epoch 1 lr 1.261828e-05
INFO:root:train 000 1.848747e+00 45.312500 84.375000
INFO:root:train 050 1.850384e+00 39.736520 85.324755
INFO:root:train 100 1.859224e+00 38.861386 84.808168
INFO:root:train 150 1.856152e+00 39.000414 84.995861
INFO:root:train 200 1.853808e+00 38.860386 85.113495
INFO:root:train 250 1.854811e+00 38.782371 85.178038
INFO:root:train 300 1.854257e+00 39.057309 85.340532
INFO:root:train 350 1.854409e+00 38.977920 85.323184
INFO:root:train 400 1.855605e+00 39.043017 85.294576
INFO:root:train 450 1.852392e+00 39.208010 85.379712
INFO:root:train 500 1.850000e+00 39.352545 85.500873
INFO:root:train 550 1.850715e+00 39.283689 85.432736
INFO:root:train 600 1.848103e+00 39.444676 85.550125
INFO:root:train 650 1.846785e+00 39.544931 85.577477
INFO:root:train 700 1.846069e+00 39.599679 85.678941
INFO:root:train_acc 39.625000
INFO:root:valid 000 1.730039e+00 50.000000 85.937500
INFO:root:valid 050 1.775394e+00 45.680147 89.185049
INFO:root:valid 100 1.788842e+00 44.291460 88.814975
INFO:root:valid 150 1.789382e+00 44.205298 88.431291
INFO:root:valid_acc 43.841667
INFO:solver.bo_hb:Configuration achieved a performance of 1.791542 
INFO:solver.bo_hb:Evaluation of this configuration took 76.920900 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 34 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([34])) that is different to the input size (torch.Size([34, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.287928
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.295732
INFO:solver.bo_hb:Next candidate [2.00000000e-01 5.55555556e+00 5.05000000e-02 1.22222222e+00
 3.50000000e+00 1.83333333e+00 6.50000000e-05]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0002993808675978e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 6.309138e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.303978e+00 14.920343 59.865196
INFO:root:train 100 2.192238e+00 22.277228 69.709158
INFO:root:train 150 2.106718e+00 26.541805 74.544702
INFO:root:train 200 2.028597e+00 30.519279 77.712998
INFO:root:train 250 1.956917e+00 34.138446 80.048556
INFO:root:train 300 1.884206e+00 37.307932 82.049419
INFO:root:train 350 1.816786e+00 40.019587 83.533654
INFO:root:train 400 1.751945e+00 42.639495 84.854271
INFO:root:train 450 1.691234e+00 45.139274 85.913248
INFO:root:train 500 1.629724e+00 47.611028 86.807635
INFO:root:train 550 1.570926e+00 49.858212 87.624773
INFO:root:train 600 1.513153e+00 51.931676 88.391743
INFO:root:train 650 1.457984e+00 53.953053 89.091302
INFO:root:train 700 1.407032e+00 55.735111 89.688837
INFO:root:train_acc 57.293750
INFO:root:valid 000 6.076704e-01 79.687500 98.437500
INFO:root:valid 050 6.430172e-01 82.015931 97.886029
INFO:root:valid 100 6.438702e-01 82.085396 97.834158
INFO:root:valid 150 6.470651e-01 81.839818 97.961507
INFO:root:valid_acc 82.083333
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 1.100570e+00 65.625000 98.437500
INFO:root:train 050 1.200060e+00 62.898284 93.075980
INFO:root:train 100 1.201619e+00 62.654703 93.115718
INFO:root:train 150 1.190083e+00 63.286424 93.336093
INFO:root:train 200 1.189153e+00 63.386194 93.415734
INFO:root:train 250 1.189707e+00 63.327938 93.407620
INFO:root:train 300 1.189272e+00 63.325374 93.391819
INFO:root:train 350 1.194590e+00 62.989672 93.331553
INFO:root:train 400 1.199903e+00 62.897444 93.239557
INFO:root:train 450 1.196688e+00 62.888027 93.285754
INFO:root:train 500 1.197505e+00 62.921033 93.285304
INFO:root:train 550 1.198092e+00 62.814769 93.304787
INFO:root:train 600 1.198429e+00 62.902974 93.295029
INFO:root:train 650 1.197526e+00 62.963230 93.253168
INFO:root:train 700 1.198366e+00 62.916815 93.275232
INFO:root:train_acc 62.997917
INFO:root:valid 000 6.409363e-01 87.500000 96.875000
INFO:root:valid 050 7.078661e-01 81.740196 97.763480
INFO:root:valid 100 7.163154e-01 81.157178 98.004332
INFO:root:valid 150 7.205664e-01 80.908526 97.930464
INFO:root:valid_acc 80.775000
INFO:solver.bo_hb:Configuration achieved a performance of 0.722953 
INFO:solver.bo_hb:Evaluation of this configuration took 76.873816 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 35 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([35])) that is different to the input size (torch.Size([35, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.378773
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.301827
INFO:solver.bo_hb:Next candidate [6.66666667e-02 6.00000000e+00 8.35000000e-02 1.11111111e-01
 3.50000000e+00 5.00000000e-01 1.75000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.026666666666666672, 'grad_clip_value': 8, 'initial_lr': 0.0014689262776438668, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0008062296110608e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 1.468926e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.278128e+00 12.622549 63.143382
INFO:root:train 100 2.107078e+00 22.602104 73.329208
INFO:root:train 150 1.960506e+00 31.281043 78.880381
INFO:root:train 200 1.825640e+00 38.152985 82.276119
INFO:root:train 250 1.697310e+00 43.862052 84.792082
INFO:root:train 300 1.578236e+00 48.697051 86.742110
INFO:root:train 350 1.478282e+00 52.354879 88.185541
INFO:root:train 400 1.387469e+00 55.688903 89.374221
INFO:root:train 450 1.308673e+00 58.522727 90.288941
INFO:root:train 500 1.238623e+00 60.890719 91.070983
INFO:root:train 550 1.176976e+00 63.041629 91.728108
INFO:root:train 600 1.119717e+00 64.969842 92.327891
INFO:root:train 650 1.067746e+00 66.729071 92.828341
INFO:root:train 700 1.021931e+00 68.275232 93.270774
INFO:root:train_acc 69.575000
INFO:root:valid 000 4.250400e-01 87.500000 100.000000
INFO:root:valid 050 3.787366e-01 89.338235 99.111520
INFO:root:valid 100 3.783165e-01 89.449257 99.056312
INFO:root:valid 150 3.769016e-01 89.445364 99.120447
INFO:root:valid_acc 89.591667
INFO:root:epoch 1 lr 1.468926e-05
INFO:root:train 000 5.142260e-01 87.500000 100.000000
INFO:root:train 050 5.404532e-01 83.823529 98.253676
INFO:root:train 100 5.366094e-01 83.895421 98.174505
INFO:root:train 150 5.200507e-01 84.602649 98.261589
INFO:root:train 200 5.166099e-01 84.685945 98.282027
INFO:root:train 250 5.135275e-01 84.785857 98.294323
INFO:root:train 300 5.064981e-01 84.992733 98.349252
INFO:root:train 350 5.070569e-01 85.056090 98.321759
INFO:root:train 400 5.035928e-01 85.088061 98.343984
INFO:root:train 450 4.980311e-01 85.279241 98.395926
INFO:root:train 500 4.944556e-01 85.354291 98.434381
INFO:root:train 550 4.925417e-01 85.432736 98.451679
INFO:root:train 600 4.907720e-01 85.492928 98.463498
INFO:root:train 650 4.871842e-01 85.572677 98.485503
INFO:root:train 700 4.846771e-01 85.620988 98.504369
INFO:root:train_acc 85.664583
INFO:root:valid 000 3.216067e-01 89.062500 96.875000
INFO:root:valid 050 3.281629e-01 90.931373 99.264706
INFO:root:valid 100 3.342222e-01 90.717822 99.303837
INFO:root:valid 150 3.403382e-01 90.604305 99.192881
INFO:root:valid_acc 90.716667
INFO:solver.bo_hb:Configuration achieved a performance of 0.340399 
INFO:solver.bo_hb:Evaluation of this configuration took 85.658556 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 36 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([36])) that is different to the input size (torch.Size([36, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.233058
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.304673
INFO:solver.bo_hb:Next candidate [6.66666667e-02 6.00000000e+00 1.75000000e-02 1.88888889e+00
 5.50000000e+00 5.00000000e-01 3.95000000e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000198 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 37 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([37])) that is different to the input size (torch.Size([37, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.254477
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.210462
INFO:solver.bo_hb:Next candidate [6.66666667e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 5.50000000e+00 2.50000000e+00 5.05000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.026666666666666672, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 6.309138e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.303976e+00 14.920343 59.865196
INFO:root:train 100 2.192204e+00 22.308168 69.693688
INFO:root:train 150 2.106805e+00 26.510762 74.617136
INFO:root:train 200 2.028935e+00 30.433769 77.806281
INFO:root:train 250 1.957273e+00 34.013944 80.123257
INFO:root:train 300 1.884740e+00 37.250831 82.096138
INFO:root:train 350 1.817329e+00 39.952813 83.591524
INFO:root:train 400 1.752297e+00 42.627805 84.901029
INFO:root:train 450 1.691565e+00 45.135809 85.944429
INFO:root:train 500 1.629896e+00 47.614147 86.845060
INFO:root:train 550 1.571115e+00 49.824183 87.653131
INFO:root:train 600 1.513264e+00 51.851082 88.422941
INFO:root:train 650 1.458068e+00 53.864247 89.112903
INFO:root:train 700 1.406956e+00 55.625892 89.717814
INFO:root:train_acc 57.200000
INFO:root:valid 000 6.072544e-01 81.250000 98.437500
INFO:root:valid 050 6.439308e-01 82.015931 97.886029
INFO:root:valid 100 6.453251e-01 82.069926 97.772277
INFO:root:valid 150 6.488982e-01 81.839818 97.951159
INFO:root:valid_acc 81.925000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 8.924807e-01 73.437500 98.437500
INFO:root:train 050 8.505441e-01 74.785539 96.446078
INFO:root:train 100 8.552695e-01 74.056312 96.519183
INFO:root:train 150 8.443273e-01 74.606788 96.533526
INFO:root:train 200 8.413423e-01 74.735697 96.548507
INFO:root:train 250 8.409419e-01 74.800797 96.376992
INFO:root:train 300 8.374909e-01 74.979236 96.361088
INFO:root:train 350 8.448492e-01 74.666132 96.349715
INFO:root:train 400 8.472483e-01 74.563591 96.337282
INFO:root:train 450 8.436620e-01 74.556541 96.410754
INFO:root:train 500 8.440678e-01 74.560254 96.432136
INFO:root:train 550 8.444349e-01 74.526429 96.438294
INFO:root:train 600 8.442361e-01 74.597026 96.435628
INFO:root:train 650 8.434945e-01 74.683180 96.414171
INFO:root:train 700 8.435613e-01 74.658969 96.433666
INFO:root:train_acc 74.739583
INFO:root:valid 000 5.876936e-01 85.937500 96.875000
INFO:root:valid 050 6.322450e-01 82.965686 98.131127
INFO:root:valid 100 6.430807e-01 82.595916 98.220916
INFO:root:valid 150 6.475307e-01 82.502070 98.127070
INFO:root:valid_acc 82.433333
INFO:solver.bo_hb:Configuration achieved a performance of 0.649645 
INFO:solver.bo_hb:Evaluation of this configuration took 76.585279 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 38 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([38])) that is different to the input size (torch.Size([38, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.296406
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.301504
INFO:solver.bo_hb:Next candidate [3.33333333e-01 4.66666667e+00 8.35000000e-02 1.66666667e+00
 3.50000000e+00 1.50000000e+00 1.75000000e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 
INFO:solver.bo_hb:Evaluation of this configuration took 0.000196 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Start iteration 39 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([39])) that is different to the input size (torch.Size([39, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.810749
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.212847
INFO:solver.bo_hb:Next candidate [2.00000000e-01 6.00000000e+00 5.05000000e-02 3.33333333e-01
 4.50000000e+00 5.00000000e-01 5.05000000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Exponential', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 1.261828e-04
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.303493e+00 11.734069 61.550245
INFO:root:train 100 2.151574e+00 19.925743 71.318069
INFO:root:train 150 2.020133e+00 28.073262 76.883278
INFO:root:train 200 1.898869e+00 34.849192 80.441542
INFO:root:train 250 1.781797e+00 40.344871 83.142430
INFO:root:train 300 1.669776e+00 45.167151 85.236711
INFO:root:train 350 1.573575e+00 48.829238 86.796652
INFO:root:train 400 1.485035e+00 52.240493 88.103959
INFO:root:train 450 1.406429e+00 55.200249 89.124861
INFO:root:train 500 1.335373e+00 57.693987 89.979416
INFO:root:train 550 1.272078e+00 59.896779 90.707237
INFO:root:train 600 1.213154e+00 61.813644 91.363353
INFO:root:train 650 1.159387e+00 63.680876 91.921083
INFO:root:train 700 1.111505e+00 65.290656 92.405938
INFO:root:train_acc 66.683333
INFO:root:valid 000 4.606983e-01 85.937500 100.000000
INFO:root:valid 050 4.292109e-01 88.020833 98.958333
INFO:root:valid 100 4.295705e-01 88.103342 98.901609
INFO:root:valid 150 4.274924e-01 88.172599 98.944536
INFO:root:valid_acc 88.308333
INFO:root:epoch 1 lr 1.261828e-05
INFO:root:train 000 8.270524e-01 79.687500 98.437500
INFO:root:train 050 8.939586e-01 72.334559 95.802696
INFO:root:train 100 8.724520e-01 72.865099 95.946782
INFO:root:train 150 8.394308e-01 73.799669 96.223096
INFO:root:train 200 8.242673e-01 74.378109 96.307525
INFO:root:train 250 8.119249e-01 74.501992 96.408118
INFO:root:train 300 8.007763e-01 74.948090 96.480482
INFO:root:train 350 7.932748e-01 75.191417 96.558939
INFO:root:train 400 7.852540e-01 75.389651 96.621727
INFO:root:train 450 7.750682e-01 75.731014 96.698309
INFO:root:train 500 7.673723e-01 75.913797 96.843812
INFO:root:train 550 7.634121e-01 76.060572 96.863657
INFO:root:train 600 7.574979e-01 76.299917 96.913998
INFO:root:train 650 7.503180e-01 76.562500 96.966206
INFO:root:train 700 7.451640e-01 76.689551 96.999822
INFO:root:train_acc 76.895833
INFO:root:valid 000 3.975380e-01 89.062500 96.875000
INFO:root:valid 050 4.177187e-01 88.848039 98.988971
INFO:root:valid 100 4.225402e-01 88.799505 99.040842
INFO:root:valid 150 4.294190e-01 88.493377 98.944536
INFO:root:valid_acc 88.583333
INFO:solver.bo_hb:Configuration achieved a performance of 0.430356 
INFO:solver.bo_hb:Evaluation of this configuration took 85.881448 seconds
INFO:solver.bo_hb:Current incumbent [2.22222222e-02 7.33333333e+00 5.05000000e-02 1.00000000e+00
 3.16666667e+00 0.00000000e+00 6.50000000e-05] with estimated performance 0.134926
INFO:solver.bo_hb:Return [0.022222222222222233, 7.333333333333333, 0.0505, 1.0, 3.166666666666667, 0.0, 6.500000000000002e-05] as incumbent with error 0.134926 
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.008888888888888894, 'grad_clip_value': 8, 'initial_lr': 0.0012618275345906706, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0002993808675978e-05}
main.py:103: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.
  total_model_params = np.sum(p.numel() for p in model.parameters())
INFO:root:param size = 0.039065MB
INFO:root:epoch 0 lr 1.254060e-03
INFO:root:train 000 3.930316e+00 3.125000 10.937500
INFO:root:train 050 3.311303e+00 15.012255 41.574755
INFO:root:train 100 2.824229e+00 28.016708 56.590347
INFO:root:train 150 2.480870e+00 37.024007 64.838576
INFO:root:train 200 2.218005e+00 43.602301 70.343595
INFO:root:train 250 2.016325e+00 48.711404 74.259213
INFO:root:train 300 1.863632e+00 52.507267 77.050457
INFO:root:train 350 1.735324e+00 55.693554 79.331375
INFO:root:train 400 1.620731e+00 58.587905 81.218828
INFO:root:train 450 1.528524e+00 60.958287 82.666990
INFO:root:train 500 1.456078e+00 62.786926 83.801148
INFO:root:train 550 1.383748e+00 64.595622 84.919465
INFO:root:train 600 1.322573e+00 66.105969 85.856905
INFO:root:train 650 1.268560e+00 67.391513 86.662346
INFO:root:train 700 1.223756e+00 68.536020 87.328370
INFO:root:train 750 1.179297e+00 69.673768 87.938998
INFO:root:train 800 1.138488e+00 70.688983 88.502653
INFO:root:train 850 1.102592e+00 71.573884 89.009254
INFO:root:train 900 1.069177e+00 72.390053 89.471767
INFO:root:train 950 1.037748e+00 73.181191 89.913578
INFO:root:train 1000 1.007407e+00 73.916708 90.306568
INFO:root:train 1050 9.788679e-01 74.610490 90.678520
INFO:root:train 1100 9.552112e-01 75.212875 90.976953
INFO:root:train 1150 9.321768e-01 75.772426 91.295613
INFO:root:train 1200 9.104388e-01 76.324417 91.568224
INFO:root:train 1250 8.912628e-01 76.787320 91.815298
INFO:root:train 1300 8.728563e-01 77.259080 92.040978
INFO:root:train 1350 8.562528e-01 77.636936 92.266145
INFO:root:train 1400 8.402796e-01 78.050277 92.467434
INFO:root:train 1450 8.234848e-01 78.467436 92.667772
INFO:root:train 1500 8.095056e-01 78.818288 92.837067
INFO:root:train 1550 7.949255e-01 79.157600 93.024662
INFO:root:train 1600 7.816984e-01 79.476694 93.190779
INFO:root:train 1650 7.691449e-01 79.807692 93.345889
INFO:root:train 1700 7.572892e-01 80.099023 93.490043
INFO:root:train 1750 7.465081e-01 80.370146 93.627748
INFO:root:train 1800 7.352066e-01 80.652242 93.760411
INFO:root:train 1850 7.252232e-01 80.895462 93.887595
INFO:root:train 1900 7.143874e-01 81.169450 94.011376
INFO:root:train 1950 7.049692e-01 81.406971 94.125609
INFO:root:train 2000 6.950866e-01 81.650581 94.237256
INFO:root:train 2050 6.864216e-01 81.864792 94.338890
INFO:root:train 2100 6.781204e-01 82.079218 94.434198
INFO:root:train 2150 6.700387e-01 82.288761 94.527255
INFO:root:train 2200 6.624757e-01 82.493043 94.613954
INFO:root:train 2250 6.548623e-01 82.685473 94.700272
INFO:root:train 2300 6.482325e-01 82.845095 94.781481
INFO:root:train 2350 6.408508e-01 83.033815 94.865882
INFO:root:train 2400 6.345162e-01 83.178233 94.945465
INFO:root:train 2450 6.276588e-01 83.349271 95.020527
INFO:root:train 2500 6.207524e-01 83.517218 95.096961
INFO:root:train 2550 6.137576e-01 83.690832 95.175911
INFO:root:train 2600 6.077050e-01 83.852965 95.242815
INFO:root:train 2650 6.017953e-01 84.001910 95.309553
INFO:root:train 2700 5.956912e-01 84.149389 95.379026
INFO:root:train 2750 5.900364e-01 84.298323 95.440294
INFO:root:train 2800 5.841562e-01 84.448077 95.500491
INFO:root:train 2850 5.786662e-01 84.592029 95.555836
INFO:root:train 2900 5.739682e-01 84.723479 95.606580
INFO:root:train 2950 5.688171e-01 84.850474 95.662487
INFO:root:train 3000 5.640480e-01 84.968552 95.713408
INFO:root:train 3050 5.597054e-01 85.076614 95.759075
INFO:root:train 3100 5.548672e-01 85.196812 95.810323
INFO:root:train 3150 5.500903e-01 85.317657 95.862425
INFO:root:train 3200 5.454393e-01 85.430822 95.914851
INFO:root:train 3250 5.412834e-01 85.534259 95.958936
INFO:root:train 3300 5.373806e-01 85.643082 96.004998
INFO:root:train 3350 5.331925e-01 85.744927 96.048288
INFO:root:train 3400 5.291059e-01 85.840562 96.092601
INFO:root:train 3450 5.250757e-01 85.949272 96.134725
INFO:root:train 3500 5.212428e-01 86.050860 96.175200
INFO:root:train 3550 5.174800e-01 86.150468 96.214095
INFO:root:train 3600 5.142089e-01 86.233859 96.246702
INFO:root:train_acc 86.284079
INFO:root:valid 000 4.813643e-01 87.500000 98.437500
INFO:root:valid 050 5.221636e-01 86.090686 96.629902
INFO:root:valid 100 5.132544e-01 86.463490 96.565594
INFO:root:valid 150 5.057605e-01 86.527318 96.668046
INFO:root:valid 200 5.007831e-01 86.473881 96.820585
INFO:root:valid 250 5.038579e-01 86.553785 96.781624
INFO:root:valid 300 5.039754e-01 86.524086 96.760797
INFO:root:valid 350 5.054760e-01 86.453882 96.785969
INFO:root:valid 400 5.038620e-01 86.428460 96.804863
INFO:root:valid 450 5.064496e-01 86.398282 96.750277
INFO:root:valid 500 5.078146e-01 86.408433 96.712824
INFO:root:valid 550 5.068109e-01 86.459279 96.727541
INFO:root:valid 600 5.096881e-01 86.439268 96.711210
INFO:root:test_acc 86.432148
INFO:root:epoch 1 lr 1.230948e-03
INFO:root:train 000 1.719770e-01 98.437500 100.000000
INFO:root:train 050 2.231839e-01 93.504902 99.264706
INFO:root:train 100 2.290027e-01 93.517946 99.102723
INFO:root:train 150 2.228349e-01 93.605132 99.151490
INFO:root:train 200 2.304740e-01 93.361318 99.144900
INFO:root:train 250 2.303617e-01 93.469871 99.140936
INFO:root:train 300 2.331888e-01 93.381437 99.122716
INFO:root:train 350 2.366278e-01 93.264779 99.082977
INFO:root:train 400 2.333913e-01 93.336970 99.107700
INFO:root:train 450 2.331015e-01 93.365438 99.081901
INFO:root:train 500 2.340799e-01 93.282186 99.076846
INFO:root:train 550 2.340969e-01 93.259415 99.086887
INFO:root:train 600 2.326198e-01 93.300229 99.100458
INFO:root:train 650 2.338287e-01 93.313172 99.095142
INFO:root:train 700 2.334095e-01 93.315353 99.103959
INFO:root:train 750 2.342368e-01 93.308921 99.082473
INFO:root:train 800 2.355862e-01 93.297441 99.061720
INFO:root:train 850 2.360539e-01 93.279965 99.065438
INFO:root:train 900 2.355246e-01 93.304315 99.075680
INFO:root:train 950 2.354904e-01 93.321175 99.066772
INFO:root:train 1000 2.340790e-01 93.367570 99.077485
INFO:root:train 1050 2.319962e-01 93.406577 99.087179
INFO:root:train 1100 2.307885e-01 93.434946 99.105926
INFO:root:train 1150 2.300067e-01 93.445917 99.113543
INFO:root:train 1200 2.293133e-01 93.470285 99.112719
INFO:root:train 1250 2.290068e-01 93.483963 99.110711
INFO:root:train 1300 2.292903e-01 93.495388 99.107658
INFO:root:train 1350 2.286994e-01 93.509437 99.110613
INFO:root:train 1400 2.279518e-01 93.532521 99.107780
INFO:root:train 1450 2.277542e-01 93.542169 99.112681
INFO:root:train 1500 2.269156e-01 93.554297 99.118296
INFO:root:train 1550 2.263316e-01 93.572695 99.123549
INFO:root:train 1600 2.267804e-01 93.575304 99.113835
INFO:root:train 1650 2.253461e-01 93.606148 99.126476
INFO:root:train 1700 2.244610e-01 93.633341 99.131026
INFO:root:train 1750 2.240000e-01 93.642026 99.136208
INFO:root:train 1800 2.236253e-01 93.654567 99.134162
INFO:root:train 1850 2.233239e-01 93.668119 99.136447
INFO:root:train 1900 2.228424e-01 93.697396 99.137789
INFO:root:train 1950 2.232385e-01 93.696342 99.136661
INFO:root:train 2000 2.232708e-01 93.691436 99.137150
INFO:root:train 2050 2.221854e-01 93.715718 99.139139
INFO:root:train 2100 2.221551e-01 93.710584 99.136572
INFO:root:train 2150 2.217595e-01 93.729661 99.135576
INFO:root:train 2200 2.216707e-01 93.726573 99.132497
INFO:root:train 2250 2.211890e-01 93.731952 99.133024
INFO:root:train 2300 2.206743e-01 93.743209 99.137603
INFO:root:train 2350 2.202188e-01 93.763292 99.145311
INFO:root:train 2400 2.197839e-01 93.778634 99.144888
INFO:root:train 2450 2.198759e-01 93.776137 99.148307
INFO:root:train 2500 2.193625e-01 93.792483 99.153464
INFO:root:train 2550 2.192447e-01 93.794100 99.151681
INFO:root:train 2600 2.192930e-01 93.801663 99.148164
INFO:root:train 2650 2.189646e-01 93.808351 99.151853
INFO:root:train 2700 2.188827e-01 93.812477 99.149621
INFO:root:train 2750 2.189524e-01 93.818157 99.145765
INFO:root:train 2800 2.188217e-01 93.819730 99.147626
INFO:root:train 2850 2.186486e-01 93.828372 99.147777
INFO:root:train 2900 2.189394e-01 93.826482 99.147385
INFO:root:train 2950 2.186612e-01 93.834188 99.150712
INFO:root:train 3000 2.181271e-01 93.849967 99.153407
INFO:root:train 3050 2.180729e-01 93.851401 99.152942
INFO:root:train 3100 2.175643e-01 93.861355 99.157530
INFO:root:train 3150 2.175170e-01 93.863555 99.158997
INFO:root:train 3200 2.172615e-01 93.871056 99.160419
INFO:root:train 3250 2.171588e-01 93.876884 99.158432
INFO:root:train 3300 2.167486e-01 93.886322 99.161239
INFO:root:train 3350 2.164015e-01 93.896412 99.164429
INFO:root:train 3400 2.159386e-01 93.908501 99.165687
INFO:root:train 3450 2.157317e-01 93.919788 99.165550
INFO:root:train 3500 2.154800e-01 93.928074 99.168095
INFO:root:train 3550 2.154350e-01 93.922927 99.170568
INFO:root:train 3600 2.151882e-01 93.928336 99.172973
INFO:root:train_acc 93.933682
INFO:root:valid 000 5.122728e-01 89.062500 95.312500
INFO:root:valid 050 4.042542e-01 89.460784 97.579657
INFO:root:valid 100 3.909149e-01 89.603960 97.633045
INFO:root:valid 150 3.912683e-01 89.704056 97.588990
INFO:root:valid 200 3.819717e-01 89.832090 97.815609
INFO:root:valid 250 3.798418e-01 89.890438 97.771414
INFO:root:valid 300 3.827976e-01 89.768480 97.741902
INFO:root:valid 350 3.853627e-01 89.725783 97.769765
INFO:root:valid 400 3.850994e-01 89.717113 97.747818
INFO:root:valid 450 3.869281e-01 89.665327 97.754989
INFO:root:valid 500 3.882309e-01 89.720559 97.695235
INFO:root:valid 550 3.888746e-01 89.768603 97.691697
INFO:root:valid 600 3.905262e-01 89.722858 97.688748
INFO:root:test_acc 89.719044
INFO:root:epoch 2 lr 1.193062e-03
INFO:root:train 000 2.725993e-01 90.625000 98.437500
INFO:root:train 050 1.890495e-01 94.730392 99.172794
INFO:root:train 100 1.749528e-01 95.111386 99.272896
INFO:root:train 150 1.754358e-01 94.971026 99.327401
INFO:root:train 200 1.754348e-01 94.923818 99.370336
INFO:root:train 250 1.759528e-01 94.914094 99.377490
INFO:root:train 300 1.744769e-01 94.990656 99.392650
INFO:root:train 350 1.725373e-01 95.045406 99.385684
INFO:root:train 400 1.730533e-01 95.051434 99.392145
INFO:root:train 450 1.728552e-01 95.076912 99.390244
INFO:root:train 500 1.717804e-01 95.147206 99.404316
INFO:root:train 550 1.723034e-01 95.102654 99.427178
INFO:root:train 600 1.743228e-01 95.062916 99.417637
INFO:root:train 650 1.757271e-01 95.050883 99.392761
INFO:root:train 700 1.765250e-01 95.027193 99.391494
INFO:root:train 750 1.769025e-01 95.021222 99.400799
INFO:root:train 800 1.760330e-01 95.043305 99.403090
INFO:root:train 850 1.757356e-01 95.031580 99.406948
INFO:root:train 900 1.762577e-01 95.038499 99.396504
INFO:root:train 950 1.768275e-01 95.038118 99.383872
INFO:root:train 1000 1.757811e-01 95.059628 99.388112
INFO:root:train 1050 1.763279e-01 95.073145 99.388975
INFO:root:train 1100 1.769704e-01 95.057050 99.385502
INFO:root:train 1150 1.770485e-01 95.062717 99.386403
INFO:root:train 1200 1.775722e-01 95.034086 99.379423
INFO:root:train 1250 1.779823e-01 94.986511 99.379247
INFO:root:train 1300 1.780658e-01 94.984627 99.379083
INFO:root:train 1350 1.773883e-01 95.000231 99.376619
INFO:root:train 1400 1.775381e-01 95.003569 99.375446
INFO:root:train 1450 1.779328e-01 94.981909 99.377584
INFO:root:train 1500 1.776815e-01 94.980430 99.379580
INFO:root:train 1550 1.779936e-01 94.970986 99.379433
INFO:root:train 1600 1.779631e-01 94.960181 99.381246
INFO:root:train 1650 1.781891e-01 94.966119 99.380111
INFO:root:train 1700 1.774579e-01 94.989161 99.385472
INFO:root:train 1750 1.771817e-01 94.993040 99.386065
INFO:root:train 1800 1.776079e-01 94.988027 99.382288
INFO:root:train 1850 1.777415e-01 94.979065 99.382935
INFO:root:train 1900 1.772447e-01 94.987835 99.386014
INFO:root:train 1950 1.778492e-01 94.970528 99.380126
INFO:root:train 2000 1.783542e-01 94.951743 99.374531
INFO:root:train 2050 1.782458e-01 94.949110 99.373781
INFO:root:train 2100 1.784503e-01 94.928010 99.373810
INFO:root:train 2150 1.784901e-01 94.924599 99.374564
INFO:root:train 2200 1.784425e-01 94.927732 99.376704
INFO:root:train 2250 1.782531e-01 94.939055 99.376666
INFO:root:train 2300 1.780243e-01 94.943774 99.375951
INFO:root:train 2350 1.780214e-01 94.939653 99.376595
INFO:root:train 2400 1.777116e-01 94.948069 99.377863
INFO:root:train 2450 1.775085e-01 94.954228 99.377167
INFO:root:train 2500 1.769867e-01 94.977009 99.382747
INFO:root:train 2550 1.766855e-01 94.982360 99.382595
INFO:root:train 2600 1.766264e-01 94.988706 99.384251
INFO:root:train 2650 1.766489e-01 94.991866 99.382309
INFO:root:train 2700 1.770443e-01 94.977555 99.381016
INFO:root:train 2750 1.767265e-01 94.982506 99.380907
INFO:root:train 2800 1.765589e-01 94.983376 99.382475
INFO:root:train 2850 1.765955e-01 94.979832 99.381796
INFO:root:train 2900 1.763057e-01 94.987720 99.382217
INFO:root:train 2950 1.766875e-01 94.978397 99.379977
INFO:root:train 3000 1.768024e-01 94.983443 99.380415
INFO:root:train 3050 1.768158e-01 94.984226 99.378278
INFO:root:train 3100 1.767304e-01 94.995566 99.379233
INFO:root:train 3150 1.769458e-01 94.999107 99.378174
INFO:root:train 3200 1.767751e-01 94.998145 99.378612
INFO:root:train 3250 1.766957e-01 95.002499 99.377115
INFO:root:train 3300 1.763301e-01 95.012875 99.380396
INFO:root:train 3350 1.757767e-01 95.024340 99.382181
INFO:root:train 3400 1.754972e-01 95.030414 99.382535
INFO:root:train 3450 1.756065e-01 95.024540 99.384236
INFO:root:train 3500 1.753504e-01 95.028206 99.386782
INFO:root:train 3550 1.754377e-01 95.028689 99.386176
INFO:root:train 3600 1.749631e-01 95.044779 99.390794
INFO:root:train_acc 95.050890
INFO:root:valid 000 3.356008e-01 90.625000 95.312500
INFO:root:valid 050 3.209701e-01 91.207108 98.468137
INFO:root:valid 100 3.105852e-01 91.630569 98.406559
INFO:root:valid 150 3.141143e-01 91.504553 98.406457
INFO:root:valid 200 3.102771e-01 91.456779 98.460821
INFO:root:valid 250 3.157589e-01 91.446713 98.350349
INFO:root:valid 300 3.192817e-01 91.398463 98.328488
INFO:root:valid 350 3.228786e-01 91.372863 98.330662
INFO:root:valid 400 3.195901e-01 91.466646 98.340087
INFO:root:valid 450 3.229026e-01 91.369873 98.330100
INFO:root:valid 500 3.258123e-01 91.308009 98.278443
INFO:root:valid 550 3.264770e-01 91.331103 98.267355
INFO:root:valid 600 3.286360e-01 91.287958 98.265911
INFO:root:test_acc 91.291151
INFO:root:epoch 3 lr 1.141334e-03
INFO:root:train 000 1.713635e-01 93.750000 98.437500
INFO:root:train 050 1.672345e-01 95.373775 99.387255
INFO:root:train 100 1.464698e-01 96.039604 99.489480
INFO:root:train 150 1.439222e-01 95.954056 99.555050
INFO:root:train 200 1.415846e-01 96.019900 99.541356
INFO:root:train 250 1.437400e-01 95.959910 99.508217
INFO:root:train 300 1.430817e-01 95.919850 99.512043
INFO:root:train 350 1.425087e-01 95.917913 99.510328
INFO:root:train 400 1.460382e-01 95.822943 99.477868
INFO:root:train 450 1.484590e-01 95.742101 99.462999
INFO:root:train 500 1.483776e-01 95.774077 99.460454
INFO:root:train 550 1.506882e-01 95.666969 99.447028
INFO:root:train 600 1.502514e-01 95.676477 99.451435
INFO:root:train 650 1.510913e-01 95.643721 99.445565
INFO:root:train 700 1.519941e-01 95.640157 99.438302
INFO:root:train 750 1.525951e-01 95.630826 99.440330
INFO:root:train 800 1.527378e-01 95.644117 99.442104
INFO:root:train 850 1.524026e-01 95.652174 99.454686
INFO:root:train 900 1.521625e-01 95.647198 99.460669
INFO:root:train 950 1.524917e-01 95.641101 99.462737
INFO:root:train 1000 1.516323e-01 95.668394 99.470842
INFO:root:train 1050 1.511746e-01 95.682683 99.470742
INFO:root:train 1100 1.518902e-01 95.674387 99.463556
INFO:root:train 1150 1.511789e-01 95.673599 99.473284
INFO:root:train 1200 1.512084e-01 95.672877 99.482202
INFO:root:train 1250 1.518187e-01 95.654726 99.476669
INFO:root:train 1300 1.521121e-01 95.657187 99.475163
INFO:root:train 1350 1.519477e-01 95.658309 99.481865
INFO:root:train 1400 1.521884e-01 95.642621 99.481397
INFO:root:train 1450 1.520485e-01 95.650629 99.488499
INFO:root:train 1500 1.525708e-01 95.656021 99.481596
INFO:root:train 1550 1.523219e-01 95.669125 99.480174
INFO:root:train 1600 1.525889e-01 95.673602 99.475913
INFO:root:train 1650 1.527469e-01 95.665506 99.479482
INFO:root:train 1700 1.538200e-01 95.645025 99.470899
INFO:root:train 1750 1.535679e-01 95.646238 99.470838
INFO:root:train 1800 1.535873e-01 95.644781 99.472515
INFO:root:train 1850 1.533996e-01 95.656064 99.474102
INFO:root:train 1900 1.532226e-01 95.652781 99.474783
INFO:root:train 1950 1.528764e-01 95.656074 99.477031
INFO:root:train 2000 1.529073e-01 95.652955 99.475262
INFO:root:train 2050 1.525833e-01 95.663701 99.483484
INFO:root:train 2100 1.517492e-01 95.683603 99.489826
INFO:root:train 2150 1.516585e-01 95.691684 99.485704
INFO:root:train 2200 1.517590e-01 95.690169 99.484609
INFO:root:train 2250 1.519087e-01 95.685945 99.481480
INFO:root:train 2300 1.520837e-01 95.677151 99.481204
INFO:root:train 2350 1.517107e-01 95.686676 99.482268
INFO:root:train 2400 1.518629e-01 95.686693 99.478082
INFO:root:train 2450 1.513412e-01 95.705834 99.481717
INFO:root:train 2500 1.515433e-01 95.701719 99.484581
INFO:root:train 2550 1.517255e-01 95.698991 99.485496
INFO:root:train 2600 1.516390e-01 95.704777 99.485775
INFO:root:train 2650 1.517233e-01 95.703272 99.481917
INFO:root:train 2700 1.516837e-01 95.711079 99.481673
INFO:root:train 2750 1.518225e-01 95.706675 99.482007
INFO:root:train 2800 1.519500e-01 95.706890 99.478981
INFO:root:train 2850 1.516821e-01 95.710387 99.478801
INFO:root:train 2900 1.510195e-01 95.722919 99.481860
INFO:root:train 2950 1.511392e-01 95.714376 99.484285
INFO:root:train 3000 1.506804e-01 95.726425 99.487150
INFO:root:train 3050 1.505765e-01 95.724762 99.489921
INFO:root:train 3100 1.504865e-01 95.728193 99.489076
INFO:root:train 3150 1.505165e-01 95.728539 99.488258
INFO:root:train 3200 1.506686e-01 95.726433 99.486977
INFO:root:train 3250 1.507921e-01 95.715741 99.485735
INFO:root:train 3300 1.506917e-01 95.714367 99.488791
INFO:root:train 3350 1.505588e-01 95.718162 99.488492
INFO:root:train 3400 1.504724e-01 95.721387 99.488661
INFO:root:train 3450 1.504675e-01 95.722254 99.489731
INFO:root:train 3500 1.502752e-01 95.729345 99.489432
INFO:root:train 3550 1.503158e-01 95.718636 99.490900
INFO:root:train 3600 1.502130e-01 95.725580 99.492762
INFO:root:train_acc 95.729994
INFO:root:valid 000 2.748069e-01 90.625000 96.875000
INFO:root:valid 050 3.021929e-01 91.973039 98.529412
INFO:root:valid 100 2.878971e-01 92.419554 98.483911
INFO:root:valid 150 2.924290e-01 92.394454 98.427152
INFO:root:valid 200 2.857190e-01 92.436256 98.561878
INFO:root:valid 250 2.912972e-01 92.368028 98.530876
INFO:root:valid 300 2.935902e-01 92.286130 98.479028
INFO:root:valid 350 2.970154e-01 92.174145 98.459758
INFO:root:valid 400 2.965394e-01 92.129052 98.484258
INFO:root:valid 450 2.999253e-01 92.083564 98.451358
INFO:root:valid 500 3.032152e-01 92.047156 98.437500
INFO:root:valid 550 3.044825e-01 92.054220 98.426157
INFO:root:valid 600 3.062320e-01 92.083507 98.419301
INFO:root:test_acc 92.087581
INFO:root:epoch 4 lr 1.077037e-03
INFO:root:train 000 1.023404e-01 98.437500 98.437500
INFO:root:train 050 1.201047e-01 96.721814 99.540441
INFO:root:train 100 1.282122e-01 96.472772 99.551361
INFO:root:train 150 1.331982e-01 96.254139 99.586093
INFO:root:train 200 1.331739e-01 96.268657 99.595771
INFO:root:train 250 1.335904e-01 96.283616 99.595369
INFO:root:train 300 1.330859e-01 96.257267 99.605482
INFO:root:train 350 1.337442e-01 96.216168 99.621617
INFO:root:train 400 1.353292e-01 96.189214 99.606453
INFO:root:train 450 1.347360e-01 96.220205 99.594651
INFO:root:train 500 1.353715e-01 96.163922 99.597680
INFO:root:train 550 1.336276e-01 96.211434 99.614338
INFO:root:train 600 1.350161e-01 96.204243 99.604825
INFO:root:train 650 1.342268e-01 96.181356 99.623176
INFO:root:train 700 1.343648e-01 96.195168 99.612161
INFO:root:train 750 1.342858e-01 96.173852 99.604694
INFO:root:train 800 1.346427e-01 96.178605 99.604011
INFO:root:train 850 1.338436e-01 96.199324 99.605244
INFO:root:train 900 1.328068e-01 96.221213 99.606340
INFO:root:train 950 1.325257e-01 96.207939 99.607321
INFO:root:train 1000 1.331608e-01 96.183504 99.603521
INFO:root:train 1050 1.336400e-01 96.183694 99.592650
INFO:root:train 1100 1.327203e-01 96.203735 99.605472
INFO:root:train 1150 1.324603e-01 96.193527 99.615823
INFO:root:train 1200 1.321543e-01 96.201082 99.614904
INFO:root:train 1250 1.326697e-01 96.198042 99.605316
INFO:root:train 1300 1.327995e-01 96.192832 99.598866
INFO:root:train 1350 1.329534e-01 96.172974 99.599833
INFO:root:train 1400 1.331244e-01 96.179069 99.596271
INFO:root:train 1450 1.331889e-01 96.192281 99.590799
INFO:root:train 1500 1.333299e-01 96.190040 99.587775
INFO:root:train 1550 1.339205e-01 96.176862 99.581923
INFO:root:train 1600 1.343413e-01 96.174266 99.580340
INFO:root:train 1650 1.343730e-01 96.168042 99.584532
INFO:root:train 1700 1.346003e-01 96.155754 99.585722
INFO:root:train 1750 1.351877e-01 96.135244 99.581489
INFO:root:train 1800 1.353047e-01 96.131489 99.580962
INFO:root:train 1850 1.349735e-01 96.143976 99.582152
INFO:root:train 1900 1.348895e-01 96.144299 99.581635
INFO:root:train 1950 1.352880e-01 96.136597 99.576339
INFO:root:train 2000 1.350088e-01 96.156609 99.575212
INFO:root:train 2050 1.349219e-01 96.156600 99.574141
INFO:root:train 2100 1.350796e-01 96.139487 99.573864
INFO:root:train 2150 1.350697e-01 96.147141 99.574326
INFO:root:train 2200 1.351556e-01 96.138829 99.577607
INFO:root:train 2250 1.354537e-01 96.135051 99.573106
INFO:root:train 2300 1.356275e-01 96.134154 99.568123
INFO:root:train 2350 1.357510e-01 96.127313 99.565345
INFO:root:train 2400 1.355540e-01 96.131820 99.564635
INFO:root:train 2450 1.357603e-01 96.132318 99.560129
INFO:root:train 2500 1.359226e-01 96.124675 99.560176
INFO:root:train 2550 1.363089e-01 96.106306 99.562059
INFO:root:train 2600 1.363880e-01 96.107867 99.561467
INFO:root:train 2650 1.366382e-01 96.104065 99.562665
INFO:root:train 2700 1.369117e-01 96.086519 99.559770
INFO:root:train 2750 1.367162e-01 96.088922 99.562091
INFO:root:train 2800 1.365110e-01 96.093471 99.562656
INFO:root:train 2850 1.368806e-01 96.089640 99.558269
INFO:root:train 2900 1.372693e-01 96.082170 99.554572
INFO:root:train 2950 1.375721e-01 96.069659 99.552588
INFO:root:train 3000 1.374464e-01 96.071622 99.555357
INFO:root:train 3050 1.375908e-01 96.070448 99.555986
INFO:root:train 3100 1.375213e-01 96.075863 99.555083
INFO:root:train 3150 1.377961e-01 96.074163 99.549250
INFO:root:train 3200 1.378722e-01 96.069588 99.549945
INFO:root:train 3250 1.377696e-01 96.068517 99.550138
INFO:root:train 3300 1.376908e-01 96.070320 99.549379
INFO:root:train 3350 1.375740e-01 96.073467 99.548642
INFO:root:train 3400 1.373022e-01 96.079738 99.552062
INFO:root:train 3450 1.370176e-01 96.085374 99.554930
INFO:root:train 3500 1.367116e-01 96.094866 99.557269
INFO:root:train 3550 1.367144e-01 96.094850 99.556023
INFO:root:train 3600 1.366764e-01 96.097872 99.554811
INFO:root:train_acc 96.100962
INFO:root:valid 000 2.375837e-01 93.750000 98.437500
INFO:root:valid 050 3.187947e-01 91.574755 98.468137
INFO:root:valid 100 2.978925e-01 92.094678 98.545792
INFO:root:valid 150 2.978316e-01 92.187500 98.520281
INFO:root:valid 200 2.933519e-01 92.203047 98.639614
INFO:root:valid 250 2.938436e-01 92.293327 98.580677
INFO:root:valid 300 2.947506e-01 92.280939 98.515365
INFO:root:valid 350 2.951642e-01 92.276531 98.517628
INFO:root:valid 400 2.959357e-01 92.269327 98.488155
INFO:root:valid 450 2.983492e-01 92.177106 98.489468
INFO:root:valid 500 3.000252e-01 92.165669 98.468688
INFO:root:valid 550 3.021672e-01 92.147799 98.451679
INFO:root:valid 600 3.029383e-01 92.197899 98.445300
INFO:root:test_acc 92.199134
INFO:root:epoch 5 lr 1.001756e-03
INFO:root:train 000 1.120614e-01 98.437500 98.437500
INFO:root:train 050 9.969997e-02 97.058824 99.693627
INFO:root:train 100 1.053796e-01 96.844059 99.675124
INFO:root:train 150 1.063541e-01 96.802566 99.720613
INFO:root:train 200 1.133959e-01 96.571828 99.712376
INFO:root:train 250 1.153515e-01 96.576195 99.657620
INFO:root:train 300 1.147083e-01 96.610257 99.678156
INFO:root:train 350 1.164826e-01 96.621261 99.670584
INFO:root:train 400 1.167197e-01 96.617830 99.680486
INFO:root:train 450 1.158026e-01 96.653271 99.698586
INFO:root:train 500 1.142467e-01 96.653568 99.722430
INFO:root:train 550 1.158592e-01 96.628289 99.702246
INFO:root:train 600 1.161806e-01 96.628016 99.695819
INFO:root:train 650 1.163423e-01 96.644585 99.683180
INFO:root:train 700 1.160409e-01 96.640959 99.679030
INFO:root:train 750 1.155351e-01 96.660702 99.685836
INFO:root:train 800 1.169105e-01 96.621411 99.680087
INFO:root:train 850 1.167963e-01 96.608769 99.682359
INFO:root:train 900 1.175270e-01 96.581923 99.679176
INFO:root:train 950 1.182884e-01 96.579259 99.677971
INFO:root:train 1000 1.188638e-01 96.564373 99.681568
INFO:root:train 1050 1.189247e-01 96.555364 99.683337
INFO:root:train 1100 1.188681e-01 96.545754 99.680688
INFO:root:train 1150 1.198927e-01 96.531549 99.666051
INFO:root:train 1200 1.202465e-01 96.505516 99.669546
INFO:root:train 1250 1.199671e-01 96.512790 99.667766
INFO:root:train 1300 1.201372e-01 96.521906 99.664921
INFO:root:train 1350 1.216812e-01 96.482929 99.656504
INFO:root:train 1400 1.214660e-01 96.485769 99.660956
INFO:root:train 1450 1.211950e-01 96.487336 99.666178
INFO:root:train 1500 1.210678e-01 96.482553 99.664807
INFO:root:train 1550 1.207906e-01 96.499234 99.670575
INFO:root:train 1600 1.214104e-01 96.483643 99.669152
INFO:root:train 1650 1.213888e-01 96.483192 99.664029
INFO:root:train 1700 1.223500e-01 96.465315 99.655534
INFO:root:train 1750 1.231103e-01 96.452920 99.650200
INFO:root:train 1800 1.227282e-01 96.462903 99.654706
INFO:root:train 1850 1.225668e-01 96.471502 99.657280
INFO:root:train 1900 1.227292e-01 96.464032 99.653965
INFO:root:train 1950 1.225674e-01 96.465755 99.657227
INFO:root:train 2000 1.226783e-01 96.464268 99.656422
INFO:root:train 2050 1.227942e-01 96.469710 99.652608
INFO:root:train 2100 1.229905e-01 96.468200 99.652695
INFO:root:train 2150 1.227346e-01 96.474024 99.652778
INFO:root:train 2200 1.223850e-01 96.483133 99.650017
INFO:root:train 2250 1.223978e-01 96.482119 99.653626
INFO:root:train 2300 1.220864e-01 96.490656 99.652325
INFO:root:train 2350 1.222931e-01 96.476234 99.650415
INFO:root:train 2400 1.228424e-01 96.466316 99.644679
INFO:root:train 2450 1.225744e-01 96.476566 99.644278
INFO:root:train 2500 1.226569e-01 96.477659 99.642643
INFO:root:train 2550 1.228627e-01 96.471972 99.639847
INFO:root:train 2600 1.231094e-01 96.465903 99.641364
INFO:root:train 2650 1.229750e-01 96.472440 99.640466
INFO:root:train 2700 1.226652e-01 96.478156 99.641915
INFO:root:train 2750 1.228319e-01 96.478553 99.638200
INFO:root:train 2800 1.229111e-01 96.476147 99.637964
INFO:root:train 2850 1.228895e-01 96.479854 99.635545
INFO:root:train 2900 1.224886e-01 96.493666 99.636979
INFO:root:train 2950 1.225593e-01 96.491126 99.638364
INFO:root:train 3000 1.227725e-01 96.484505 99.636059
INFO:root:train 3050 1.230075e-01 96.474004 99.635878
INFO:root:train 3100 1.230413e-01 96.473920 99.637214
INFO:root:train 3150 1.232248e-01 96.469375 99.636524
INFO:root:train 3200 1.230883e-01 96.474734 99.638297
INFO:root:train 3250 1.234111e-01 96.471759 99.635689
INFO:root:train 3300 1.236551e-01 96.460353 99.633160
INFO:root:train 3350 1.236644e-01 96.462810 99.630707
INFO:root:train 3400 1.238162e-01 96.455087 99.632461
INFO:root:train 3450 1.240653e-01 96.449399 99.625109
INFO:root:train 3500 1.244356e-01 96.442534 99.623768
INFO:root:train 3550 1.241845e-01 96.444663 99.626866
INFO:root:train 3600 1.237463e-01 96.451507 99.630311
INFO:root:train_acc 96.455146
INFO:root:valid 000 2.454694e-01 92.187500 100.000000
INFO:root:valid 050 2.694652e-01 93.167892 98.835784
INFO:root:valid 100 2.534167e-01 93.378713 98.793317
INFO:root:valid 150 2.559181e-01 93.418874 98.737583
INFO:root:valid 200 2.491746e-01 93.462376 98.833955
INFO:root:valid 250 2.538458e-01 93.401394 98.779880
INFO:root:valid 300 2.556630e-01 93.371055 98.733389
INFO:root:valid 350 2.585286e-01 93.220264 98.740207
INFO:root:valid 400 2.580714e-01 93.259040 98.737531
INFO:root:valid 450 2.607767e-01 93.178354 98.700804
INFO:root:valid 500 2.634707e-01 93.091941 98.683882
INFO:root:valid 550 2.642602e-01 93.097777 98.667196
INFO:root:valid 600 2.652634e-01 93.110441 98.655886
INFO:root:test_acc 93.104522
INFO:root:epoch 6 lr 9.173426e-04
INFO:root:train 000 4.074677e-02 98.437500 100.000000
INFO:root:train 050 1.025605e-01 96.966912 99.632353
INFO:root:train 100 1.035782e-01 96.813119 99.767946
INFO:root:train 150 1.111156e-01 96.719785 99.720613
INFO:root:train 200 1.108170e-01 96.688433 99.712376
INFO:root:train 250 1.075302e-01 96.806524 99.707420
INFO:root:train 300 1.068856e-01 96.854236 99.688538
INFO:root:train 350 1.104652e-01 96.781517 99.661681
INFO:root:train 400 1.103752e-01 96.738622 99.668797
INFO:root:train 450 1.090244e-01 96.746813 99.691657
INFO:root:train 500 1.086590e-01 96.784556 99.688124
INFO:root:train 550 1.081459e-01 96.806942 99.688067
INFO:root:train 600 1.085206e-01 96.833403 99.688020
INFO:root:train 650 1.085958e-01 96.877400 99.695180
INFO:root:train 700 1.087203e-01 96.886145 99.694633
INFO:root:train 750 1.094703e-01 96.883322 99.685836
INFO:root:train 800 1.101662e-01 96.878901 99.678137
INFO:root:train 850 1.097150e-01 96.871328 99.680523
INFO:root:train 900 1.110944e-01 96.850721 99.672239
INFO:root:train 950 1.113869e-01 96.838854 99.674685
INFO:root:train 1000 1.114408e-01 96.842220 99.675325
INFO:root:train 1050 1.108196e-01 96.867567 99.678877
INFO:root:train 1100 1.115873e-01 96.845198 99.677850
INFO:root:train 1150 1.116802e-01 96.838347 99.678269
INFO:root:train 1200 1.114781e-01 96.843776 99.681255
INFO:root:train 1250 1.120416e-01 96.822542 99.677758
INFO:root:train 1300 1.123512e-01 96.822156 99.674529
INFO:root:train 1350 1.118586e-01 96.837990 99.677322
INFO:root:train 1400 1.122420e-01 96.830389 99.677686
INFO:root:train 1450 1.125719e-01 96.827619 99.673716
INFO:root:train 1500 1.125988e-01 96.836484 99.666889
INFO:root:train 1550 1.125875e-01 96.838733 99.668561
INFO:root:train 1600 1.128612e-01 96.835962 99.665248
INFO:root:train 1650 1.128580e-01 96.839037 99.664029
INFO:root:train 1700 1.128712e-01 96.840094 99.661045
INFO:root:train 1750 1.123785e-01 96.843768 99.666262
INFO:root:train 1800 1.128078e-01 96.832489 99.663381
INFO:root:train 1850 1.125064e-01 96.833637 99.666565
INFO:root:train 1900 1.125731e-01 96.828972 99.666294
INFO:root:train 1950 1.122038e-01 96.845368 99.667638
INFO:root:train 2000 1.122145e-01 96.843766 99.667354
INFO:root:train 2050 1.121101e-01 96.844527 99.670892
INFO:root:train 2100 1.121481e-01 96.842277 99.672031
INFO:root:train 2150 1.122658e-01 96.833595 99.673117
INFO:root:train 2200 1.117094e-01 96.848734 99.675574
INFO:root:train 2250 1.113174e-01 96.856952 99.680003
INFO:root:train 2300 1.115184e-01 96.853270 99.679487
INFO:root:train 2350 1.114852e-01 96.846422 99.679658
INFO:root:train 2400 1.113627e-01 96.841811 99.680472
INFO:root:train 2450 1.113805e-01 96.844400 99.679340
INFO:root:train 2500 1.112744e-01 96.846261 99.678878
INFO:root:train 2550 1.112280e-01 96.846212 99.679047
INFO:root:train 2600 1.117528e-01 96.843161 99.675005
INFO:root:train 2650 1.117304e-01 96.845530 99.675830
INFO:root:train 2700 1.117122e-01 96.849546 99.676046
INFO:root:train 2750 1.115899e-01 96.856257 99.676822
INFO:root:train 2800 1.115752e-01 96.851571 99.678686
INFO:root:train 2850 1.114813e-01 96.851434 99.678841
INFO:root:train 2900 1.117374e-01 96.836220 99.677913
INFO:root:train 2950 1.120509e-01 96.828406 99.677546
INFO:root:train 3000 1.123122e-01 96.815645 99.679274
INFO:root:train 3050 1.124399e-01 96.811496 99.679408
INFO:root:train 3100 1.123545e-01 96.807985 99.682058
INFO:root:train 3150 1.125555e-01 96.800619 99.681649
INFO:root:train 3200 1.125634e-01 96.795435 99.681740
INFO:root:train 3250 1.128205e-01 96.789930 99.681829
INFO:root:train 3300 1.130084e-01 96.784118 99.682388
INFO:root:train 3350 1.128810e-01 96.783609 99.684796
INFO:root:train 3400 1.128393e-01 96.779899 99.685754
INFO:root:train 3450 1.129205e-01 96.775844 99.686232
INFO:root:train 3500 1.129605e-01 96.775921 99.688036
INFO:root:train 3550 1.133577e-01 96.765876 99.685828
INFO:root:train 3600 1.135753e-01 96.762184 99.684549
INFO:root:train_acc 96.760269
INFO:root:valid 000 1.599427e-01 93.750000 100.000000
INFO:root:valid 050 2.467390e-01 93.658088 99.019608
INFO:root:valid 100 2.341898e-01 93.951114 98.978960
INFO:root:valid 150 2.387568e-01 93.894868 98.882450
INFO:root:valid 200 2.332376e-01 93.990983 98.950560
INFO:root:valid 250 2.375594e-01 93.992779 98.885707
INFO:root:valid 300 2.394751e-01 93.947259 98.868355
INFO:root:valid 350 2.403286e-01 93.856838 98.904915
INFO:root:valid 400 2.373763e-01 93.905860 98.901185
INFO:root:valid 450 2.408231e-01 93.822755 98.842849
INFO:root:valid 500 2.426359e-01 93.787425 98.821108
INFO:root:valid 550 2.441439e-01 93.775522 98.800476
INFO:root:valid 600 2.457605e-01 93.742200 98.791077
INFO:root:test_acc 93.734921
INFO:root:epoch 7 lr 8.258768e-04
INFO:root:train 000 7.276054e-02 96.875000 98.437500
INFO:root:train 050 1.099394e-01 96.997549 99.662990
INFO:root:train 100 1.062565e-01 97.107054 99.690594
INFO:root:train 150 1.020624e-01 97.185430 99.668874
INFO:root:train 200 1.044372e-01 97.154851 99.681281
INFO:root:train 250 1.032927e-01 97.161355 99.688745
INFO:root:train 300 1.023731e-01 97.196844 99.714493
INFO:root:train 350 1.045889e-01 97.146546 99.683939
INFO:root:train 400 1.027206e-01 97.151652 99.699969
INFO:root:train 450 1.044293e-01 97.120981 99.688193
INFO:root:train 500 1.053363e-01 97.099551 99.678767
INFO:root:train 550 1.065367e-01 97.073503 99.682396
INFO:root:train 600 1.048645e-01 97.111585 99.690620
INFO:root:train 650 1.054545e-01 97.067012 99.687980
INFO:root:train 700 1.051576e-01 97.068919 99.692404
INFO:root:train 750 1.036363e-01 97.114264 99.702480
INFO:root:train 800 1.031983e-01 97.105181 99.707397
INFO:root:train 850 1.033778e-01 97.091657 99.715408
INFO:root:train 900 1.025775e-01 97.093507 99.720796
INFO:root:train 950 1.018511e-01 97.114879 99.722332
INFO:root:train 1000 1.013094e-01 97.116946 99.726836
INFO:root:train 1050 1.006601e-01 97.127735 99.733884
INFO:root:train 1100 9.991694e-02 97.140384 99.737455
INFO:root:train 1150 1.005884e-01 97.135643 99.735285
INFO:root:train 1200 1.014868e-01 97.114384 99.726790
INFO:root:train 1250 1.021346e-01 97.109812 99.722722
INFO:root:train 1300 1.020913e-01 97.093582 99.723770
INFO:root:train 1350 1.016143e-01 97.097058 99.729367
INFO:root:train 1400 1.015411e-01 97.101401 99.731219
INFO:root:train 1450 1.016032e-01 97.091446 99.731866
INFO:root:train 1500 1.021197e-01 97.083195 99.731429
INFO:root:train 1550 1.025031e-01 97.067416 99.727998
INFO:root:train 1600 1.027940e-01 97.066287 99.725757
INFO:root:train 1650 1.031840e-01 97.069958 99.721760
INFO:root:train 1700 1.036057e-01 97.057797 99.721671
INFO:root:train 1750 1.037618e-01 97.054362 99.717126
INFO:root:train 1800 1.033261e-01 97.069336 99.718906
INFO:root:train 1850 1.036225e-01 97.058178 99.716370
INFO:root:train 1900 1.037944e-01 97.058292 99.715610
INFO:root:train 1950 1.038673e-01 97.053594 99.717292
INFO:root:train 2000 1.037782e-01 97.053817 99.716548
INFO:root:train 2050 1.040366e-01 97.054029 99.716602
INFO:root:train 2100 1.039537e-01 97.055718 99.718884
INFO:root:train 2150 1.041531e-01 97.050064 99.720334
INFO:root:train 2200 1.039068e-01 97.062415 99.721007
INFO:root:train 2250 1.042475e-01 97.057558 99.718875
INFO:root:train 2300 1.040165e-01 97.056307 99.719551
INFO:root:train 2350 1.039960e-01 97.053780 99.718870
INFO:root:train 2400 1.042124e-01 97.050057 99.719518
INFO:root:train 2450 1.041705e-01 97.054136 99.719502
INFO:root:train 2500 1.044351e-01 97.043058 99.718862
INFO:root:train 2550 1.043630e-01 97.047726 99.718860
INFO:root:train 2600 1.044522e-01 97.045607 99.718257
INFO:root:train 2650 1.045551e-01 97.038853 99.717088
INFO:root:train 2700 1.044983e-01 97.039869 99.717119
INFO:root:train 2750 1.043743e-01 97.035737 99.717148
INFO:root:train 2800 1.040343e-01 97.039562 99.718850
INFO:root:train 2850 1.037421e-01 97.051473 99.721589
INFO:root:train 2900 1.039475e-01 97.046816 99.721540
INFO:root:train 2950 1.041179e-01 97.041257 99.719375
INFO:root:train 3000 1.041687e-01 97.037967 99.721447
INFO:root:train 3050 1.045194e-01 97.027614 99.718842
INFO:root:train 3100 1.046197e-01 97.019107 99.720856
INFO:root:train 3150 1.045904e-01 97.017812 99.721815
INFO:root:train 3200 1.048145e-01 97.012164 99.720790
INFO:root:train 3250 1.048227e-01 97.015341 99.719798
INFO:root:train 3300 1.048842e-01 97.007536 99.718362
INFO:root:train 3350 1.048880e-01 97.008356 99.718368
INFO:root:train 3400 1.049362e-01 97.006395 99.715617
INFO:root:train 3450 1.049795e-01 97.004491 99.714304
INFO:root:train 3500 1.048508e-01 97.005320 99.716153
INFO:root:train 3550 1.047781e-01 97.007885 99.715749
INFO:root:train 3600 1.048727e-01 97.006474 99.715357
INFO:root:train_acc 97.009016
INFO:root:valid 000 2.702654e-01 92.187500 100.000000
INFO:root:valid 050 2.407705e-01 93.780637 98.927696
INFO:root:valid 100 2.340073e-01 94.028465 98.855198
INFO:root:valid 150 2.396405e-01 93.936258 98.789321
INFO:root:valid 200 2.351996e-01 94.060945 98.911692
INFO:root:valid 250 2.395674e-01 94.023904 98.842131
INFO:root:valid 300 2.406257e-01 93.968023 98.800872
INFO:root:valid 350 2.422095e-01 93.879095 98.802528
INFO:root:valid 400 2.390004e-01 93.925343 98.811565
INFO:root:valid 450 2.430469e-01 93.840078 98.780488
INFO:root:valid 500 2.454397e-01 93.793663 98.758733
INFO:root:valid 550 2.467230e-01 93.809551 98.735254
INFO:root:valid 600 2.492284e-01 93.757800 98.720882
INFO:root:test_acc 93.755675
INFO:root:epoch 8 lr 7.296104e-04
INFO:root:train 000 8.465833e-02 96.875000 100.000000
INFO:root:train 050 8.507316e-02 97.671569 99.785539
INFO:root:train 100 8.148375e-02 97.663985 99.845297
INFO:root:train 150 8.674422e-02 97.547599 99.803394
INFO:root:train 200 8.699321e-02 97.520211 99.797886
INFO:root:train 250 8.566435e-02 97.578436 99.825697
INFO:root:train 300 8.550095e-02 97.617317 99.828696
INFO:root:train 350 8.472636e-02 97.645121 99.835292
INFO:root:train 400 8.491482e-02 97.630923 99.844140
INFO:root:train 450 8.534062e-02 97.616408 99.833703
INFO:root:train 500 8.649617e-02 97.579840 99.819112
INFO:root:train 550 8.758650e-02 97.558417 99.804333
INFO:root:train 600 8.978160e-02 97.517159 99.784214
INFO:root:train 650 8.834640e-02 97.547043 99.793587
INFO:root:train 700 8.879469e-02 97.503566 99.799394
INFO:root:train 750 8.906308e-02 97.505409 99.798186
INFO:root:train 800 9.071060e-02 97.458255 99.775671
INFO:root:train 850 9.037533e-02 97.464380 99.775999
INFO:root:train 900 9.025059e-02 97.455952 99.779759
INFO:root:train 950 9.016412e-02 97.446767 99.783123
INFO:root:train 1000 9.018639e-02 97.447865 99.784590
INFO:root:train 1050 9.021751e-02 97.456292 99.784431
INFO:root:train 1100 9.052691e-02 97.444085 99.787125
INFO:root:train 1150 9.051039e-02 97.438369 99.786870
INFO:root:train 1200 9.156040e-02 97.420119 99.781432
INFO:root:train 1250 9.243705e-02 97.378347 99.780176
INFO:root:train 1300 9.204417e-02 97.377018 99.779016
INFO:root:train 1350 9.193235e-02 97.372317 99.774473
INFO:root:train 1400 9.206570e-02 97.353453 99.772484
INFO:root:train 1450 9.212813e-02 97.344504 99.776017
INFO:root:train 1500 9.220223e-02 97.349684 99.776191
INFO:root:train 1550 9.197425e-02 97.347477 99.775347
INFO:root:train 1600 9.200809e-02 97.346385 99.773579
INFO:root:train 1650 9.196801e-02 97.345359 99.775704
INFO:root:train 1700 9.241510e-02 97.324184 99.770356
INFO:root:train 1750 9.261343e-02 97.324743 99.767097
INFO:root:train 1800 9.271520e-02 97.313992 99.765755
INFO:root:train 1850 9.252919e-02 97.322393 99.766174
INFO:root:train 1900 9.273190e-02 97.323777 99.764926
INFO:root:train 1950 9.276610e-02 97.313877 99.764544
INFO:root:train 2000 9.303329e-02 97.307596 99.761838
INFO:root:train 2050 9.330751e-02 97.298574 99.761549
INFO:root:train 2100 9.370936e-02 97.285519 99.762018
INFO:root:train 2150 9.398482e-02 97.283967 99.761012
INFO:root:train 2200 9.427322e-02 97.273256 99.762182
INFO:root:train 2250 9.454258e-02 97.265104 99.761217
INFO:root:train 2300 9.478736e-02 97.264776 99.759615
INFO:root:train 2350 9.490055e-02 97.267785 99.759411
INFO:root:train 2400 9.574747e-02 97.247241 99.753358
INFO:root:train 2450 9.604619e-02 97.246660 99.752014
INFO:root:train 2500 9.584867e-02 97.258597 99.749475
INFO:root:train 2550 9.568985e-02 97.259041 99.748873
INFO:root:train 2600 9.592662e-02 97.255262 99.748294
INFO:root:train 2650 9.579271e-02 97.253395 99.748916
INFO:root:train 2700 9.586777e-02 97.246969 99.748936
INFO:root:train 2750 9.584441e-02 97.252704 99.748955
INFO:root:train 2800 9.579621e-02 97.247635 99.749531
INFO:root:train 2850 9.557408e-02 97.244936 99.748444
INFO:root:train 2900 9.558954e-02 97.238021 99.751163
INFO:root:train 2950 9.562302e-02 97.239283 99.750614
INFO:root:train 3000 9.559139e-02 97.245710 99.751645
INFO:root:train 3050 9.562018e-02 97.244244 99.749570
INFO:root:train 3100 9.565269e-02 97.242321 99.749073
INFO:root:train 3150 9.569466e-02 97.243930 99.750575
INFO:root:train 3200 9.568868e-02 97.245978 99.752519
INFO:root:train 3250 9.546600e-02 97.247962 99.754883
INFO:root:train 3300 9.571008e-02 97.241366 99.751022
INFO:root:train 3350 9.586319e-02 97.239164 99.749142
INFO:root:train 3400 9.617628e-02 97.231053 99.748236
INFO:root:train 3450 9.629860e-02 97.233139 99.748261
INFO:root:train 3500 9.635803e-02 97.237843 99.746947
INFO:root:train 3550 9.636909e-02 97.235814 99.747430
INFO:root:train 3600 9.649835e-02 97.239482 99.746164
INFO:root:train_acc 97.237536
INFO:root:valid 000 1.649284e-01 95.312500 100.000000
INFO:root:valid 050 2.340734e-01 94.087010 99.019608
INFO:root:valid 100 2.259577e-01 94.121287 99.025371
INFO:root:valid 150 2.320104e-01 94.163907 98.923841
INFO:root:valid 200 2.237734e-01 94.309701 98.997201
INFO:root:valid 250 2.271753e-01 94.254233 98.954183
INFO:root:valid 300 2.285727e-01 94.227575 98.930648
INFO:root:valid 350 2.322459e-01 94.110577 98.918269
INFO:root:valid 400 2.317682e-01 94.112375 98.916771
INFO:root:valid 450 2.348812e-01 94.037555 98.870565
INFO:root:valid 500 2.367534e-01 94.043164 98.839820
INFO:root:valid 550 2.384120e-01 94.039247 98.823162
INFO:root:valid 600 2.397930e-01 94.035982 98.822275
INFO:root:test_acc 94.033258
INFO:root:epoch 9 lr 6.309138e-04
INFO:root:train 000 1.016844e-01 93.750000 100.000000
INFO:root:train 050 9.624631e-02 97.487745 99.662990
INFO:root:train 100 9.187890e-02 97.478342 99.706064
INFO:root:train 150 8.760477e-02 97.402732 99.730960
INFO:root:train 200 9.063773e-02 97.240361 99.743470
INFO:root:train 250 8.769735e-02 97.323207 99.775896
INFO:root:train 300 8.758374e-02 97.352575 99.776786
INFO:root:train 350 8.682541e-02 97.386930 99.781873
INFO:root:train 400 8.729594e-02 97.358167 99.781796
INFO:root:train 450 8.652743e-02 97.405072 99.781735
INFO:root:train 500 8.642902e-02 97.389596 99.791043
INFO:root:train 550 8.654759e-02 97.382600 99.790154
INFO:root:train 600 8.592290e-02 97.392367 99.799813
INFO:root:train 650 8.539970e-02 97.443836 99.798387
INFO:root:train 700 8.449615e-02 97.465674 99.801623
INFO:root:train 750 8.520446e-02 97.430509 99.802347
INFO:root:train 800 8.498458e-02 97.446551 99.806882
INFO:root:train 850 8.483877e-02 97.438675 99.805376
INFO:root:train 900 8.542190e-02 97.416065 99.807506
INFO:root:train 950 8.550170e-02 97.428693 99.811054
INFO:root:train 1000 8.565570e-02 97.432255 99.808004
INFO:root:train 1050 8.549303e-02 97.442912 99.806732
INFO:root:train 1100 8.538988e-02 97.442666 99.805574
INFO:root:train 1150 8.586380e-02 97.420721 99.804518
INFO:root:train 1200 8.580679e-02 97.420119 99.804850
INFO:root:train 1250 8.557222e-02 97.432054 99.805156
INFO:root:train 1300 8.588471e-02 97.425058 99.804237
INFO:root:train 1350 8.605410e-02 97.432457 99.798760
INFO:root:train 1400 8.616003e-02 97.435983 99.797020
INFO:root:train 1450 8.655949e-02 97.428498 99.795400
INFO:root:train 1500 8.716698e-02 97.434002 99.790764
INFO:root:train 1550 8.699940e-02 97.444189 99.788443
INFO:root:train 1600 8.678215e-02 97.445932 99.788218
INFO:root:train 1650 8.640007e-02 97.458926 99.788954
INFO:root:train 1700 8.714538e-02 97.448192 99.785053
INFO:root:train 1750 8.750989e-02 97.438963 99.786729
INFO:root:train 1800 8.742091e-02 97.451936 99.786577
INFO:root:train 1850 8.706803e-02 97.459988 99.788965
INFO:root:train 1900 8.745235e-02 97.450355 99.783831
INFO:root:train 1950 8.745072e-02 97.446822 99.784566
INFO:root:train 2000 8.754873e-02 97.446589 99.783702
INFO:root:train 2050 8.779427e-02 97.447129 99.783642
INFO:root:train 2100 8.739911e-02 97.461774 99.782098
INFO:root:train 2150 8.762404e-02 97.456125 99.781352
INFO:root:train 2200 8.731526e-02 97.462091 99.781349
INFO:root:train 2250 8.767262e-02 97.455992 99.775794
INFO:root:train 2300 8.801395e-02 97.444046 99.775234
INFO:root:train 2350 8.818349e-02 97.447230 99.774032
INFO:root:train 2400 8.831053e-02 97.443773 99.773532
INFO:root:train 2450 8.823971e-02 97.449383 99.774964
INFO:root:train 2500 8.820380e-02 97.449145 99.773840
INFO:root:train 2550 8.866981e-02 97.447080 99.769086
INFO:root:train 2600 8.882507e-02 97.444493 99.766917
INFO:root:train 2650 8.853606e-02 97.453791 99.767776
INFO:root:train 2700 8.841840e-02 97.452911 99.766869
INFO:root:train 2750 8.844192e-02 97.442975 99.767130
INFO:root:train 2800 8.836584e-02 97.442320 99.768498
INFO:root:train 2850 8.830465e-02 97.437851 99.769270
INFO:root:train 2900 8.826258e-02 97.438922 99.769476
INFO:root:train 2950 8.823395e-02 97.439427 99.770205
INFO:root:train 3000 8.847572e-02 97.435230 99.768827
INFO:root:train 3050 8.837382e-02 97.434243 99.771079
INFO:root:train 3100 8.862463e-02 97.431776 99.769228
INFO:root:train 3150 8.880365e-02 97.423933 99.768427
INFO:root:train 3200 8.887687e-02 97.423169 99.766674
INFO:root:train 3250 8.876102e-02 97.426753 99.767860
INFO:root:train 3300 8.878809e-02 97.422183 99.769009
INFO:root:train 3350 8.917245e-02 97.413552 99.766861
INFO:root:train 3400 8.921894e-02 97.414363 99.765234
INFO:root:train 3450 8.933061e-02 97.416962 99.765919
INFO:root:train 3500 8.924350e-02 97.423058 99.767031
INFO:root:train 3550 8.907638e-02 97.430741 99.768111
INFO:root:train 3600 8.896612e-02 97.431269 99.767860
INFO:root:train_acc 97.429475
INFO:root:valid 000 1.390858e-01 95.312500 100.000000
INFO:root:valid 050 2.296851e-01 94.301471 98.988971
INFO:root:valid 100 2.217171e-01 94.477104 98.932550
INFO:root:valid 150 2.266108e-01 94.432947 98.913493
INFO:root:valid 200 2.184336e-01 94.511816 99.004975
INFO:root:valid 250 2.237874e-01 94.515687 98.954183
INFO:root:valid 300 2.256127e-01 94.450789 98.925457
INFO:root:valid 350 2.274520e-01 94.350962 98.927172
INFO:root:valid 400 2.275960e-01 94.307201 98.908978
INFO:root:valid 450 2.307292e-01 94.203853 98.856707
INFO:root:valid 500 2.339081e-01 94.146083 98.842939
INFO:root:valid 550 2.349957e-01 94.115812 98.828834
INFO:root:valid 600 2.358293e-01 94.113977 98.819676
INFO:root:test_acc 94.111085
INFO:root:epoch 10 lr 5.322171e-04
INFO:root:train 000 3.937077e-02 98.437500 100.000000
INFO:root:train 050 7.268602e-02 97.702206 99.908088
INFO:root:train 100 7.588588e-02 97.617574 99.907178
INFO:root:train 150 7.437656e-02 97.692467 99.865480
INFO:root:train 200 8.017720e-02 97.551306 99.860075
INFO:root:train 250 7.892013e-02 97.597112 99.863048
INFO:root:train 300 7.883777e-02 97.591362 99.875415
INFO:root:train 350 7.702314e-02 97.645121 99.875356
INFO:root:train 400 7.827358e-02 97.603647 99.859726
INFO:root:train 450 7.891978e-02 97.595621 99.833703
INFO:root:train 500 7.896586e-02 97.617265 99.828468
INFO:root:train 550 7.782030e-02 97.649161 99.832691
INFO:root:train 600 7.729627e-02 97.673149 99.838810
INFO:root:train 650 7.799641e-02 97.647849 99.836790
INFO:root:train 700 7.885375e-02 97.606098 99.837286
INFO:root:train 750 7.901480e-02 97.603196 99.837716
INFO:root:train 800 7.938784e-02 97.610409 99.832241
INFO:root:train 850 7.896464e-02 97.620447 99.832917
INFO:root:train 900 7.844940e-02 97.632839 99.831784
INFO:root:train 950 7.795304e-02 97.657072 99.835699
INFO:root:train 1000 7.761840e-02 97.680445 99.837662
INFO:root:train 1050 7.745911e-02 97.692674 99.839439
INFO:root:train 1100 7.702091e-02 97.700954 99.841054
INFO:root:train 1150 7.617765e-02 97.718017 99.845243
INFO:root:train 1200 7.636798e-02 97.723251 99.838676
INFO:root:train 1250 7.591180e-02 97.745554 99.840128
INFO:root:train 1300 7.601433e-02 97.745724 99.839066
INFO:root:train 1350 7.623392e-02 97.741256 99.835770
INFO:root:train 1400 7.649308e-02 97.734877 99.832709
INFO:root:train 1450 7.702074e-02 97.720322 99.833089
INFO:root:train 1500 7.738564e-02 97.712983 99.830321
INFO:root:train 1550 7.826167e-02 97.685969 99.826725
INFO:root:train 1600 7.796983e-02 97.686993 99.827256
INFO:root:train 1650 7.883031e-02 97.669026 99.822078
INFO:root:train 1700 7.872233e-02 97.670488 99.826389
INFO:root:train 1750 7.904514e-02 97.653127 99.826885
INFO:root:train 1800 7.929645e-02 97.649743 99.823015
INFO:root:train 1850 7.964148e-02 97.649075 99.821887
INFO:root:train 1900 7.990295e-02 97.646798 99.819996
INFO:root:train 1950 8.022047e-02 97.646239 99.815800
INFO:root:train 2000 8.071487e-02 97.639462 99.812594
INFO:root:train 2050 8.084828e-02 97.636824 99.813353
INFO:root:train 2100 8.081354e-02 97.632080 99.814077
INFO:root:train 2150 8.097511e-02 97.625378 99.813314
INFO:root:train 2200 8.102029e-02 97.623239 99.811165
INFO:root:train 2250 8.105531e-02 97.620502 99.813277
INFO:root:train 2300 8.105756e-02 97.615846 99.812581
INFO:root:train 2350 8.108625e-02 97.612053 99.813244
INFO:root:train 2400 8.125416e-02 97.610371 99.811277
INFO:root:train 2450 8.124180e-02 97.608119 99.811302
INFO:root:train 2500 8.156781e-02 97.598461 99.811325
INFO:root:train 2550 8.209193e-02 97.591018 99.807061
INFO:root:train 2600 8.220897e-02 97.585664 99.806565
INFO:root:train 2650 8.213062e-02 97.586995 99.809624
INFO:root:train 2700 8.220705e-02 97.592327 99.807942
INFO:root:train 2750 8.196914e-02 97.598033 99.809728
INFO:root:train 2800 8.221155e-02 97.596283 99.808662
INFO:root:train 2850 8.233048e-02 97.597334 99.806537
INFO:root:train 2900 8.224359e-02 97.594579 99.806640
INFO:root:train 2950 8.258580e-02 97.591389 99.806210
INFO:root:train 3000 8.280868e-02 97.583618 99.804753
INFO:root:train 3050 8.270223e-02 97.581223 99.804367
INFO:root:train 3100 8.305372e-02 97.578402 99.803995
INFO:root:train 3150 8.303039e-02 97.578646 99.804130
INFO:root:train 3200 8.314857e-02 97.577905 99.802308
INFO:root:train 3250 8.301850e-02 97.580072 99.802465
INFO:root:train 3300 8.315607e-02 97.576492 99.803563
INFO:root:train 3350 8.308082e-02 97.580480 99.802764
INFO:root:train 3400 8.347974e-02 97.562757 99.802448
INFO:root:train 3450 8.330875e-02 97.565923 99.803499
INFO:root:train 3500 8.323207e-02 97.571230 99.802735
INFO:root:train 3550 8.331457e-02 97.568467 99.802432
INFO:root:train 3600 8.336864e-02 97.571855 99.802138
INFO:root:train_acc 97.573215
INFO:root:valid 000 2.015027e-01 95.312500 100.000000
INFO:root:valid 050 2.195606e-01 94.699755 99.111520
INFO:root:valid 100 2.117429e-01 94.724629 99.071782
INFO:root:valid 150 2.165123e-01 94.660596 98.996275
INFO:root:valid 200 2.087161e-01 94.760572 99.090485
INFO:root:valid 250 2.116606e-01 94.770916 99.041335
INFO:root:valid 300 2.137674e-01 94.699958 99.029277
INFO:root:valid 350 2.163041e-01 94.618056 99.042913
INFO:root:valid 400 2.146579e-01 94.622818 99.041459
INFO:root:valid 450 2.179458e-01 94.526053 98.991824
INFO:root:valid 500 2.198836e-01 94.492265 98.973927
INFO:root:valid 550 2.212211e-01 94.481624 98.945100
INFO:root:valid 600 2.224875e-01 94.475354 98.934068
INFO:root:test_acc 94.474278
INFO:root:epoch 11 lr 4.359507e-04
INFO:root:train 000 1.665612e-01 96.875000 100.000000
INFO:root:train 050 7.785437e-02 97.916667 99.816176
INFO:root:train 100 8.037452e-02 97.710396 99.798886
INFO:root:train 150 7.798143e-02 97.775248 99.803394
INFO:root:train 200 7.480076e-02 97.854478 99.821206
INFO:root:train 250 7.412656e-02 97.927042 99.819472
INFO:root:train 300 7.176595e-02 97.991071 99.828696
INFO:root:train 350 7.264360e-02 97.943376 99.821937
INFO:root:train 400 7.417290e-02 97.927057 99.801278
INFO:root:train 450 7.605720e-02 97.872783 99.799058
INFO:root:train 500 7.380414e-02 97.935379 99.803518
INFO:root:train 550 7.312552e-02 97.941243 99.812840
INFO:root:train 600 7.284724e-02 97.959131 99.812812
INFO:root:train 650 7.217267e-02 97.959869 99.819988
INFO:root:train 700 7.235388e-02 97.960503 99.821683
INFO:root:train 750 7.313895e-02 97.942327 99.818991
INFO:root:train 800 7.216612e-02 97.943976 99.826389
INFO:root:train 850 7.237468e-02 97.938088 99.823737
INFO:root:train 900 7.238473e-02 97.925916 99.824847
INFO:root:train 950 7.248270e-02 97.915024 99.824198
INFO:root:train 1000 7.275903e-02 97.914585 99.820492
INFO:root:train 1050 7.327977e-02 97.894862 99.817138
INFO:root:train 1100 7.358989e-02 97.891122 99.815509
INFO:root:train 1150 7.376572e-02 97.891779 99.816735
INFO:root:train 1200 7.349890e-02 97.892381 99.819161
INFO:root:train 1250 7.395917e-02 97.899181 99.816397
INFO:root:train 1300 7.382251e-02 97.912663 99.817448
INFO:root:train 1350 7.374421e-02 97.922835 99.817265
INFO:root:train 1400 7.355638e-02 97.927819 99.813749
INFO:root:train 1450 7.363884e-02 97.912000 99.813706
INFO:root:train 1500 7.391001e-02 97.901399 99.812625
INFO:root:train 1550 7.397959e-02 97.897526 99.814636
INFO:root:train 1600 7.418249e-02 97.893894 99.813593
INFO:root:train 1650 7.382982e-02 97.899947 99.813560
INFO:root:train 1700 7.395258e-02 97.900132 99.812610
INFO:root:train 1750 7.428484e-02 97.886029 99.811715
INFO:root:train 1800 7.439608e-02 97.872710 99.810869
INFO:root:train 1850 7.434373e-02 97.873616 99.813445
INFO:root:train 1900 7.437448e-02 97.864611 99.816708
INFO:root:train 1950 7.437758e-02 97.868080 99.817401
INFO:root:train 2000 7.424932e-02 97.871377 99.818841
INFO:root:train 2050 7.477036e-02 97.864609 99.813353
INFO:root:train 2100 7.498541e-02 97.860394 99.811846
INFO:root:train 2150 7.495422e-02 97.862913 99.812587
INFO:root:train 2200 7.485728e-02 97.863187 99.814715
INFO:root:train 2250 7.528923e-02 97.847484 99.813277
INFO:root:train 2300 7.504618e-02 97.855552 99.815298
INFO:root:train 2350 7.516328e-02 97.845996 99.815238
INFO:root:train 2400 7.503829e-02 97.842696 99.817133
INFO:root:train 2450 7.504308e-02 97.844630 99.815764
INFO:root:train 2500 7.501082e-02 97.842113 99.816948
INFO:root:train 2550 7.495501e-02 97.840920 99.817474
INFO:root:train 2600 7.492612e-02 97.843978 99.816176
INFO:root:train 2650 7.478655e-02 97.850457 99.817286
INFO:root:train 2700 7.500936e-02 97.846284 99.816040
INFO:root:train 2750 7.484525e-02 97.846806 99.817680
INFO:root:train 2800 7.506079e-02 97.844520 99.817587
INFO:root:train 2850 7.522759e-02 97.846150 99.816950
INFO:root:train 2900 7.523530e-02 97.848802 99.817951
INFO:root:train 2950 7.530713e-02 97.842363 99.817329
INFO:root:train 3000 7.550329e-02 97.839783 99.815686
INFO:root:train 3050 7.551555e-02 97.832678 99.817171
INFO:root:train 3100 7.555800e-02 97.837391 99.814576
INFO:root:train 3150 7.553022e-02 97.838484 99.814543
INFO:root:train 3200 7.560210e-02 97.833197 99.814999
INFO:root:train 3250 7.577221e-02 97.827111 99.812558
INFO:root:train 3300 7.584028e-02 97.827836 99.812083
INFO:root:train 3350 7.588198e-02 97.829473 99.812090
INFO:root:train 3400 7.589097e-02 97.827845 99.812096
INFO:root:train 3450 7.607398e-02 97.819925 99.812102
INFO:root:train 3500 7.604533e-02 97.821158 99.811661
INFO:root:train 3550 7.589560e-02 97.822356 99.812113
INFO:root:train 3600 7.606540e-02 97.815711 99.812552
INFO:root:train_acc 97.812063
INFO:root:valid 000 1.289549e-01 95.312500 100.000000
INFO:root:valid 050 2.272678e-01 94.638480 99.050245
INFO:root:valid 100 2.163610e-01 94.832921 99.040842
INFO:root:valid 150 2.199376e-01 94.681291 98.965232
INFO:root:valid 200 2.129128e-01 94.737251 99.074938
INFO:root:valid 250 2.178523e-01 94.658865 99.047560
INFO:root:valid 300 2.181098e-01 94.616902 99.008513
INFO:root:valid 350 2.183064e-01 94.569088 99.016204
INFO:root:valid 400 2.158764e-01 94.611128 99.033666
INFO:root:valid 450 2.194080e-01 94.543376 98.981430
INFO:root:valid 500 2.210883e-01 94.539047 98.970808
INFO:root:valid 550 2.222264e-01 94.546847 98.939428
INFO:root:valid 600 2.227646e-01 94.568948 98.928869
INFO:root:test_acc 94.565076
INFO:root:epoch 12 lr 3.444849e-04
INFO:root:train 000 5.465719e-02 96.875000 100.000000
INFO:root:train 050 6.393784e-02 97.886029 99.877451
INFO:root:train 100 5.864167e-02 98.081683 99.891708
INFO:root:train 150 5.869225e-02 98.116722 99.834437
INFO:root:train 200 6.402247e-02 97.994403 99.828980
INFO:root:train 250 6.515606e-02 98.001743 99.831922
INFO:root:train 300 6.922591e-02 97.949543 99.813123
INFO:root:train 350 6.966057e-02 97.965634 99.821937
INFO:root:train 400 6.877816e-02 97.989401 99.824657
INFO:root:train 450 6.918885e-02 97.983647 99.823309
INFO:root:train 500 6.925682e-02 97.960329 99.834706
INFO:root:train 550 6.925007e-02 97.927064 99.835526
INFO:root:train 600 6.978636e-02 97.894135 99.833611
INFO:root:train 650 6.981478e-02 97.883065 99.841590
INFO:root:train 700 6.833947e-02 97.931526 99.848431
INFO:root:train 750 6.760684e-02 97.961052 99.848119
INFO:root:train 800 6.727257e-02 97.977138 99.851748
INFO:root:train 850 6.721983e-02 97.980317 99.856786
INFO:root:train 900 6.761640e-02 97.958865 99.850860
INFO:root:train 950 6.715599e-02 97.982387 99.852129
INFO:root:train 1000 6.775215e-02 97.969218 99.851711
INFO:root:train 1050 6.739296e-02 97.981089 99.854305
INFO:root:train 1100 6.727923e-02 97.991882 99.850988
INFO:root:train 1150 6.783312e-02 97.973230 99.847958
INFO:root:train 1200 6.736543e-02 97.989956 99.851686
INFO:root:train 1250 6.743839e-02 97.995354 99.852618
INFO:root:train 1300 6.702831e-02 98.005140 99.853478
INFO:root:train 1350 6.703572e-02 98.008420 99.857744
INFO:root:train 1400 6.724882e-02 98.002543 99.853899
INFO:root:train 1450 6.713303e-02 98.004609 99.855703
INFO:root:train 1500 6.734593e-02 98.008619 99.854264
INFO:root:train 1550 6.751571e-02 97.995245 99.853925
INFO:root:train 1600 6.717616e-02 98.007105 99.853607
INFO:root:train 1650 6.773213e-02 97.989855 99.851416
INFO:root:train 1700 6.783370e-02 97.986479 99.851190
INFO:root:train 1750 6.819620e-02 97.975264 99.851870
INFO:root:train 1800 6.828628e-02 97.973348 99.853380
INFO:root:train 1850 6.820199e-02 97.974912 99.857341
INFO:root:train 1900 6.844452e-02 97.973106 99.856983
INFO:root:train 1950 6.865112e-02 97.959380 99.858246
INFO:root:train 2000 6.841044e-02 97.965080 99.860226
INFO:root:train 2050 6.844420e-02 97.962884 99.858301
INFO:root:train 2100 6.889889e-02 97.961536 99.852749
INFO:root:train 2150 6.930769e-02 97.958798 99.851087
INFO:root:train 2200 6.913431e-02 97.966833 99.853050
INFO:root:train 2250 6.965096e-02 97.951605 99.851455
INFO:root:train 2300 7.002459e-02 97.943829 99.849250
INFO:root:train 2350 7.027534e-02 97.937048 99.849133
INFO:root:train 2400 7.006974e-02 97.943565 99.850974
INFO:root:train 2450 7.012062e-02 97.944716 99.848276
INFO:root:train 2500 7.016464e-02 97.941448 99.847561
INFO:root:train 2550 7.027909e-02 97.941984 99.845649
INFO:root:train 2600 7.069708e-02 97.942498 99.845012
INFO:root:train 2650 7.087282e-02 97.934152 99.842630
INFO:root:train 2700 7.103013e-02 97.929586 99.842072
INFO:root:train 2750 7.106814e-02 97.926322 99.840967
INFO:root:train 2800 7.105094e-02 97.928195 99.841574
INFO:root:train 2850 7.083097e-02 97.930551 99.842709
INFO:root:train 2900 7.073713e-02 97.928516 99.844881
INFO:root:train 2950 7.071998e-02 97.931316 99.844332
INFO:root:train 3000 7.046040e-02 97.938187 99.845364
INFO:root:train 3050 7.049208e-02 97.935103 99.843801
INFO:root:train 3100 7.069472e-02 97.934134 99.843297
INFO:root:train 3150 7.059671e-02 97.936171 99.844295
INFO:root:train 3200 7.061301e-02 97.934727 99.843311
INFO:root:train 3250 7.050842e-02 97.933328 99.844279
INFO:root:train 3300 7.057978e-02 97.929605 99.843797
INFO:root:train 3350 7.066959e-02 97.930655 99.844263
INFO:root:train 3400 7.053709e-02 97.933972 99.844255
INFO:root:train 3450 7.049112e-02 97.934023 99.843343
INFO:root:train 3500 7.037214e-02 97.936750 99.843348
INFO:root:train 3550 7.036702e-02 97.935881 99.842034
INFO:root:train 3600 7.046651e-02 97.931998 99.841190
INFO:root:train_acc 97.933854
INFO:root:valid 000 2.023842e-01 95.312500 100.000000
INFO:root:valid 050 2.172518e-01 95.373775 99.142157
INFO:root:valid 100 2.078028e-01 95.420792 99.040842
INFO:root:valid 150 2.099345e-01 95.291805 99.006623
INFO:root:valid 200 2.036721e-01 95.289179 99.098259
INFO:root:valid 250 2.063957e-01 95.268924 99.060010
INFO:root:valid 300 2.044336e-01 95.307309 99.044850
INFO:root:valid 350 2.066665e-01 95.227920 99.069623
INFO:root:valid 400 2.052754e-01 95.183915 99.088217
INFO:root:valid 450 2.083531e-01 95.097700 99.029933
INFO:root:valid 500 2.102462e-01 95.050524 99.014471
INFO:root:valid 550 2.114340e-01 95.020417 98.987636
INFO:root:valid 600 2.122675e-01 94.992720 98.999064
INFO:root:test_acc 94.987937
INFO:root:epoch 13 lr 2.600720e-04
INFO:root:train 000 7.684127e-02 95.312500 100.000000
INFO:root:train 050 6.685412e-02 97.824755 99.816176
INFO:root:train 100 7.018885e-02 97.834158 99.829827
INFO:root:train 150 6.700224e-02 97.961507 99.855132
INFO:root:train 200 6.458027e-02 98.087687 99.852301
INFO:root:train 250 6.393546e-02 98.126245 99.869273
INFO:root:train 300 6.346394e-02 98.126038 99.875415
INFO:root:train 350 6.550766e-02 98.085826 99.879808
INFO:root:train 400 6.412574e-02 98.141365 99.883105
INFO:root:train 450 6.430693e-02 98.149945 99.875277
INFO:root:train 500 6.395263e-02 98.169286 99.878368
INFO:root:train 550 6.375301e-02 98.173775 99.883734
INFO:root:train 600 6.355225e-02 98.161918 99.880408
INFO:root:train 650 6.404927e-02 98.183084 99.875192
INFO:root:train 700 6.430333e-02 98.167796 99.875178
INFO:root:train 750 6.394138e-02 98.191994 99.873086
INFO:root:train 800 6.338748e-02 98.205368 99.877107
INFO:root:train 850 6.294117e-02 98.215335 99.876983
INFO:root:train 900 6.349917e-02 98.196448 99.875139
INFO:root:train 950 6.324930e-02 98.207479 99.871845
INFO:root:train 1000 6.311976e-02 98.212725 99.873564
INFO:root:train 1050 6.344105e-02 98.193685 99.870659
INFO:root:train 1100 6.319786e-02 98.193404 99.872275
INFO:root:train 1150 6.271709e-02 98.205365 99.872394
INFO:root:train 1200 6.333332e-02 98.192912 99.867298
INFO:root:train 1250 6.316143e-02 98.205186 99.868855
INFO:root:train 1300 6.303706e-02 98.204506 99.872694
INFO:root:train 1350 6.309540e-02 98.193468 99.871623
INFO:root:train 1400 6.353608e-02 98.179872 99.867282
INFO:root:train 1450 6.388123e-02 98.170443 99.867548
INFO:root:train 1500 6.389359e-02 98.167888 99.868837
INFO:root:train 1550 6.395742e-02 98.167513 99.868029
INFO:root:train 1600 6.403897e-02 98.163257 99.869222
INFO:root:train 1650 6.417952e-02 98.162099 99.870344
INFO:root:train 1700 6.435609e-02 98.153660 99.869562
INFO:root:train 1750 6.445245e-02 98.140348 99.871502
INFO:root:train 1800 6.399433e-02 98.153803 99.873334
INFO:root:train 1850 6.388946e-02 98.159778 99.874223
INFO:root:train 1900 6.367870e-02 98.165439 99.876710
INFO:root:train 1950 6.348892e-02 98.166805 99.877467
INFO:root:train 2000 6.343433e-02 98.165761 99.875843
INFO:root:train 2050 6.331458e-02 98.169338 99.874299
INFO:root:train 2100 6.333244e-02 98.174233 99.870597
INFO:root:train 2150 6.352612e-02 98.172362 99.865615
INFO:root:train 2200 6.384116e-02 98.169866 99.862988
INFO:root:train 2250 6.366848e-02 98.173728 99.863255
INFO:root:train 2300 6.385691e-02 98.180818 99.860115
INFO:root:train 2350 6.423320e-02 98.168333 99.859767
INFO:root:train 2400 6.442274e-02 98.169383 99.858783
INFO:root:train 2450 6.437440e-02 98.162102 99.860389
INFO:root:train 2500 6.422588e-02 98.164484 99.861930
INFO:root:train 2550 6.415219e-02 98.163098 99.864024
INFO:root:train 2600 6.424497e-02 98.162966 99.864836
INFO:root:train 2650 6.426310e-02 98.162839 99.865027
INFO:root:train 2700 6.419665e-02 98.165031 99.866369
INFO:root:train 2750 6.440071e-02 98.158624 99.865958
INFO:root:train 2800 6.433787e-02 98.159140 99.862214
INFO:root:train 2850 6.425031e-02 98.166762 99.861343
INFO:root:train 2900 6.443025e-02 98.160656 99.861578
INFO:root:train 2950 6.432715e-02 98.160581 99.860217
INFO:root:train 3000 6.431504e-02 98.157906 99.859422
INFO:root:train 3050 6.414440e-02 98.163000 99.860701
INFO:root:train 3100 6.414464e-02 98.162891 99.862444
INFO:root:train 3150 6.427967e-02 98.161298 99.861651
INFO:root:train 3200 6.438109e-02 98.158290 99.860883
INFO:root:train 3250 6.460331e-02 98.148647 99.861100
INFO:root:train 3300 6.476332e-02 98.145922 99.860838
INFO:root:train 3350 6.468693e-02 98.148407 99.860583
INFO:root:train 3400 6.475501e-02 98.146685 99.859876
INFO:root:train 3450 6.455705e-02 98.153615 99.860548
INFO:root:train 3500 6.457600e-02 98.150975 99.860308
INFO:root:train 3550 6.477067e-02 98.144449 99.860515
INFO:root:train 3600 6.484084e-02 98.142877 99.859414
INFO:root:train_acc 98.142147
INFO:root:valid 000 1.617874e-01 93.750000 100.000000
INFO:root:valid 050 2.321039e-01 94.730392 99.111520
INFO:root:valid 100 2.164043e-01 95.049505 99.056312
INFO:root:valid 150 2.172112e-01 95.084851 99.006623
INFO:root:valid 200 2.090615e-01 95.149254 99.098259
INFO:root:valid 250 2.129948e-01 95.156873 99.066235
INFO:root:valid 300 2.120604e-01 95.161960 99.039659
INFO:root:valid 350 2.138260e-01 95.072115 99.060719
INFO:root:valid 400 2.122156e-01 95.055330 99.076527
INFO:root:valid 450 2.142909e-01 94.976441 99.029933
INFO:root:valid 500 2.159150e-01 94.950724 99.026946
INFO:root:valid 550 2.168942e-01 94.909823 99.015994
INFO:root:valid 600 2.176197e-01 94.909526 99.004264
INFO:root:test_acc 94.904921
INFO:root:epoch 14 lr 1.847904e-04
INFO:root:train 000 2.115104e-01 98.437500 98.437500
INFO:root:train 050 6.778326e-02 98.192402 99.846814
INFO:root:train 100 6.139443e-02 98.298267 99.876238
INFO:root:train 150 5.843145e-02 98.385762 99.886175
INFO:root:train 200 5.922453e-02 98.313122 99.875622
INFO:root:train 250 5.799319e-02 98.350349 99.887948
INFO:root:train 300 5.924451e-02 98.338870 99.875415
INFO:root:train 350 5.985541e-02 98.308405 99.879808
INFO:root:train 400 6.024316e-02 98.328398 99.875312
INFO:root:train 450 6.095829e-02 98.295455 99.875277
INFO:root:train 500 6.019832e-02 98.337700 99.878368
INFO:root:train 550 6.025385e-02 98.324070 99.878063
INFO:root:train 600 6.016920e-02 98.328307 99.880408
INFO:root:train 650 6.024411e-02 98.329493 99.875192
INFO:root:train 700 6.063255e-02 98.314907 99.872949
INFO:root:train 750 6.076856e-02 98.318908 99.875166
INFO:root:train 800 6.094277e-02 98.293149 99.875156
INFO:root:train 850 6.146028e-02 98.275925 99.873311
INFO:root:train 900 6.159331e-02 98.271018 99.869936
INFO:root:train 950 6.133928e-02 98.271556 99.873488
INFO:root:train 1000 6.081770e-02 98.282967 99.878247
INFO:root:train 1050 6.099881e-02 98.284372 99.876606
INFO:root:train 1100 6.154566e-02 98.268619 99.875114
INFO:root:train 1150 6.139968e-02 98.273241 99.879181
INFO:root:train 1200 6.138158e-02 98.268370 99.880308
INFO:root:train 1250 6.142685e-02 98.263889 99.880096
INFO:root:train 1300 6.142139e-02 98.263355 99.881101
INFO:root:train 1350 6.124295e-02 98.262861 99.880875
INFO:root:train 1400 6.131502e-02 98.256825 99.881781
INFO:root:train 1450 6.107202e-02 98.254437 99.882624
INFO:root:train 1500 6.076216e-02 98.275108 99.882370
INFO:root:train 1550 6.084740e-02 98.271277 99.882132
INFO:root:train 1600 6.070496e-02 98.278420 99.881910
INFO:root:train 1650 6.080416e-02 98.268095 99.884540
INFO:root:train 1700 6.080232e-02 98.271238 99.884259
INFO:root:train 1750 6.084170e-02 98.279555 99.884887
INFO:root:train 1800 6.045155e-02 98.283939 99.886348
INFO:root:train 1850 6.027643e-02 98.288088 99.886041
INFO:root:train 1900 6.005297e-02 98.292017 99.884929
INFO:root:train 1950 6.016520e-02 98.286135 99.882272
INFO:root:train 2000 6.025403e-02 98.282109 99.881309
INFO:root:train 2050 6.042794e-02 98.279803 99.880394
INFO:root:train 2100 6.032820e-02 98.285043 99.878778
INFO:root:train 2150 6.040266e-02 98.283502 99.880143
INFO:root:train 2200 6.051275e-02 98.269963 99.882156
INFO:root:train 2250 6.052099e-02 98.264660 99.881303
INFO:root:train 2300 6.068751e-02 98.261625 99.880487
INFO:root:train 2350 6.052002e-02 98.262707 99.881699
INFO:root:train 2400 6.049359e-02 98.265046 99.882861
INFO:root:train 2450 6.039200e-02 98.266014 99.883338
INFO:root:train 2500 6.043068e-02 98.260696 99.883172
INFO:root:train 2550 6.039032e-02 98.262324 99.882399
INFO:root:train 2600 6.048822e-02 98.265090 99.882257
INFO:root:train 2650 6.055067e-02 98.265985 99.881531
INFO:root:train 2700 6.079020e-02 98.257590 99.879096
INFO:root:train 2750 6.078234e-02 98.251772 99.879021
INFO:root:train 2800 6.064358e-02 98.253972 99.879507
INFO:root:train 2850 6.076555e-02 98.249518 99.880524
INFO:root:train 2900 6.078310e-02 98.245217 99.882045
INFO:root:train 2950 6.091375e-02 98.244769 99.881396
INFO:root:train 3000 6.065161e-02 98.252145 99.882331
INFO:root:train 3050 6.071857e-02 98.250061 99.881699
INFO:root:train 3100 6.087616e-02 98.238975 99.883606
INFO:root:train 3150 6.089195e-02 98.235679 99.883470
INFO:root:train 3200 6.106376e-02 98.233462 99.882849
INFO:root:train 3250 6.110882e-02 98.231794 99.884170
INFO:root:train 3300 6.104764e-02 98.234437 99.884505
INFO:root:train 3350 6.108076e-02 98.232804 99.882964
INFO:root:train 3400 6.098001e-02 98.237191 99.883766
INFO:root:train 3450 6.077702e-02 98.239188 99.884997
INFO:root:train 3500 6.075006e-02 98.241574 99.885301
INFO:root:train 3550 6.083816e-02 98.239052 99.884716
INFO:root:train 3600 6.075342e-02 98.241374 99.885448
INFO:root:train_acc 98.235965
INFO:root:valid 000 1.680002e-01 95.312500 100.000000
INFO:root:valid 050 2.105431e-01 94.883578 99.325980
INFO:root:valid 100 2.013241e-01 95.204208 99.211015
INFO:root:valid 150 2.060015e-01 95.229719 99.151490
INFO:root:valid 200 1.989360e-01 95.273632 99.222637
INFO:root:valid 250 2.023009e-01 95.250249 99.178287
INFO:root:valid 300 2.027037e-01 95.213870 99.153862
INFO:root:valid 350 2.033119e-01 95.156695 99.167557
INFO:root:valid 400 2.014502e-01 95.172226 99.158354
INFO:root:valid 450 2.044686e-01 95.094235 99.106153
INFO:root:valid 500 2.065801e-01 95.022455 99.095559
INFO:root:valid 550 2.077606e-01 94.975045 99.069873
INFO:root:valid 600 2.083825e-01 94.992720 99.064060
INFO:root:test_acc 94.987937
INFO:root:epoch 15 lr 1.204938e-04
INFO:root:train 000 1.126343e-01 96.875000 100.000000
INFO:root:train 050 4.837332e-02 98.468137 99.938725
INFO:root:train 100 4.821778e-02 98.530322 99.922649
INFO:root:train 150 5.583812e-02 98.385762 99.896523
INFO:root:train 200 5.577891e-02 98.476368 99.914490
INFO:root:train 250 5.567829e-02 98.474851 99.912849
INFO:root:train 300 5.648031e-02 98.437500 99.890988
INFO:root:train 350 5.650883e-02 98.415242 99.888711
INFO:root:train 400 5.806701e-02 98.351777 99.883105
INFO:root:train 450 5.745458e-02 98.395926 99.889135
INFO:root:train 500 5.648996e-02 98.428144 99.893962
INFO:root:train 550 5.589626e-02 98.451679 99.895077
INFO:root:train 600 5.551664e-02 98.447899 99.898606
INFO:root:train 650 5.561884e-02 98.435100 99.894393
INFO:root:train 700 5.574119e-02 98.430813 99.895239
INFO:root:train 750 5.664225e-02 98.385486 99.891811
INFO:root:train 800 5.659826e-02 98.375078 99.888811
INFO:root:train 850 5.733524e-02 98.354877 99.886163
INFO:root:train 900 5.699519e-02 98.355993 99.890746
INFO:root:train 950 5.636921e-02 98.375066 99.894848
INFO:root:train 1000 5.638708e-02 98.376623 99.895417
INFO:root:train 1050 5.666221e-02 98.367626 99.895932
INFO:root:train 1100 5.660139e-02 98.376476 99.890724
INFO:root:train 1150 5.659636e-02 98.370982 99.891399
INFO:root:train 1200 5.688650e-02 98.364644 99.892017
INFO:root:train 1250 5.685761e-02 98.366307 99.888839
INFO:root:train 1300 5.684837e-02 98.359435 99.889508
INFO:root:train 1350 5.663176e-02 98.368107 99.890128
INFO:root:train 1400 5.690709e-02 98.366122 99.888473
INFO:root:train 1450 5.676636e-02 98.364275 99.890162
INFO:root:train 1500 5.671187e-02 98.369837 99.888616
INFO:root:train 1550 5.687199e-02 98.365974 99.884147
INFO:root:train 1600 5.666869e-02 98.362352 99.885814
INFO:root:train 1650 5.647919e-02 98.374091 99.886432
INFO:root:train 1700 5.678720e-02 98.359421 99.884259
INFO:root:train 1750 5.639306e-02 98.370574 99.887564
INFO:root:train 1800 5.642040e-02 98.369829 99.887215
INFO:root:train 1850 5.615415e-02 98.375034 99.887730
INFO:root:train 1900 5.623772e-02 98.375855 99.886573
INFO:root:train 1950 5.626611e-02 98.371028 99.887878
INFO:root:train 2000 5.660535e-02 98.366442 99.886775
INFO:root:train 2050 5.678570e-02 98.361318 99.885726
INFO:root:train 2100 5.661955e-02 98.365362 99.886215
INFO:root:train 2150 5.652640e-02 98.373576 99.885954
INFO:root:train 2200 5.672821e-02 98.372899 99.883576
INFO:root:train 2250 5.655637e-02 98.377110 99.884079
INFO:root:train 2300 5.680962e-02 98.373669 99.882524
INFO:root:train 2350 5.706667e-02 98.365057 99.880370
INFO:root:train 2400 5.711627e-02 98.362661 99.881560
INFO:root:train 2450 5.740620e-02 98.357176 99.881426
INFO:root:train 2500 5.737291e-02 98.356907 99.883172
INFO:root:train 2550 5.731754e-02 98.357262 99.884849
INFO:root:train 2600 5.714954e-02 98.360006 99.885861
INFO:root:train 2650 5.711520e-02 98.357931 99.886835
INFO:root:train 2700 5.714251e-02 98.353619 99.886616
INFO:root:train 2750 5.751839e-02 98.344920 99.886405
INFO:root:train 2800 5.746635e-02 98.347130 99.886759
INFO:root:train 2850 5.734176e-02 98.349263 99.887101
INFO:root:train 2900 5.752624e-02 98.340012 99.887970
INFO:root:train 2950 5.775979e-02 98.331074 99.886691
INFO:root:train 3000 5.792344e-02 98.334409 99.887537
INFO:root:train 3050 5.773822e-02 98.336611 99.889381
INFO:root:train 3100 5.804941e-02 98.329672 99.888645
INFO:root:train 3150 5.802233e-02 98.325432 99.889916
INFO:root:train 3200 5.796631e-02 98.325719 99.890659
INFO:root:train 3250 5.805005e-02 98.324073 99.892341
INFO:root:train 3300 5.805762e-02 98.324845 99.893972
INFO:root:train 3350 5.809167e-02 98.325593 99.893688
INFO:root:train 3400 5.822532e-02 98.324022 99.893414
INFO:root:train 3450 5.827798e-02 98.320686 99.892694
INFO:root:train 3500 5.828446e-02 98.318338 99.892888
INFO:root:train 3550 5.830310e-02 98.315175 99.893516
INFO:root:train 3600 5.827762e-02 98.314270 99.894561
INFO:root:train_acc 98.315581
INFO:root:valid 000 1.927534e-01 93.750000 100.000000
INFO:root:valid 050 2.134767e-01 95.189951 99.264706
INFO:root:valid 100 2.019616e-01 95.343441 99.241955
INFO:root:valid 150 2.061931e-01 95.271109 99.161838
INFO:root:valid 200 1.987887e-01 95.304726 99.230410
INFO:root:valid 250 2.037719e-01 95.324950 99.172062
INFO:root:valid 300 2.023204e-01 95.296927 99.159053
INFO:root:valid 350 2.033321e-01 95.250178 99.154202
INFO:root:valid 400 2.009675e-01 95.265742 99.170044
INFO:root:valid 450 2.040558e-01 95.191242 99.109618
INFO:root:valid 500 2.062024e-01 95.112899 99.092440
INFO:root:valid 550 2.079025e-01 95.077132 99.078380
INFO:root:valid 600 2.084644e-01 95.065516 99.082259
INFO:root:test_acc 95.065764
INFO:root:epoch 16 lr 6.876548e-05
INFO:root:train 000 3.678501e-03 100.000000 100.000000
INFO:root:train 050 5.349822e-02 98.560049 99.908088
INFO:root:train 100 5.646233e-02 98.406559 99.876238
INFO:root:train 150 5.536733e-02 98.437500 99.855132
INFO:root:train 200 5.485004e-02 98.390858 99.867848
INFO:root:train 250 5.513854e-02 98.418825 99.856823
INFO:root:train 300 5.490453e-02 98.473837 99.870224
INFO:root:train 350 5.483885e-02 98.468661 99.862001
INFO:root:train 400 5.410983e-02 98.492051 99.863622
INFO:root:train 450 5.487038e-02 98.458287 99.861419
INFO:root:train 500 5.462594e-02 98.462450 99.865893
INFO:root:train 550 5.449004e-02 98.471529 99.872391
INFO:root:train 600 5.366005e-02 98.499896 99.870008
INFO:root:train 650 5.348886e-02 98.504704 99.870392
INFO:root:train 700 5.270517e-02 98.528887 99.877407
INFO:root:train 750 5.262761e-02 98.512400 99.881408
INFO:root:train 800 5.277698e-02 98.511626 99.882959
INFO:root:train 850 5.309990e-02 98.496254 99.889835
INFO:root:train 900 5.260502e-02 98.505133 99.895949
INFO:root:train 950 5.277669e-02 98.509792 99.896491
INFO:root:train 1000 5.285452e-02 98.504620 99.896978
INFO:root:train 1050 5.293613e-02 98.489534 99.895932
INFO:root:train 1100 5.265568e-02 98.498524 99.900658
INFO:root:train 1150 5.304015e-02 98.490443 99.896829
INFO:root:train 1200 5.306350e-02 98.485637 99.897221
INFO:root:train 1250 5.344750e-02 98.483713 99.891337
INFO:root:train 1300 5.356685e-02 98.479535 99.890709
INFO:root:train 1350 5.431422e-02 98.459474 99.891284
INFO:root:train 1400 5.438458e-02 98.465382 99.888473
INFO:root:train 1450 5.430756e-02 98.465498 99.888008
INFO:root:train 1500 5.446749e-02 98.456238 99.887575
INFO:root:train 1550 5.417350e-02 98.463693 99.888177
INFO:root:train 1600 5.433746e-02 98.461899 99.889717
INFO:root:train 1650 5.483740e-02 98.452642 99.887379
INFO:root:train 1700 5.495829e-02 98.457709 99.887934
INFO:root:train 1750 5.487866e-02 98.453562 99.889349
INFO:root:train 1800 5.481540e-02 98.450514 99.890686
INFO:root:train 1850 5.481440e-02 98.449318 99.889418
INFO:root:train 1900 5.487283e-02 98.451473 99.888217
INFO:root:train 1950 5.500304e-02 98.452717 99.888679
INFO:root:train 2000 5.499402e-02 98.454679 99.890680
INFO:root:train 2050 5.511748e-02 98.451975 99.891059
INFO:root:train 2100 5.508842e-02 98.458323 99.892164
INFO:root:train 2150 5.513271e-02 98.456387 99.892492
INFO:root:train 2200 5.503777e-02 98.457377 99.890675
INFO:root:train 2250 5.512944e-02 98.449300 99.891021
INFO:root:train 2300 5.513791e-02 98.442253 99.892031
INFO:root:train 2350 5.535319e-02 98.438165 99.891004
INFO:root:train 2400 5.540534e-02 98.441405 99.890020
INFO:root:train 2450 5.536948e-02 98.443875 99.889713
INFO:root:train 2500 5.545813e-02 98.438125 99.888794
INFO:root:train 2550 5.541265e-02 98.434437 99.889137
INFO:root:train 2600 5.541912e-02 98.435097 99.889466
INFO:root:train 2650 5.551658e-02 98.432195 99.889193
INFO:root:train 2700 5.539419e-02 98.434608 99.890087
INFO:root:train 2750 5.539865e-02 98.435796 99.888109
INFO:root:train 2800 5.554130e-02 98.430248 99.888433
INFO:root:train 2850 5.558418e-02 98.429827 99.889293
INFO:root:train 2900 5.554972e-02 98.426189 99.890124
INFO:root:train 2950 5.570020e-02 98.421616 99.890397
INFO:root:train 3000 5.570369e-02 98.416674 99.891703
INFO:root:train 3050 5.565010e-02 98.415479 99.891941
INFO:root:train 3100 5.558219e-02 98.414322 99.892676
INFO:root:train 3150 5.538594e-02 98.423120 99.893883
INFO:root:train 3200 5.541949e-02 98.420415 99.893100
INFO:root:train 3250 5.535859e-02 98.416833 99.893783
INFO:root:train 3300 5.529271e-02 98.418566 99.894445
INFO:root:train 3350 5.529705e-02 98.417916 99.893688
INFO:root:train 3400 5.526198e-02 98.417745 99.893873
INFO:root:train 3450 5.527049e-02 98.419842 99.894052
INFO:root:train 3500 5.549468e-02 98.415631 99.892441
INFO:root:train 3550 5.535768e-02 98.419459 99.893076
INFO:root:train 3600 5.549697e-02 98.417106 99.893259
INFO:root:train_acc 98.420158
INFO:root:valid 000 1.313765e-01 93.750000 100.000000
INFO:root:valid 050 2.099603e-01 95.343137 99.295343
INFO:root:valid 100 1.990016e-01 95.467203 99.241955
INFO:root:valid 150 2.033951e-01 95.395281 99.172185
INFO:root:valid 200 1.961115e-01 95.429104 99.230410
INFO:root:valid 250 2.009388e-01 95.437002 99.184512
INFO:root:valid 300 2.003267e-01 95.405939 99.169435
INFO:root:valid 350 2.014784e-01 95.325855 99.172009
INFO:root:valid 400 1.993422e-01 95.355362 99.181733
INFO:root:valid 450 2.027643e-01 95.263997 99.116547
INFO:root:valid 500 2.048304e-01 95.212700 99.108034
INFO:root:valid 550 2.065601e-01 95.167877 99.086887
INFO:root:valid 600 2.071964e-01 95.161710 99.079659
INFO:root:test_acc 95.156562
INFO:root:epoch 17 lr 3.087912e-05
INFO:root:train 000 4.702598e-02 98.437500 100.000000
INFO:root:train 050 4.321413e-02 98.866422 99.908088
INFO:root:train 100 4.757016e-02 98.685025 99.922649
INFO:root:train 150 5.229916e-02 98.561672 99.896523
INFO:root:train 200 5.241260e-02 98.561878 99.898943
INFO:root:train 250 5.125665e-02 98.568227 99.906624
INFO:root:train 300 4.930008e-02 98.608804 99.911752
INFO:root:train 350 5.140554e-02 98.535434 99.915420
INFO:root:train 400 5.090803e-02 98.562188 99.922070
INFO:root:train 450 4.988682e-02 98.600333 99.920316
INFO:root:train 500 5.038205e-02 98.587201 99.922031
INFO:root:train 550 5.061853e-02 98.590631 99.917763
INFO:root:train 600 5.082554e-02 98.577891 99.911606
INFO:root:train 650 5.054322e-02 98.591110 99.913594
INFO:root:train 700 5.103581e-02 98.575695 99.913071
INFO:root:train 750 5.012169e-02 98.601864 99.914697
INFO:root:train 800 5.038841e-02 98.597456 99.910268
INFO:root:train 850 5.009934e-02 98.599075 99.913704
INFO:root:train 900 5.025629e-02 98.597045 99.913291
INFO:root:train 950 5.027902e-02 98.596872 99.911278
INFO:root:train 1000 5.049107e-02 98.576424 99.912587
INFO:root:train 1050 5.034475e-02 98.587655 99.915259
INFO:root:train 1100 5.078257e-02 98.570901 99.909173
INFO:root:train 1150 5.044469e-02 98.573252 99.911762
INFO:root:train 1200 5.062772e-02 98.557192 99.911532
INFO:root:train 1250 5.063925e-02 98.557404 99.912570
INFO:root:train 1300 5.038466e-02 98.561203 99.914729
INFO:root:train 1350 5.118594e-02 98.542746 99.912102
INFO:root:train 1400 5.112968e-02 98.550143 99.907432
INFO:root:train 1450 5.105753e-02 98.552722 99.908468
INFO:root:train 1500 5.108392e-02 98.557212 99.907353
INFO:root:train 1550 5.123271e-02 98.546301 99.908325
INFO:root:train 1600 5.118507e-02 98.542903 99.908260
INFO:root:train 1650 5.082871e-02 98.552960 99.910092
INFO:root:train 1700 5.105001e-02 98.539462 99.907224
INFO:root:train 1750 5.108803e-02 98.542797 99.907196
INFO:root:train 1800 5.130012e-02 98.529463 99.906302
INFO:root:train 1850 5.121683e-02 98.534576 99.907145
INFO:root:train 1900 5.116742e-02 98.533666 99.907943
INFO:root:train 1950 5.129416e-02 98.529600 99.903895
INFO:root:train 2000 5.111878e-02 98.531203 99.904735
INFO:root:train 2050 5.107051e-02 98.528919 99.904772
INFO:root:train 2100 5.114762e-02 98.525256 99.905551
INFO:root:train 2150 5.133254e-02 98.519584 99.906294
INFO:root:train 2200 5.111188e-02 98.521269 99.907002
INFO:root:train 2250 5.117293e-02 98.519408 99.906986
INFO:root:train 2300 5.141323e-02 98.516949 99.906291
INFO:root:train 2350 5.144882e-02 98.518583 99.905625
INFO:root:train 2400 5.174185e-02 98.515592 99.903686
INFO:root:train 2450 5.165251e-02 98.521649 99.903738
INFO:root:train 2500 5.172789e-02 98.520592 99.902539
INFO:root:train 2550 5.188347e-02 98.515288 99.903224
INFO:root:train 2600 5.208132e-02 98.508386 99.904484
INFO:root:train 2650 5.218418e-02 98.504692 99.905696
INFO:root:train 2700 5.214743e-02 98.504605 99.905128
INFO:root:train 2750 5.216932e-02 98.499977 99.905148
INFO:root:train 2800 5.229908e-02 98.492168 99.906283
INFO:root:train 2850 5.204930e-02 98.498334 99.907379
INFO:root:train 2900 5.217481e-02 98.497824 99.905205
INFO:root:train 2950 5.229362e-02 98.495213 99.903634
INFO:root:train 3000 5.225538e-02 98.496335 99.903157
INFO:root:train 3050 5.240079e-02 98.494346 99.902696
INFO:root:train 3100 5.240432e-02 98.491414 99.903257
INFO:root:train 3150 5.236632e-02 98.491550 99.903800
INFO:root:train 3200 5.242186e-02 98.490706 99.904327
INFO:root:train 3250 5.259095e-02 98.490368 99.903876
INFO:root:train 3300 5.262857e-02 98.489094 99.904385
INFO:root:train 3350 5.284686e-02 98.482263 99.903947
INFO:root:train 3400 5.290992e-02 98.481605 99.903980
INFO:root:train 3450 5.288125e-02 98.480513 99.904013
INFO:root:train 3500 5.289105e-02 98.479452 99.904492
INFO:root:train 3550 5.302200e-02 98.472701 99.904516
INFO:root:train 3600 5.311484e-02 98.466572 99.904974
INFO:root:train_acc 98.467928
INFO:root:valid 000 1.647948e-01 93.750000 100.000000
INFO:root:valid 050 2.109365e-01 95.189951 99.234069
INFO:root:valid 100 1.988471e-01 95.420792 99.195545
INFO:root:valid 150 2.027730e-01 95.364238 99.120447
INFO:root:valid 200 1.953269e-01 95.374689 99.183769
INFO:root:valid 250 1.994996e-01 95.374751 99.153386
INFO:root:valid 300 1.984264e-01 95.364410 99.127907
INFO:root:valid 350 1.996214e-01 95.334758 99.131944
INFO:root:valid 400 1.978681e-01 95.339776 99.146665
INFO:root:valid 450 2.011927e-01 95.263997 99.092295
INFO:root:valid 500 2.031992e-01 95.218937 99.083084
INFO:root:valid 550 2.045758e-01 95.190563 99.064201
INFO:root:valid 600 2.051666e-01 95.177309 99.064060
INFO:root:test_acc 95.172128
INFO:root:epoch 18 lr 7.767595e-06
INFO:root:train 000 6.985223e-02 98.437500 100.000000
INFO:root:train 050 5.533007e-02 98.621324 99.846814
INFO:root:train 100 5.144349e-02 98.483911 99.876238
INFO:root:train 150 4.891707e-02 98.582368 99.906871
INFO:root:train 200 4.919288e-02 98.577425 99.906716
INFO:root:train 250 4.915766e-02 98.574452 99.906624
INFO:root:train 300 5.055151e-02 98.546512 99.896179
INFO:root:train 350 5.030005e-02 98.557692 99.897614
INFO:root:train 400 5.045423e-02 98.534913 99.898691
INFO:root:train 450 5.135571e-02 98.520649 99.885671
INFO:root:train 500 5.145113e-02 98.509232 99.893962
INFO:root:train 550 5.094655e-02 98.536751 99.895077
INFO:root:train 600 5.045582e-02 98.564892 99.896007
INFO:root:train 650 5.008395e-02 98.567108 99.896793
INFO:root:train 700 4.944615e-02 98.582382 99.897468
INFO:root:train 750 5.005814e-02 98.564414 99.900133
INFO:root:train 800 5.076180e-02 98.529182 99.898564
INFO:root:train 850 5.178091e-02 98.501763 99.895344
INFO:root:train 900 5.257487e-02 98.492994 99.894215
INFO:root:train 950 5.313087e-02 98.485147 99.896491
INFO:root:train 1000 5.294037e-02 98.478084 99.900100
INFO:root:train 1050 5.305312e-02 98.471694 99.900392
INFO:root:train 1100 5.260277e-02 98.480075 99.904916
INFO:root:train 1150 5.262508e-02 98.478225 99.907689
INFO:root:train 1200 5.259709e-02 98.480433 99.905027
INFO:root:train 1250 5.259028e-02 98.481215 99.905076
INFO:root:train 1300 5.250380e-02 98.475932 99.907523
INFO:root:train 1350 5.272089e-02 98.474510 99.908632
INFO:root:train 1400 5.335863e-02 98.460921 99.907432
INFO:root:train 1450 5.331635e-02 98.465498 99.907391
INFO:root:train 1500 5.355172e-02 98.462483 99.907353
INFO:root:train 1550 5.344805e-02 98.471752 99.906310
INFO:root:train 1600 5.328443e-02 98.470682 99.905333
INFO:root:train 1650 5.297130e-02 98.478195 99.903468
INFO:root:train 1700 5.314776e-02 98.469650 99.904468
INFO:root:train 1750 5.374449e-02 98.463378 99.901842
INFO:root:train 1800 5.358519e-02 98.470468 99.901964
INFO:root:train 1850 5.354555e-02 98.469577 99.902924
INFO:root:train 1900 5.346407e-02 98.472021 99.903012
INFO:root:train 1950 5.323384e-02 98.479145 99.903895
INFO:root:train 2000 5.334571e-02 98.485132 99.902393
INFO:root:train 2050 5.330423e-02 98.482448 99.900963
INFO:root:train 2100 5.320056e-02 98.489559 99.901832
INFO:root:train 2150 5.307337e-02 98.492707 99.902662
INFO:root:train 2200 5.359044e-02 98.485064 99.899194
INFO:root:train 2250 5.379597e-02 98.481231 99.897268
INFO:root:train 2300 5.383520e-02 98.479601 99.896784
INFO:root:train 2350 5.384121e-02 98.477377 99.897650
INFO:root:train 2400 5.364412e-02 98.485006 99.897829
INFO:root:train 2450 5.364718e-02 98.487225 99.896726
INFO:root:train 2500 5.350681e-02 98.489979 99.897541
INFO:root:train 2550 5.342737e-02 98.487725 99.898324
INFO:root:train 2600 5.352660e-02 98.482555 99.898477
INFO:root:train 2650 5.354722e-02 98.475811 99.898623
INFO:root:train 2700 5.360721e-02 98.469317 99.899343
INFO:root:train 2750 5.340414e-02 98.476122 99.900604
INFO:root:train 2800 5.350335e-02 98.479338 99.899589
INFO:root:train 2850 5.360467e-02 98.474768 99.899706
INFO:root:train 2900 5.350196e-02 98.481127 99.900896
INFO:root:train 2950 5.340676e-02 98.477741 99.902046
INFO:root:train 3000 5.315591e-02 98.483318 99.903678
INFO:root:train 3050 5.316463e-02 98.484104 99.904232
INFO:root:train 3100 5.309368e-02 98.486375 99.903761
INFO:root:train 3150 5.322181e-02 98.479649 99.902809
INFO:root:train 3200 5.327281e-02 98.479479 99.902374
INFO:root:train 3250 5.303094e-02 98.486043 99.903395
INFO:root:train 3300 5.305705e-02 98.482941 99.902965
INFO:root:train 3350 5.297305e-02 98.484594 99.902081
INFO:root:train 3400 5.305495e-02 98.482064 99.901224
INFO:root:train 3450 5.293429e-02 98.484135 99.901297
INFO:root:train 3500 5.298745e-02 98.481238 99.900475
INFO:root:train 3550 5.285822e-02 98.485022 99.900116
INFO:root:train 3600 5.305260e-02 98.479155 99.899767
INFO:root:train_acc 98.478687
INFO:root:valid 000 1.588849e-01 95.312500 100.000000
INFO:root:valid 050 2.085872e-01 95.312500 99.264706
INFO:root:valid 100 1.971672e-01 95.513614 99.211015
INFO:root:valid 150 2.013428e-01 95.436672 99.151490
INFO:root:valid 200 1.941247e-01 95.452425 99.214863
INFO:root:valid 250 1.984256e-01 95.424552 99.172062
INFO:root:valid 300 1.973468e-01 95.431894 99.153862
INFO:root:valid 350 1.984653e-01 95.379274 99.172009
INFO:root:valid 400 1.965514e-01 95.394327 99.189526
INFO:root:valid 450 1.999311e-01 95.291713 99.123476
INFO:root:valid 500 2.020157e-01 95.231412 99.111153
INFO:root:valid 550 2.037170e-01 95.199070 99.089723
INFO:root:valid 600 2.043401e-01 95.203307 99.084859
INFO:root:test_acc 95.198070
INFO:root:epoch 19 lr 0.000000e+00
INFO:root:train 000 3.081912e-02 100.000000 100.000000
INFO:root:train 050 5.101907e-02 98.468137 99.877451
INFO:root:train 100 4.716654e-02 98.514851 99.922649
INFO:root:train 150 4.704057e-02 98.561672 99.917219
INFO:root:train 200 5.137924e-02 98.491915 99.906716
INFO:root:train 250 5.180466e-02 98.487301 99.887948
INFO:root:train 300 5.229947e-02 98.468646 99.880606
INFO:root:train 350 5.299356e-02 98.486467 99.875356
INFO:root:train 400 5.303503e-02 98.456983 99.883105
INFO:root:train 450 5.427777e-02 98.434035 99.882206
INFO:root:train 500 5.588224e-02 98.406312 99.865893
INFO:root:train 550 5.590212e-02 98.394964 99.863884
INFO:root:train 600 5.528543e-02 98.414101 99.872608
INFO:root:train 650 5.514777e-02 98.420699 99.875192
INFO:root:train 700 5.473733e-02 98.437500 99.879636
INFO:root:train 750 5.471877e-02 98.439581 99.879328
INFO:root:train 800 5.473312e-02 98.437500 99.879057
INFO:root:train 850 5.504995e-02 98.424647 99.878819
INFO:root:train 900 5.517719e-02 98.425361 99.880341
INFO:root:train 950 5.489795e-02 98.427642 99.881703
INFO:root:train 1000 5.491613e-02 98.414086 99.881369
INFO:root:train 1050 5.436273e-02 98.428580 99.885526
INFO:root:train 1100 5.433245e-02 98.423308 99.887886
INFO:root:train 1150 5.391006e-02 98.430712 99.891399
INFO:root:train 1200 5.433162e-02 98.427092 99.888114
INFO:root:train 1250 5.415039e-02 98.433753 99.888839
INFO:root:train 1300 5.432758e-02 98.431495 99.890709
INFO:root:train 1350 5.430955e-02 98.444439 99.892441
INFO:root:train 1400 5.400120e-02 98.450883 99.892934
INFO:root:train 1450 5.413497e-02 98.443961 99.894469
INFO:root:train 1500 5.407366e-02 98.436459 99.895903
INFO:root:train 1550 5.418612e-02 98.428433 99.897244
INFO:root:train 1600 5.374675e-02 98.438476 99.896549
INFO:root:train 1650 5.384741e-02 98.436554 99.898736
INFO:root:train 1700 5.378147e-02 98.445767 99.899875
INFO:root:train 1750 5.414039e-02 98.443746 99.899165
INFO:root:train 1800 5.442456e-02 98.437500 99.896759
INFO:root:train 1850 5.427317e-02 98.447630 99.897015
INFO:root:train 1900 5.411206e-02 98.449007 99.896436
INFO:root:train 1950 5.411174e-02 98.443106 99.895887
INFO:root:train 2000 5.406180e-02 98.444528 99.896146
INFO:root:train 2050 5.428518e-02 98.440547 99.896392
INFO:root:train 2100 5.414481e-02 98.442706 99.895883
INFO:root:train 2150 5.420727e-02 98.443311 99.897577
INFO:root:train 2200 5.450642e-02 98.437500 99.897064
INFO:root:train 2250 5.438323e-02 98.439582 99.898656
INFO:root:train 2300 5.430292e-02 98.442253 99.900179
INFO:root:train 2350 5.411844e-02 98.447469 99.900308
INFO:root:train 2400 5.415791e-02 98.445960 99.901083
INFO:root:train 2450 5.434808e-02 98.442600 99.901188
INFO:root:train 2500 5.420233e-02 98.444997 99.901289
INFO:root:train 2550 5.409698e-02 98.448525 99.899549
INFO:root:train 2600 5.413258e-02 98.449515 99.898477
INFO:root:train 2650 5.380565e-02 98.458129 99.899213
INFO:root:train 2700 5.393960e-02 98.458326 99.898186
INFO:root:train 2750 5.382207e-02 98.464195 99.897196
INFO:root:train 2800 5.374349e-02 98.465392 99.897358
INFO:root:train 2850 5.384336e-02 98.461066 99.898062
INFO:root:train 2900 5.377940e-02 98.466046 99.898203
INFO:root:train 2950 5.396555e-02 98.462386 99.896751
INFO:root:train 3000 5.392076e-02 98.463533 99.896909
INFO:root:train 3050 5.377509e-02 98.469252 99.898087
INFO:root:train 3100 5.376657e-02 98.467732 99.897714
INFO:root:train 3150 5.381309e-02 98.465269 99.896858
INFO:root:train 3200 5.374619e-02 98.465811 99.897493
INFO:root:train 3250 5.373563e-02 98.467299 99.897147
INFO:root:train 3300 5.359820e-02 98.471581 99.898232
INFO:root:train 3350 5.374389e-02 98.467808 99.894621
INFO:root:train 3400 5.371276e-02 98.465065 99.895251
INFO:root:train 3450 5.389384e-02 98.455611 99.894052
INFO:root:train 3500 5.395696e-02 98.453567 99.894673
INFO:root:train 3550 5.415630e-02 98.447180 99.893956
INFO:root:train 3600 5.414717e-02 98.448782 99.894561
INFO:root:train_acc 98.448562
INFO:root:valid 000 1.484783e-01 95.312500 100.000000
INFO:root:valid 050 2.102766e-01 95.373775 99.264706
INFO:root:valid 100 1.982358e-01 95.482673 99.195545
INFO:root:valid 150 2.020564e-01 95.447020 99.130795
INFO:root:valid 200 1.946038e-01 95.483520 99.214863
INFO:root:valid 250 1.988864e-01 95.474353 99.172062
INFO:root:valid 300 1.978329e-01 95.468231 99.143480
INFO:root:valid 350 1.989769e-01 95.423789 99.149751
INFO:root:valid 400 1.969282e-01 95.429395 99.170044
INFO:root:valid 450 2.001261e-01 95.322894 99.099224
INFO:root:valid 500 2.020796e-01 95.250125 99.095559
INFO:root:valid 550 2.036390e-01 95.235935 99.072709
INFO:root:valid 600 2.040560e-01 95.231905 99.071859
INFO:root:test_acc 95.226606
Traceback (most recent call last):
  File "main.py", line 267, in <module>
    model_description['es']['config'] = es_worker.run_es(iterations=40) 
  File "/home/rkohli/aml-project/src/darts_es.py", line 158, in run_es
    cs = self.__class__.get_configspace()
  File "/home/rkohli/aml-project/src/darts_es.py", line 146, in get_configspace
    optimizer = CSH.CategoricalHyperparameter('optimizer', get['opti_dict'].keys())
TypeError: 'function' object is not subscriptable
