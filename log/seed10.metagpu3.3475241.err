INFO:root:gpu device = cuda:0
INFO:root:args = {'data': './data', 'set': 'cifar10', 'batch_size': 32, 'learning_rate': 0.1, 'learning_rate_min': 0.001, 'momentum': 0.9, 'weight_decay': 0.0003, 'report_freq': 50, 'gpu': 'cuda:0', 'epochs': 20, 'init_channels': 8, 'layers': 4, 'model_path': 'saved_models', 'cutout': False, 'cutout_length': 16, 'drop_path_prob': 0.3, 'save': './EXP1/EXP-20190915-004608-15', 'seed': 15, 'grad_clip': 5, 'train_portion': 0.5, 'unrolled': False, 'arch_learning_rate': 0.0006, 'arch_weight_decay': 0.001}
INFO:root:param size = 0.056538MB
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
INFO:root:epoch 0 lr 9.939057e-02
INFO:root:genotype = Genotype(normal=[('sep_conv_3x3', 0), ('sep_conv_5x5', 1), ('skip_connect', 0), ('sep_conv_3x3', 2), ('avg_pool_3x3', 2), ('skip_connect', 3), ('skip_connect', 1), ('sep_conv_3x3', 3)], normal_concat=range(2, 6), reduce=[('avg_pool_3x3', 0), ('max_pool_3x3', 1), ('dil_conv_3x3', 0), ('max_pool_3x3', 1), ('dil_conv_5x5', 1), ('sep_conv_5x5', 0), ('sep_conv_3x3', 2), ('sep_conv_5x5', 0)], reduce_concat=range(2, 6))
/home/rkohli/aml-project/src/train_search.py:137: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), args['grad_clip'])
INFO:root:train 000 2.289118e+00 6.250000 65.625000
INFO:root:train 050 2.035027e+00 26.225490 78.615196
INFO:root:train 100 1.847157e+00 33.446782 83.941832
INFO:root:train 150 1.715183e+00 39.341887 86.589404
INFO:root:train 200 1.579334e+00 44.713930 88.572761
INFO:root:train 250 1.462751e+00 49.215637 90.052291
INFO:root:train 300 1.373253e+00 52.408638 91.154485
slurmstepd-metagpu3: error: *** JOB 3475241 ON metagpu3 CANCELLED AT 2019-09-15T00:48:38 ***
