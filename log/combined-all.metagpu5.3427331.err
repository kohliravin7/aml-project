INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.04992452260234029, 'grad_clip_value': 7, 'initial_lr': 0.0028493238730520885, 'lr_scheduler': 'Cosine', 'n_conv_layers': 5, 'optimizer': 'adam', 'weight_decay': 1.465223282456662e-05}
main.py:103: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.
  total_model_params = np.sum(p.numel() for p in model.parameters())
INFO:root:param size = 0.038025MB
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
INFO:root:epoch 0 lr 2.831784e-03
/home/rkohli/aml-project/src/train.py:131: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), grad_clip)
INFO:root:train 000 4.107047e+00 0.000000 9.375000
INFO:root:train 050 2.966637e+00 24.203431 51.011029
INFO:root:train 100 2.423633e+00 37.561881 65.423886
INFO:root:train 150 2.087488e+00 45.912666 72.971854
INFO:root:train 200 1.837275e+00 52.246580 77.705224
INFO:root:train 250 1.661785e+00 56.828934 80.739542
INFO:root:train 300 1.536614e+00 59.857766 82.890365
INFO:root:train 350 1.428462e+00 62.597934 84.593127
INFO:root:train 400 1.331468e+00 65.067799 86.046602
INFO:root:train 450 1.257475e+00 66.958841 87.143154
INFO:root:train 500 1.195381e+00 68.544162 88.048902
INFO:root:train 550 1.137949e+00 69.997731 88.875340
INFO:root:train 600 1.088729e+00 71.243240 89.551269
INFO:root:train 650 1.048012e+00 72.266225 90.077765
INFO:root:train 700 1.010862e+00 73.214604 90.567047
INFO:root:train 750 9.788771e-01 73.984687 91.011984
INFO:root:train 800 9.476331e-01 74.793227 91.409176
INFO:root:train 850 9.179709e-01 75.547150 91.785400
INFO:root:train 900 8.917944e-01 76.222600 92.123335
INFO:root:train 950 8.685662e-01 76.820452 92.417521
INFO:root:train 1000 8.473642e-01 77.339848 92.676074
INFO:root:train 1050 8.270432e-01 77.872265 92.941246
INFO:root:train 1100 8.091271e-01 78.352066 93.152532
INFO:root:train 1150 7.905821e-01 78.848556 93.386186
INFO:root:train 1200 7.735517e-01 79.305006 93.586074
INFO:root:train 1250 7.571014e-01 79.731215 93.782474
INFO:root:train 1300 7.436128e-01 80.067016 93.939758
INFO:root:train 1350 7.292582e-01 80.421910 94.118940
INFO:root:train 1400 7.155577e-01 80.787161 94.271949
INFO:root:train 1450 7.033249e-01 81.112164 94.412259
INFO:root:train 1500 6.918070e-01 81.390531 94.534893
INFO:root:train 1550 6.823466e-01 81.652966 94.636525
INFO:root:train 1600 6.721738e-01 81.911696 94.750351
INFO:root:train 1650 6.622138e-01 82.189771 94.858230
INFO:root:train 1700 6.527526e-01 82.431290 94.967115
INFO:root:train 1750 6.426824e-01 82.692033 95.076028
INFO:root:train 1800 6.341037e-01 82.906198 95.170218
INFO:root:train 1850 6.258808e-01 83.120610 95.252566
INFO:root:train 1900 6.173075e-01 83.338539 95.340446
INFO:root:train 1950 6.089419e-01 83.544496 95.430228
INFO:root:train 2000 6.016611e-01 83.733914 95.506153
INFO:root:train 2050 5.936947e-01 83.952188 95.582186
INFO:root:train 2100 5.867201e-01 84.130325 95.655343
INFO:root:train 2150 5.805171e-01 84.291463 95.725825
INFO:root:train 2200 5.741350e-01 84.453090 95.795235
INFO:root:train 2250 5.676820e-01 84.616559 95.867809
INFO:root:train 2300 5.606377e-01 84.800087 95.936549
INFO:root:train 2350 5.545218e-01 84.950553 95.998378
INFO:root:train 2400 5.495473e-01 85.068721 96.051775
INFO:root:train 2450 5.447504e-01 85.201831 96.102994
INFO:root:train 2500 5.393545e-01 85.342738 96.161535
INFO:root:train 2550 5.348366e-01 85.463421 96.212270
INFO:root:train 2600 5.300115e-01 85.585472 96.270064
INFO:root:train 2650 5.250136e-01 85.717654 96.318606
INFO:root:train 2700 5.201277e-01 85.844942 96.365351
INFO:root:train 2750 5.157021e-01 85.967035 96.410396
INFO:root:train 2800 5.112547e-01 86.080306 96.458296
INFO:root:train 2850 5.062945e-01 86.198921 96.505064
INFO:root:train 2900 5.027017e-01 86.292981 96.545372
INFO:root:train 2950 4.986538e-01 86.401855 96.585903
INFO:root:train 3000 4.944181e-01 86.506060 96.623521
INFO:root:train 3050 4.901536e-01 86.609411 96.667076
INFO:root:train 3100 4.867418e-01 86.696832 96.707211
INFO:root:train 3150 4.831176e-01 86.785941 96.743097
INFO:root:train 3200 4.794161e-01 86.889351 96.780303
INFO:root:train 3250 4.758892e-01 86.983332 96.814922
INFO:root:train 3300 4.726319e-01 87.071626 96.847073
INFO:root:train 3350 4.692476e-01 87.157751 96.883859
INFO:root:train 3400 4.665216e-01 87.234913 96.906700
INFO:root:train 3450 4.632176e-01 87.325232 96.936576
INFO:root:train 3500 4.601117e-01 87.414756 96.965153
INFO:root:train 3550 4.572592e-01 87.495600 96.995565
INFO:root:train 3600 4.541236e-01 87.575066 97.026000
INFO:root:train_acc 87.619048
INFO:root:valid 000 4.086320e-01 87.500000 98.437500
INFO:root:valid 050 4.648732e-01 87.408088 97.518382
INFO:root:valid 100 4.516169e-01 87.747525 97.509282
INFO:root:valid 150 4.525053e-01 87.717301 97.506209
INFO:root:valid 200 4.530282e-01 87.764303 97.551306
INFO:root:valid 250 4.543861e-01 87.792580 97.541086
INFO:root:valid 300 4.603494e-01 87.795889 97.440822
INFO:root:valid 350 4.603798e-01 87.838319 97.382479
INFO:root:valid 400 4.605159e-01 87.811721 97.365960
INFO:root:valid 450 4.656467e-01 87.683620 97.325388
INFO:root:valid 500 4.664551e-01 87.702720 97.283558
INFO:root:valid 550 4.656423e-01 87.763725 97.274841
INFO:root:valid 600 4.668354e-01 87.762583 97.270175
INFO:root:test_acc 87.755208
INFO:root:epoch 1 lr 2.779596e-03
INFO:root:train 000 2.201477e-01 95.312500 98.437500
INFO:root:train 050 2.330712e-01 93.627451 99.203431
INFO:root:train 100 2.283765e-01 93.548886 99.272896
INFO:root:train 150 2.330365e-01 93.501656 99.213576
INFO:root:train 200 2.305399e-01 93.547886 99.191542
INFO:root:train 250 2.360776e-01 93.569472 99.103586
INFO:root:train 300 2.357177e-01 93.485257 99.138289
INFO:root:train 350 2.367665e-01 93.460648 99.114138
INFO:root:train 400 2.367012e-01 93.504520 99.103803
INFO:root:train 450 2.358472e-01 93.517877 99.092295
INFO:root:train 500 2.359388e-01 93.491143 99.076846
INFO:root:train 550 2.346019e-01 93.514632 99.086887
INFO:root:train 600 2.340408e-01 93.487417 99.095258
INFO:root:train 650 2.356524e-01 93.452381 99.104743
INFO:root:train 700 2.364761e-01 93.458006 99.101730
INFO:root:train 750 2.360831e-01 93.439997 99.101198
INFO:root:train 800 2.370187e-01 93.426186 99.079276
INFO:root:train 850 2.367059e-01 93.443375 99.085635
INFO:root:train 900 2.363660e-01 93.443049 99.082617
INFO:root:train 950 2.355207e-01 93.472332 99.081559
INFO:root:train 1000 2.355906e-01 93.467470 99.085290
INFO:root:train 1050 2.343472e-01 93.464558 99.097586
INFO:root:train 1100 2.346064e-01 93.440622 99.097411
INFO:root:train 1150 2.335974e-01 93.440487 99.109470
INFO:root:train 1200 2.338570e-01 93.422148 99.111418
INFO:root:train 1250 2.326348e-01 93.425260 99.125699
INFO:root:train 1300 2.327157e-01 93.428132 99.128075
INFO:root:train 1350 2.325020e-01 93.427322 99.124491
INFO:root:train 1400 2.322494e-01 93.422109 99.122279
INFO:root:train 1450 2.318427e-01 93.426947 99.122373
INFO:root:train 1500 2.315386e-01 93.449159 99.118296
INFO:root:train 1550 2.314395e-01 93.449790 99.126572
INFO:root:train 1600 2.313678e-01 93.437695 99.131402
INFO:root:train 1650 2.307708e-01 93.453778 99.134048
INFO:root:train 1700 2.303827e-01 93.464322 99.132863
INFO:root:train 1750 2.300911e-01 93.482296 99.130854
INFO:root:train 1800 2.289637e-01 93.514888 99.133294
INFO:root:train 1850 2.286834e-01 93.528836 99.133914
INFO:root:train 1900 2.280939e-01 93.546160 99.138611
INFO:root:train 1950 2.279089e-01 93.544176 99.139063
INFO:root:train 2000 2.276462e-01 93.561813 99.140274
INFO:root:train 2050 2.271625e-01 93.577828 99.142187
INFO:root:train 2100 2.270474e-01 93.594568 99.138059
INFO:root:train 2150 2.269154e-01 93.593823 99.136303
INFO:root:train 2200 2.267293e-01 93.604470 99.136756
INFO:root:train 2250 2.263106e-01 93.616032 99.141354
INFO:root:train 2300 2.263218e-01 93.612152 99.139640
INFO:root:train 2350 2.257514e-01 93.636352 99.147304
INFO:root:train 2400 2.260139e-01 93.622449 99.146189
INFO:root:train 2450 2.256297e-01 93.637801 99.148307
INFO:root:train 2500 2.255354e-01 93.634421 99.149090
INFO:root:train 2550 2.253039e-01 93.642812 99.149843
INFO:root:train 2600 2.245452e-01 93.668301 99.149966
INFO:root:train 2650 2.236567e-01 93.682808 99.157747
INFO:root:train 2700 2.234644e-01 93.680581 99.155984
INFO:root:train 2750 2.233041e-01 93.685819 99.155421
INFO:root:train 2800 2.233698e-01 93.687522 99.153204
INFO:root:train 2850 2.231291e-01 93.700675 99.154902
INFO:root:train 2900 2.227258e-01 93.706911 99.155464
INFO:root:train 2950 2.225718e-01 93.722996 99.154947
INFO:root:train 3000 2.224092e-01 93.720843 99.157572
INFO:root:train 3050 2.221949e-01 93.719784 99.163696
INFO:root:train 3100 2.220064e-01 93.723799 99.164584
INFO:root:train 3150 2.221301e-01 93.723719 99.164452
INFO:root:train 3200 2.219074e-01 93.728522 99.162859
INFO:root:train 3250 2.213691e-01 93.741349 99.167564
INFO:root:train 3300 2.208614e-01 93.749053 99.171653
INFO:root:train 3350 2.205538e-01 93.760724 99.173288
INFO:root:train 3400 2.203323e-01 93.766999 99.173037
INFO:root:train 3450 2.202083e-01 93.765394 99.173247
INFO:root:train 3500 2.198782e-01 93.780348 99.173897
INFO:root:train 3550 2.194432e-01 93.791802 99.175408
INFO:root:train 3600 2.190094e-01 93.800767 99.179047
INFO:root:train_acc 93.799410
INFO:root:valid 000 5.888554e-01 84.375000 96.875000
INFO:root:valid 050 4.657748e-01 88.388480 97.579657
INFO:root:valid 100 4.371715e-01 88.830446 97.663985
INFO:root:valid 150 4.358656e-01 88.927980 97.661424
INFO:root:valid 200 4.309054e-01 88.868159 97.768968
INFO:root:valid 250 4.320459e-01 88.888197 97.659363
INFO:root:valid 300 4.316071e-01 88.896387 97.601744
INFO:root:valid 350 4.334540e-01 88.808761 97.587251
INFO:root:valid 400 4.311057e-01 88.793641 97.599751
INFO:root:valid 450 4.354563e-01 88.712583 97.557511
INFO:root:valid 500 4.371413e-01 88.716317 97.539296
INFO:root:valid 550 4.370977e-01 88.759074 97.527223
INFO:root:valid 600 4.377949e-01 88.784318 97.524958
INFO:root:test_acc 88.787714
INFO:root:epoch 2 lr 2.694045e-03
INFO:root:train 000 4.571860e-01 87.500000 95.312500
INFO:root:train 050 2.159218e-01 94.485294 99.172794
INFO:root:train 100 2.008482e-01 94.724629 99.288366
INFO:root:train 150 1.953281e-01 94.784768 99.327401
INFO:root:train 200 1.876051e-01 94.861629 99.362562
INFO:root:train 250 1.860185e-01 94.826942 99.365040
INFO:root:train 300 1.837272e-01 94.824543 99.382267
INFO:root:train 350 1.846575e-01 94.720442 99.394587
INFO:root:train 400 1.887427e-01 94.579956 99.372662
INFO:root:train 450 1.889672e-01 94.609202 99.372921
INFO:root:train 500 1.880774e-01 94.626372 99.376248
INFO:root:train 550 1.878501e-01 94.614905 99.361956
INFO:root:train 600 1.880414e-01 94.633943 99.363041
INFO:root:train 650 1.876123e-01 94.616455 99.371160
INFO:root:train 700 1.847334e-01 94.697307 99.384807
INFO:root:train 750 1.852612e-01 94.671688 99.379993
INFO:root:train 800 1.850589e-01 94.649267 99.383583
INFO:root:train 850 1.837497e-01 94.697415 99.386751
INFO:root:train 900 1.836866e-01 94.702067 99.377428
INFO:root:train 950 1.844341e-01 94.679942 99.372371
INFO:root:train 1000 1.844044e-01 94.655345 99.372502
INFO:root:train 1050 1.864693e-01 94.609301 99.362215
INFO:root:train 1100 1.860532e-01 94.598660 99.364214
INFO:root:train 1150 1.854457e-01 94.612022 99.368755
INFO:root:train 1200 1.840994e-01 94.634679 99.375520
INFO:root:train 1250 1.839382e-01 94.640538 99.374251
INFO:root:train 1300 1.848785e-01 94.620724 99.375480
INFO:root:train 1350 1.847303e-01 94.635918 99.374306
INFO:root:train 1400 1.858176e-01 94.621030 99.369870
INFO:root:train 1450 1.867147e-01 94.615782 99.364662
INFO:root:train 1500 1.866587e-01 94.614007 99.367089
INFO:root:train 1550 1.865066e-01 94.624436 99.365329
INFO:root:train 1600 1.858863e-01 94.630309 99.369535
INFO:root:train 1650 1.851934e-01 94.641505 99.374432
INFO:root:train 1700 1.854811e-01 94.647450 99.377205
INFO:root:train 1750 1.854281e-01 94.649486 99.378034
INFO:root:train 1800 1.856132e-01 94.645336 99.379685
INFO:root:train 1850 1.850224e-01 94.657449 99.382091
INFO:root:train 1900 1.845682e-01 94.674678 99.376973
INFO:root:train 1950 1.846624e-01 94.675807 99.375320
INFO:root:train 2000 1.851600e-01 94.665948 99.371408
INFO:root:train 2050 1.851724e-01 94.668759 99.379114
INFO:root:train 2100 1.854970e-01 94.661768 99.378272
INFO:root:train 2150 1.864012e-01 94.636942 99.376743
INFO:root:train 2200 1.867892e-01 94.635251 99.378124
INFO:root:train 2250 1.869130e-01 94.622529 99.379442
INFO:root:train 2300 1.873895e-01 94.607643 99.378667
INFO:root:train 2350 1.873394e-01 94.613329 99.379254
INFO:root:train 2400 1.872799e-01 94.620731 99.376562
INFO:root:train 2450 1.868138e-01 94.627830 99.380992
INFO:root:train 2500 1.865198e-01 94.645892 99.377749
INFO:root:train 2550 1.869730e-01 94.642420 99.376470
INFO:root:train 2600 1.867801e-01 94.640283 99.377643
INFO:root:train 2650 1.864775e-01 94.645888 99.375825
INFO:root:train 2700 1.865350e-01 94.639138 99.376388
INFO:root:train 2750 1.863433e-01 94.647969 99.376931
INFO:root:train 2800 1.862366e-01 94.650348 99.374665
INFO:root:train 2850 1.863355e-01 94.649904 99.377959
INFO:root:train 2900 1.860955e-01 94.653245 99.376293
INFO:root:train 2950 1.861265e-01 94.649060 99.377330
INFO:root:train 3000 1.866155e-01 94.641890 99.371043
INFO:root:train 3050 1.864455e-01 94.646223 99.372132
INFO:root:train 3100 1.862272e-01 94.656462 99.371171
INFO:root:train 3150 1.858800e-01 94.674310 99.372719
INFO:root:train 3200 1.854477e-01 94.686231 99.376660
INFO:root:train 3250 1.850889e-01 94.693940 99.378076
INFO:root:train 3300 1.847828e-01 94.708516 99.378029
INFO:root:train 3350 1.846100e-01 94.714731 99.378450
INFO:root:train 3400 1.846699e-01 94.714790 99.379319
INFO:root:train 3450 1.846197e-01 94.711678 99.380162
INFO:root:train 3500 1.845651e-01 94.711779 99.380534
INFO:root:train 3550 1.844111e-01 94.722877 99.379576
INFO:root:train 3600 1.844071e-01 94.726291 99.380380
INFO:root:train_acc 94.730704
INFO:root:valid 000 3.691998e-01 90.625000 96.875000
INFO:root:valid 050 3.194430e-01 91.973039 98.223039
INFO:root:valid 100 3.107897e-01 92.125619 98.251856
INFO:root:valid 150 3.065007e-01 92.166805 98.292632
INFO:root:valid 200 3.027273e-01 92.063122 98.398632
INFO:root:valid 250 3.087614e-01 91.963396 98.350349
INFO:root:valid 300 3.141648e-01 91.896802 98.312915
INFO:root:valid 350 3.150776e-01 91.844729 98.335114
INFO:root:valid 400 3.128325e-01 91.883572 98.351777
INFO:root:valid 450 3.172694e-01 91.764828 98.309313
INFO:root:valid 500 3.197226e-01 91.725923 98.331462
INFO:root:valid 550 3.202302e-01 91.728108 98.332577
INFO:root:valid 600 3.231349e-01 91.672733 98.320507
INFO:root:test_acc 91.680286
INFO:root:epoch 3 lr 2.577238e-03
INFO:root:train 000 8.016000e-02 98.437500 100.000000
INFO:root:train 050 1.860765e-01 94.669118 99.387255
INFO:root:train 100 1.732317e-01 94.987624 99.474010
INFO:root:train 150 1.693903e-01 95.177980 99.524007
INFO:root:train 200 1.662584e-01 95.289179 99.549129
INFO:root:train 250 1.681547e-01 95.293825 99.520667
INFO:root:train 300 1.678912e-01 95.281354 99.501661
INFO:root:train 350 1.684236e-01 95.267984 99.470264
INFO:root:train 400 1.661746e-01 95.261845 99.485661
INFO:root:train 450 1.671142e-01 95.267461 99.476857
INFO:root:train 500 1.679321e-01 95.212700 99.479167
INFO:root:train 550 1.680160e-01 95.255785 99.469714
INFO:root:train 600 1.685378e-01 95.283902 99.459235
INFO:root:train 650 1.694418e-01 95.233295 99.459965
INFO:root:train 700 1.699007e-01 95.232257 99.444989
INFO:root:train 750 1.708034e-01 95.185586 99.440330
INFO:root:train 800 1.712817e-01 95.174001 99.436252
INFO:root:train 850 1.715530e-01 95.180303 99.436325
INFO:root:train 900 1.720443e-01 95.156423 99.434656
INFO:root:train 950 1.715263e-01 95.172844 99.431519
INFO:root:train 1000 1.710713e-01 95.161089 99.430257
INFO:root:train 1050 1.712936e-01 95.153425 99.430602
INFO:root:train 1100 1.706501e-01 95.180518 99.435173
INFO:root:train 1150 1.708419e-01 95.164531 99.440704
INFO:root:train 1200 1.702449e-01 95.182400 99.444473
INFO:root:train 1250 1.700371e-01 95.170114 99.455436
INFO:root:train 1300 1.695774e-01 95.179189 99.459550
INFO:root:train 1350 1.691078e-01 95.189906 99.461047
INFO:root:train 1400 1.694829e-01 95.190935 99.457976
INFO:root:train 1450 1.701192e-01 95.178971 99.454040
INFO:root:train 1500 1.700583e-01 95.181337 99.448284
INFO:root:train 1550 1.698280e-01 95.187581 99.447937
INFO:root:train 1600 1.698334e-01 95.193434 99.446635
INFO:root:train 1650 1.706491e-01 95.169594 99.440680
INFO:root:train 1700 1.701640e-01 95.174713 99.447935
INFO:root:train 1750 1.697089e-01 95.192925 99.449422
INFO:root:train 1800 1.696214e-01 95.191907 99.446488
INFO:root:train 1850 1.695403e-01 95.190944 99.445401
INFO:root:train 1900 1.692399e-01 95.209758 99.446015
INFO:root:train 1950 1.692320e-01 95.211590 99.445797
INFO:root:train 2000 1.693946e-01 95.210988 99.447932
INFO:root:train 2050 1.685523e-01 95.230985 99.450725
INFO:root:train 2100 1.680885e-01 95.247055 99.446692
INFO:root:train 2150 1.675543e-01 95.263831 99.449384
INFO:root:train 2200 1.669376e-01 95.285524 99.454083
INFO:root:train 2250 1.667551e-01 95.293758 99.457880
INFO:root:train 2300 1.665823e-01 95.300277 99.458116
INFO:root:train 2350 1.666446e-01 95.303195 99.455019
INFO:root:train 2400 1.668882e-01 95.303389 99.452051
INFO:root:train 2450 1.669090e-01 95.303575 99.452392
INFO:root:train 2500 1.667968e-01 95.311250 99.452719
INFO:root:train 2550 1.666518e-01 95.314338 99.452421
INFO:root:train 2600 1.663051e-01 95.319709 99.455137
INFO:root:train 2650 1.659461e-01 95.326646 99.458931
INFO:root:train 2700 1.661444e-01 95.318863 99.460269
INFO:root:train 2750 1.660344e-01 95.313636 99.459288
INFO:root:train 2800 1.661941e-01 95.317521 99.458899
INFO:root:train 2850 1.658633e-01 95.328942 99.460167
INFO:root:train 2900 1.657164e-01 95.332967 99.461931
INFO:root:train 2950 1.655455e-01 95.340563 99.461517
INFO:root:train 3000 1.653376e-01 95.344260 99.460596
INFO:root:train 3050 1.652045e-01 95.349885 99.459194
INFO:root:train 3100 1.652761e-01 95.345252 99.457836
INFO:root:train 3150 1.654904e-01 95.341757 99.454538
INFO:root:train 3200 1.656074e-01 95.338371 99.452808
INFO:root:train 3250 1.655344e-01 95.336050 99.454495
INFO:root:train 3300 1.650905e-01 95.345634 99.456131
INFO:root:train 3350 1.649599e-01 95.350735 99.459117
INFO:root:train 3400 1.647901e-01 95.353848 99.459258
INFO:root:train 3450 1.647655e-01 95.351891 99.462112
INFO:root:train 3500 1.648872e-01 95.349989 99.459976
INFO:root:train 3550 1.649639e-01 95.353862 99.459659
INFO:root:train 3600 1.648109e-01 95.355891 99.461521
INFO:root:train_acc 95.353861
INFO:root:valid 000 3.101255e-01 92.187500 98.437500
INFO:root:valid 050 3.210940e-01 92.432598 98.314951
INFO:root:valid 100 3.026720e-01 92.496906 98.267327
INFO:root:valid 150 3.023730e-01 92.394454 98.261589
INFO:root:valid 200 2.950174e-01 92.420709 98.406405
INFO:root:valid 250 2.979834e-01 92.324452 98.406375
INFO:root:valid 300 2.976288e-01 92.379568 98.416736
INFO:root:valid 350 2.990614e-01 92.307692 98.406339
INFO:root:valid 400 2.962059e-01 92.358946 98.445293
INFO:root:valid 450 3.026344e-01 92.229074 98.409784
INFO:root:valid 500 3.035667e-01 92.168787 98.421906
INFO:root:valid 550 3.039858e-01 92.264065 98.403471
INFO:root:valid 600 3.066860e-01 92.226498 98.385503
INFO:root:test_acc 92.217293
INFO:root:epoch 4 lr 2.432050e-03
INFO:root:train 000 8.194267e-02 96.875000 100.000000
INFO:root:train 050 1.321989e-01 95.772059 99.724265
INFO:root:train 100 1.349704e-01 95.915842 99.690594
INFO:root:train 150 1.404195e-01 95.798841 99.637831
INFO:root:train 200 1.444490e-01 95.833333 99.611318
INFO:root:train 250 1.410454e-01 95.910110 99.620269
INFO:root:train 300 1.404900e-01 95.893895 99.631437
INFO:root:train 350 1.411032e-01 95.860043 99.608262
INFO:root:train 400 1.425470e-01 95.795667 99.602556
INFO:root:train 450 1.462634e-01 95.686669 99.611973
INFO:root:train 500 1.463623e-01 95.702345 99.619511
INFO:root:train 550 1.461471e-01 95.698162 99.625681
INFO:root:train 600 1.476175e-01 95.684276 99.604825
INFO:root:train 650 1.473688e-01 95.667723 99.603975
INFO:root:train 700 1.488551e-01 95.642386 99.594330
INFO:root:train 750 1.481796e-01 95.649551 99.596372
INFO:root:train 800 1.486965e-01 95.616807 99.596208
INFO:root:train 850 1.500922e-01 95.602600 99.583211
INFO:root:train 900 1.495106e-01 95.614248 99.582062
INFO:root:train 950 1.489717e-01 95.673962 99.576104
INFO:root:train 1000 1.486309e-01 95.702735 99.567620
INFO:root:train 1050 1.492948e-01 95.704983 99.562916
INFO:root:train 1100 1.495974e-01 95.695674 99.568574
INFO:root:train 1150 1.499689e-01 95.703464 99.561523
INFO:root:train 1200 1.492874e-01 95.713208 99.562864
INFO:root:train 1250 1.491138e-01 95.720923 99.554107
INFO:root:train 1300 1.498262e-01 95.701624 99.544821
INFO:root:train 1350 1.499501e-01 95.703414 99.539693
INFO:root:train 1400 1.497816e-01 95.705077 99.537161
INFO:root:train 1450 1.505206e-01 95.681857 99.530496
INFO:root:train 1500 1.507592e-01 95.676840 99.531562
INFO:root:train 1550 1.514465e-01 95.672147 99.526515
INFO:root:train 1600 1.509926e-01 95.683362 99.528615
INFO:root:train 1650 1.511221e-01 95.694844 99.527748
INFO:root:train 1700 1.512841e-01 95.693710 99.526933
INFO:root:train 1750 1.507603e-01 95.704240 99.531518
INFO:root:train 1800 1.502352e-01 95.712451 99.535848
INFO:root:train 1850 1.499889e-01 95.721907 99.533192
INFO:root:train 1900 1.497818e-01 95.720180 99.535606
INFO:root:train 1950 1.499792e-01 95.708931 99.528287
INFO:root:train 2000 1.500881e-01 95.713081 99.531484
INFO:root:train 2050 1.501061e-01 95.704839 99.533764
INFO:root:train 2100 1.503875e-01 95.696990 99.535192
INFO:root:train 2150 1.502584e-01 95.698948 99.535826
INFO:root:train 2200 1.500208e-01 95.700818 99.537142
INFO:root:train 2250 1.500938e-01 95.701910 99.536317
INFO:root:train 2300 1.503298e-01 95.692769 99.533491
INFO:root:train 2350 1.505871e-01 95.689334 99.534108
INFO:root:train 2400 1.502312e-01 95.702312 99.535350
INFO:root:train 2450 1.503396e-01 95.702647 99.531441
INFO:root:train 2500 1.501510e-01 95.707967 99.532687
INFO:root:train 2550 1.501296e-01 95.706953 99.535109
INFO:root:train 2600 1.504773e-01 95.708982 99.534434
INFO:root:train 2650 1.505622e-01 95.711524 99.532016
INFO:root:train 2700 1.504867e-01 95.716864 99.533159
INFO:root:train 2750 1.509210e-01 95.710083 99.530284
INFO:root:train 2800 1.511488e-01 95.705775 99.528628
INFO:root:train 2850 1.510086e-01 95.703262 99.529770
INFO:root:train 2900 1.509018e-01 95.709454 99.530334
INFO:root:train 2950 1.509513e-01 95.705905 99.529291
INFO:root:train 3000 1.507653e-01 95.709763 99.527762
INFO:root:train 3050 1.505186e-01 95.722714 99.530379
INFO:root:train 3100 1.510357e-01 95.709045 99.527874
INFO:root:train 3150 1.512632e-01 95.706720 99.526440
INFO:root:train 3200 1.509738e-01 95.712766 99.526515
INFO:root:train 3250 1.512265e-01 95.710935 99.524665
INFO:root:train 3300 1.510622e-01 95.713894 99.525712
INFO:root:train 3350 1.510972e-01 95.706972 99.527193
INFO:root:train 3400 1.511025e-01 95.711739 99.524037
INFO:root:train 3450 1.510191e-01 95.712294 99.524142
INFO:root:train 3500 1.506605e-01 95.721312 99.525136
INFO:root:train 3550 1.503831e-01 95.725676 99.524782
INFO:root:train 3600 1.505813e-01 95.719071 99.520966
INFO:root:train_acc 95.720956
INFO:root:valid 000 4.176658e-01 89.062500 100.000000
INFO:root:valid 050 2.960004e-01 92.769608 98.406863
INFO:root:valid 100 2.864262e-01 92.821782 98.499381
INFO:root:valid 150 2.815942e-01 92.860099 98.520281
INFO:root:valid 200 2.791906e-01 92.809391 98.631841
INFO:root:valid 250 2.810694e-01 92.828685 98.605578
INFO:root:valid 300 2.827213e-01 92.800042 98.556894
INFO:root:valid 350 2.852409e-01 92.784010 98.539886
INFO:root:valid 400 2.848173e-01 92.748597 98.581671
INFO:root:valid 450 2.884800e-01 92.641353 98.537971
INFO:root:valid 500 2.905586e-01 92.599177 98.543538
INFO:root:valid 550 2.924938e-01 92.615699 98.502722
INFO:root:valid 600 2.954566e-01 92.564476 98.479097
INFO:root:test_acc 92.559732
INFO:root:epoch 5 lr 2.262057e-03
INFO:root:train 000 9.774543e-02 98.437500 100.000000
INFO:root:train 050 1.341136e-01 96.200980 99.693627
INFO:root:train 100 1.351448e-01 96.147896 99.675124
INFO:root:train 150 1.345672e-01 96.098924 99.710265
INFO:root:train 200 1.341228e-01 96.097637 99.704602
INFO:root:train 250 1.343349e-01 96.134213 99.663845
INFO:root:train 300 1.364204e-01 96.143065 99.662583
INFO:root:train 350 1.342593e-01 96.185007 99.679487
INFO:root:train 400 1.336465e-01 96.126870 99.692176
INFO:root:train 450 1.353007e-01 96.098947 99.674335
INFO:root:train 500 1.362110e-01 96.070359 99.641342
INFO:root:train 550 1.356987e-01 96.098004 99.634188
INFO:root:train 600 1.352453e-01 96.082051 99.636023
INFO:root:train 650 1.373850e-01 96.032546 99.613575
INFO:root:train 700 1.378118e-01 96.052514 99.594330
INFO:root:train 750 1.377519e-01 96.067743 99.594291
INFO:root:train 800 1.373784e-01 96.088873 99.594257
INFO:root:train 850 1.368546e-01 96.105684 99.592391
INFO:root:train 900 1.371502e-01 96.101554 99.594201
INFO:root:train 950 1.374683e-01 96.097858 99.589248
INFO:root:train 1000 1.379216e-01 96.097652 99.584790
INFO:root:train 1050 1.387157e-01 96.073680 99.576296
INFO:root:train 1100 1.390539e-01 96.070334 99.574251
INFO:root:train 1150 1.398622e-01 96.061848 99.568310
INFO:root:train 1200 1.401881e-01 96.044963 99.566767
INFO:root:train 1250 1.402192e-01 96.038169 99.570344
INFO:root:train 1300 1.411906e-01 96.021090 99.564037
INFO:root:train 1350 1.407857e-01 96.027248 99.568607
INFO:root:train 1400 1.414078e-01 96.010662 99.567273
INFO:root:train 1450 1.414177e-01 96.003834 99.569263
INFO:root:train 1500 1.421352e-01 95.977682 99.571119
INFO:root:train 1550 1.418490e-01 95.983438 99.569834
INFO:root:train 1600 1.414401e-01 95.978100 99.572533
INFO:root:train 1650 1.408945e-01 95.982548 99.576961
INFO:root:train 1700 1.411015e-01 95.964690 99.577454
INFO:root:train 1750 1.411932e-01 95.970160 99.573458
INFO:root:train 1800 1.405818e-01 95.983134 99.579227
INFO:root:train 1850 1.403238e-01 95.991187 99.580463
INFO:root:train 1900 1.408081e-01 95.984844 99.580813
INFO:root:train 1950 1.407416e-01 95.986033 99.581144
INFO:root:train 2000 1.406696e-01 95.984039 99.582240
INFO:root:train 2050 1.410384e-01 95.976048 99.582521
INFO:root:train 2100 1.407871e-01 95.975131 99.584275
INFO:root:train 2150 1.406997e-01 95.972077 99.583769
INFO:root:train 2200 1.410237e-01 95.964192 99.582576
INFO:root:train 2250 1.411996e-01 95.962905 99.585601
INFO:root:train 2300 1.414135e-01 95.968465 99.582383
INFO:root:train 2350 1.416292e-01 95.971129 99.581960
INFO:root:train 2400 1.413174e-01 95.979540 99.584158
INFO:root:train 2450 1.414124e-01 95.984420 99.583078
INFO:root:train 2500 1.411955e-01 95.995977 99.582042
INFO:root:train 2550 1.406858e-01 96.010756 99.585947
INFO:root:train 2600 1.403758e-01 96.026168 99.583694
INFO:root:train 2650 1.403565e-01 96.025674 99.579168
INFO:root:train 2700 1.400425e-01 96.034455 99.578860
INFO:root:train 2750 1.400595e-01 96.027013 99.579698
INFO:root:train 2800 1.397766e-01 96.032109 99.579391
INFO:root:train 2850 1.401152e-01 96.017297 99.577999
INFO:root:train 2900 1.403964e-01 96.012690 99.573423
INFO:root:train 2950 1.406817e-01 96.000297 99.573767
INFO:root:train 3000 1.407577e-01 96.000812 99.572538
INFO:root:train 3050 1.408512e-01 95.994653 99.572886
INFO:root:train 3100 1.409723e-01 95.985670 99.572215
INFO:root:train 3150 1.407971e-01 95.982922 99.571565
INFO:root:train 3200 1.406666e-01 95.992463 99.572399
INFO:root:train 3250 1.404752e-01 95.998827 99.572247
INFO:root:train 3300 1.405069e-01 96.000265 99.572099
INFO:root:train 3350 1.407324e-01 96.001194 99.569625
INFO:root:train 3400 1.407169e-01 96.003473 99.568142
INFO:root:train 3450 1.404668e-01 96.008856 99.569871
INFO:root:train 3500 1.407307e-01 96.008730 99.567981
INFO:root:train 3550 1.407960e-01 96.005527 99.569663
INFO:root:train 3600 1.406790e-01 96.007186 99.572167
INFO:root:train_acc 96.004131
INFO:root:valid 000 3.113809e-01 90.625000 100.000000
INFO:root:valid 050 2.787635e-01 92.984069 98.682598
INFO:root:valid 100 2.655296e-01 93.146658 98.746906
INFO:root:valid 150 2.633551e-01 93.232616 98.747930
INFO:root:valid 200 2.637472e-01 93.244714 98.779540
INFO:root:valid 250 2.725144e-01 93.096365 98.686504
INFO:root:valid 300 2.732231e-01 93.075166 98.645141
INFO:root:valid 350 2.742167e-01 92.966524 98.651175
INFO:root:valid 400 2.734191e-01 92.959009 98.632325
INFO:root:valid 450 2.767418e-01 92.828437 98.614191
INFO:root:valid 500 2.766683e-01 92.839321 98.627745
INFO:root:valid 550 2.766296e-01 92.907781 98.616152
INFO:root:valid 600 2.808049e-01 92.866057 98.580491
INFO:root:test_acc 92.863258
INFO:root:epoch 6 lr 2.071445e-03
INFO:root:train 000 9.574165e-02 98.437500 100.000000
INFO:root:train 050 1.397217e-01 96.415441 99.540441
INFO:root:train 100 1.355464e-01 96.457302 99.613243
INFO:root:train 150 1.307721e-01 96.450745 99.617136
INFO:root:train 200 1.253210e-01 96.579602 99.650187
INFO:root:train 250 1.229391e-01 96.657122 99.676295
INFO:root:train 300 1.235698e-01 96.594684 99.683347
INFO:root:train 350 1.248218e-01 96.550036 99.679487
INFO:root:train 400 1.246482e-01 96.547693 99.688279
INFO:root:train 450 1.255580e-01 96.525083 99.677799
INFO:root:train 500 1.254110e-01 96.482036 99.663174
INFO:root:train 550 1.276433e-01 96.446801 99.645531
INFO:root:train 600 1.281758e-01 96.451227 99.646423
INFO:root:train 650 1.287635e-01 96.399770 99.642377
INFO:root:train 700 1.300024e-01 96.355653 99.645596
INFO:root:train 750 1.314974e-01 96.282041 99.633822
INFO:root:train 800 1.327454e-01 96.274189 99.613764
INFO:root:train 850 1.335528e-01 96.252571 99.605244
INFO:root:train 900 1.338368e-01 96.229883 99.606340
INFO:root:train 950 1.336301e-01 96.245728 99.605678
INFO:root:train 1000 1.340693e-01 96.241259 99.601961
INFO:root:train 1050 1.336194e-01 96.243161 99.600083
INFO:root:train 1100 1.332000e-01 96.243472 99.605472
INFO:root:train 1150 1.326276e-01 96.265476 99.600891
INFO:root:train 1200 1.319274e-01 96.285647 99.607098
INFO:root:train 1250 1.315564e-01 96.286721 99.607814
INFO:root:train 1300 1.311292e-01 96.285309 99.608474
INFO:root:train 1350 1.310083e-01 96.282846 99.613712
INFO:root:train 1400 1.319547e-01 96.267175 99.610769
INFO:root:train 1450 1.318811e-01 96.259045 99.612336
INFO:root:train 1500 1.312652e-01 96.269154 99.613799
INFO:root:train 1550 1.311773e-01 96.256447 99.614160
INFO:root:train 1600 1.319913e-01 96.251366 99.605715
INFO:root:train 1650 1.328359e-01 96.231451 99.602514
INFO:root:train 1700 1.330235e-01 96.227403 99.603175
INFO:root:train 1750 1.335820e-01 96.221802 99.598444
INFO:root:train 1800 1.338673e-01 96.215644 99.598313
INFO:root:train 1850 1.334032e-01 96.229234 99.601567
INFO:root:train 1900 1.335517e-01 96.223205 99.599717
INFO:root:train 1950 1.331772e-01 96.234303 99.602768
INFO:root:train 2000 1.331339e-01 96.233133 99.604104
INFO:root:train 2050 1.329853e-01 96.234306 99.603852
INFO:root:train 2100 1.329751e-01 96.224268 99.603611
INFO:root:train 2150 1.327092e-01 96.226319 99.604835
INFO:root:train 2200 1.326491e-01 96.227567 99.604583
INFO:root:train 2250 1.323653e-01 96.230148 99.607813
INFO:root:train 2300 1.320861e-01 96.242123 99.607508
INFO:root:train 2350 1.321465e-01 96.237638 99.607880
INFO:root:train 2400 1.320069e-01 96.232690 99.608236
INFO:root:train 2450 1.315654e-01 96.241968 99.612403
INFO:root:train 2500 1.315684e-01 96.234006 99.613904
INFO:root:train 2550 1.317201e-01 96.225745 99.614122
INFO:root:train 2600 1.317717e-01 96.230416 99.613730
INFO:root:train 2650 1.320917e-01 96.224302 99.610996
INFO:root:train 2700 1.322254e-01 96.220728 99.611255
INFO:root:train 2750 1.326103e-01 96.210469 99.604121
INFO:root:train 2800 1.327176e-01 96.210059 99.604494
INFO:root:train 2850 1.327262e-01 96.209115 99.604854
INFO:root:train 2900 1.328649e-01 96.205511 99.604662
INFO:root:train 2950 1.326264e-01 96.209971 99.607654
INFO:root:train 3000 1.328119e-01 96.204390 99.608464
INFO:root:train 3050 1.324583e-01 96.209235 99.610271
INFO:root:train 3100 1.323919e-01 96.209892 99.611516
INFO:root:train 3150 1.324174e-01 96.211520 99.610739
INFO:root:train 3200 1.323180e-01 96.208704 99.611450
INFO:root:train 3250 1.324294e-01 96.212223 99.610216
INFO:root:train 3300 1.325701e-01 96.213269 99.605707
INFO:root:train 3350 1.324367e-01 96.215216 99.607393
INFO:root:train 3400 1.322085e-01 96.220321 99.609490
INFO:root:train 3450 1.318558e-01 96.227996 99.609715
INFO:root:train 3500 1.319801e-01 96.225186 99.606362
INFO:root:train 3550 1.322400e-01 96.220695 99.604425
INFO:root:train 3600 1.322680e-01 96.217631 99.603409
INFO:root:train_acc 96.215437
INFO:root:valid 000 2.357253e-01 95.312500 98.437500
INFO:root:valid 050 2.631316e-01 93.566176 98.774510
INFO:root:valid 100 2.524312e-01 93.796411 98.731436
INFO:root:valid 150 2.489418e-01 93.822434 98.675497
INFO:root:valid 200 2.419305e-01 93.851057 98.771766
INFO:root:valid 250 2.460685e-01 93.700199 98.698954
INFO:root:valid 300 2.492881e-01 93.635797 98.691860
INFO:root:valid 350 2.485794e-01 93.629808 98.749110
INFO:root:valid 400 2.484738e-01 93.617519 98.757014
INFO:root:valid 450 2.508429e-01 93.538664 98.742378
INFO:root:valid 500 2.520610e-01 93.553518 98.755614
INFO:root:valid 550 2.533847e-01 93.582691 98.738090
INFO:root:valid 600 2.571896e-01 93.490017 98.718282
INFO:root:test_acc 93.491063
INFO:root:epoch 7 lr 1.864907e-03
INFO:root:train 000 2.718856e-01 92.187500 98.437500
INFO:root:train 050 1.371093e-01 96.415441 99.509804
INFO:root:train 100 1.270530e-01 96.287129 99.628713
INFO:root:train 150 1.179964e-01 96.543874 99.699917
INFO:root:train 200 1.215216e-01 96.548507 99.673507
INFO:root:train 250 1.185229e-01 96.613546 99.676295
INFO:root:train 300 1.187148e-01 96.594684 99.667774
INFO:root:train 350 1.204906e-01 96.558939 99.648326
INFO:root:train 400 1.206404e-01 96.582762 99.645418
INFO:root:train 450 1.202148e-01 96.622090 99.646619
INFO:root:train 500 1.217041e-01 96.584955 99.635105
INFO:root:train 550 1.199625e-01 96.639632 99.645531
INFO:root:train 600 1.190499e-01 96.656614 99.659422
INFO:root:train 650 1.191787e-01 96.620584 99.663978
INFO:root:train 700 1.199025e-01 96.587464 99.656740
INFO:root:train 750 1.197971e-01 96.567077 99.667111
INFO:root:train 800 1.200798e-01 96.545334 99.668383
INFO:root:train 850 1.193025e-01 96.559195 99.675015
INFO:root:train 900 1.193501e-01 96.547239 99.675708
INFO:root:train 950 1.202292e-01 96.500394 99.669756
INFO:root:train 1000 1.220922e-01 96.480082 99.659715
INFO:root:train 1050 1.222617e-01 96.463190 99.649144
INFO:root:train 1100 1.221529e-01 96.474796 99.646628
INFO:root:train 1150 1.220253e-01 96.478606 99.649761
INFO:root:train 1200 1.219634e-01 96.484700 99.648730
INFO:root:train 1250 1.220224e-01 96.485312 99.646533
INFO:root:train 1300 1.219682e-01 96.489479 99.648107
INFO:root:train 1350 1.220098e-01 96.493338 99.642626
INFO:root:train 1400 1.222435e-01 96.471271 99.641997
INFO:root:train 1450 1.222003e-01 96.457185 99.642488
INFO:root:train 1500 1.220445e-01 96.472144 99.639823
INFO:root:train 1550 1.220737e-01 96.469012 99.641360
INFO:root:train 1600 1.220592e-01 96.469004 99.643777
INFO:root:train 1650 1.223138e-01 96.467103 99.644155
INFO:root:train 1700 1.226885e-01 96.470826 99.640836
INFO:root:train 1750 1.227737e-01 96.476121 99.640384
INFO:root:train 1800 1.229666e-01 96.469843 99.641692
INFO:root:train 1850 1.235983e-01 96.462216 99.638709
INFO:root:train 1900 1.238645e-01 96.452525 99.638348
INFO:root:train 1950 1.235233e-01 96.452140 99.642011
INFO:root:train 2000 1.233547e-01 96.449432 99.643928
INFO:root:train 2050 1.236262e-01 96.433904 99.643467
INFO:root:train 2100 1.240878e-01 96.433246 99.640052
INFO:root:train 2150 1.238122e-01 96.435524 99.641155
INFO:root:train 2200 1.242360e-01 96.422791 99.636529
INFO:root:train 2250 1.243264e-01 96.421729 99.634190
INFO:root:train 2300 1.251711e-01 96.411886 99.625842
INFO:root:train 2350 1.251605e-01 96.403791 99.626489
INFO:root:train 2400 1.251708e-01 96.408398 99.625807
INFO:root:train 2450 1.251658e-01 96.408354 99.625153
INFO:root:train 2500 1.251402e-01 96.408936 99.628274
INFO:root:train 2550 1.254301e-01 96.401534 99.625147
INFO:root:train 2600 1.254365e-01 96.407632 99.621540
INFO:root:train 2650 1.254466e-01 96.400533 99.622784
INFO:root:train 2700 1.253085e-01 96.400639 99.625717
INFO:root:train 2750 1.254209e-01 96.396197 99.625704
INFO:root:train 2800 1.255192e-01 96.396376 99.627923
INFO:root:train 2850 1.250251e-01 96.409703 99.632256
INFO:root:train 2900 1.248831e-01 96.412875 99.632670
INFO:root:train 2950 1.247358e-01 96.411704 99.634128
INFO:root:train 3000 1.248826e-01 96.412133 99.629811
INFO:root:train 3050 1.249759e-01 96.409989 99.630244
INFO:root:train 3100 1.248734e-01 96.405393 99.631671
INFO:root:train 3150 1.253363e-01 96.393010 99.628094
INFO:root:train 3200 1.251951e-01 96.396634 99.629022
INFO:root:train 3250 1.251712e-01 96.394379 99.628480
INFO:root:train 3300 1.251415e-01 96.389352 99.630320
INFO:root:train 3350 1.249670e-01 96.395665 99.632572
INFO:root:train 3400 1.249625e-01 96.395362 99.631083
INFO:root:train 3450 1.251420e-01 96.392350 99.630542
INFO:root:train 3500 1.253074e-01 96.389871 99.630463
INFO:root:train 3550 1.253489e-01 96.393181 99.630386
INFO:root:train 3600 1.254211e-01 96.391627 99.630311
INFO:root:train_acc 96.395326
INFO:root:valid 000 2.899032e-01 93.750000 98.437500
INFO:root:valid 050 2.716431e-01 93.688725 98.560049
INFO:root:valid 100 2.573455e-01 93.889233 98.499381
INFO:root:valid 150 2.520949e-01 93.946606 98.551325
INFO:root:valid 200 2.515824e-01 93.835510 98.662935
INFO:root:valid 250 2.605674e-01 93.675299 98.593127
INFO:root:valid 300 2.599390e-01 93.677326 98.562085
INFO:root:valid 350 2.619785e-01 93.576389 98.579950
INFO:root:valid 400 2.613837e-01 93.578554 98.616739
INFO:root:valid 450 2.649806e-01 93.497090 98.614191
INFO:root:valid 500 2.652454e-01 93.478668 98.633982
INFO:root:valid 550 2.661673e-01 93.503289 98.604809
INFO:root:valid 600 2.693864e-01 93.464018 98.577891
INFO:root:test_acc 93.452149
INFO:root:epoch 8 lr 1.647528e-03
INFO:root:train 000 3.452123e-01 92.187500 98.437500
INFO:root:train 050 1.149261e-01 96.691176 99.754902
INFO:root:train 100 1.263531e-01 96.426361 99.597772
INFO:root:train 150 1.158180e-01 96.678394 99.606788
INFO:root:train 200 1.155667e-01 96.657338 99.626866
INFO:root:train 250 1.200569e-01 96.625996 99.607819
INFO:root:train 300 1.186546e-01 96.636213 99.621055
INFO:root:train 350 1.178116e-01 96.639067 99.634972
INFO:root:train 400 1.186482e-01 96.641209 99.625935
INFO:root:train 450 1.182057e-01 96.667129 99.636225
INFO:root:train 500 1.164410e-01 96.694112 99.653817
INFO:root:train 550 1.156327e-01 96.687840 99.662545
INFO:root:train 600 1.162672e-01 96.641015 99.656822
INFO:root:train 650 1.162102e-01 96.632584 99.654378
INFO:root:train 700 1.161418e-01 96.627586 99.654511
INFO:root:train 750 1.150030e-01 96.666944 99.658788
INFO:root:train 800 1.135914e-01 96.695537 99.670334
INFO:root:train 850 1.130629e-01 96.698737 99.678687
INFO:root:train 900 1.130231e-01 96.713721 99.682644
INFO:root:train 950 1.146073e-01 96.677839 99.681257
INFO:root:train 1000 1.143073e-01 96.697053 99.683129
INFO:root:train 1050 1.147927e-01 96.687678 99.675904
INFO:root:train 1100 1.153144e-01 96.676317 99.670754
INFO:root:train 1150 1.147209e-01 96.691735 99.671481
INFO:root:train 1200 1.153154e-01 96.673345 99.664342
INFO:root:train 1250 1.158281e-01 96.671413 99.656525
INFO:root:train 1300 1.159926e-01 96.661222 99.657715
INFO:root:train 1350 1.152486e-01 96.673760 99.659974
INFO:root:train 1400 1.152459e-01 96.680942 99.660956
INFO:root:train 1450 1.154887e-01 96.698398 99.659717
INFO:root:train 1500 1.159838e-01 96.679297 99.658561
INFO:root:train 1550 1.157953e-01 96.673517 99.659494
INFO:root:train 1600 1.162871e-01 96.658339 99.654513
INFO:root:train 1650 1.161087e-01 96.665846 99.657405
INFO:root:train 1700 1.165317e-01 96.653623 99.661045
INFO:root:train 1750 1.165127e-01 96.667083 99.660016
INFO:root:train 1800 1.170504e-01 96.657239 99.659044
INFO:root:train 1850 1.172320e-01 96.659745 99.660656
INFO:root:train 1900 1.172002e-01 96.662941 99.657253
INFO:root:train 1950 1.171061e-01 96.668375 99.658028
INFO:root:train 2000 1.174339e-01 96.658702 99.659545
INFO:root:train 2050 1.173634e-01 96.659404 99.660227
INFO:root:train 2100 1.175209e-01 96.664535 99.658645
INFO:root:train 2150 1.173194e-01 96.668701 99.662221
INFO:root:train 2200 1.175602e-01 96.658479 99.664215
INFO:root:train 2250 1.174002e-01 96.663289 99.665426
INFO:root:train 2300 1.178629e-01 96.650913 99.660474
INFO:root:train 2350 1.179680e-01 96.650362 99.660384
INFO:root:train 2400 1.178298e-01 96.648532 99.660949
INFO:root:train 2450 1.179067e-01 96.646777 99.658940
INFO:root:train 2500 1.181527e-01 96.639469 99.659511
INFO:root:train 2550 1.180184e-01 96.641023 99.662510
INFO:root:train 2600 1.179954e-01 96.640114 99.664192
INFO:root:train 2650 1.178047e-01 96.649849 99.665810
INFO:root:train 2700 1.174974e-01 96.667322 99.669104
INFO:root:train 2750 1.178027e-01 96.653490 99.666031
INFO:root:train 2800 1.179037e-01 96.641824 99.665298
INFO:root:train 2850 1.180064e-01 96.642625 99.664043
INFO:root:train 2900 1.178874e-01 96.647708 99.662832
INFO:root:train 2950 1.178160e-01 96.645205 99.663779
INFO:root:train 3000 1.178365e-01 96.642786 99.664174
INFO:root:train 3050 1.176459e-01 96.646079 99.665581
INFO:root:train 3100 1.175770e-01 96.646747 99.665431
INFO:root:train 3150 1.176096e-01 96.645410 99.665781
INFO:root:train 3200 1.175257e-01 96.648508 99.665632
INFO:root:train 3250 1.173739e-01 96.655356 99.666449
INFO:root:train 3300 1.176481e-01 96.652056 99.668188
INFO:root:train 3350 1.177766e-01 96.646990 99.668010
INFO:root:train 3400 1.178721e-01 96.640694 99.668296
INFO:root:train 3450 1.180470e-01 96.630958 99.666310
INFO:root:train 3500 1.182620e-01 96.629534 99.665721
INFO:root:train 3550 1.184719e-01 96.619350 99.667787
INFO:root:train 3600 1.183540e-01 96.623768 99.667193
INFO:root:train_acc 96.622985
INFO:root:valid 000 2.582158e-01 93.750000 96.875000
INFO:root:valid 050 2.563786e-01 94.178922 98.774510
INFO:root:valid 100 2.436644e-01 94.245050 98.700495
INFO:root:valid 150 2.419607e-01 94.184603 98.747930
INFO:root:valid 200 2.380686e-01 94.123134 98.841729
INFO:root:valid 250 2.448676e-01 94.067480 98.786106
INFO:root:valid 300 2.431490e-01 94.077035 98.764535
INFO:root:valid 350 2.461235e-01 94.012642 98.793625
INFO:root:valid 400 2.454009e-01 94.038342 98.799875
INFO:root:valid 450 2.489073e-01 93.937084 98.759701
INFO:root:valid 500 2.495982e-01 93.918413 98.771208
INFO:root:valid 550 2.508472e-01 93.925817 98.749433
INFO:root:valid 600 2.540364e-01 93.879992 98.749480
INFO:root:test_acc 93.875010
INFO:root:epoch 9 lr 1.424662e-03
INFO:root:train 000 8.021007e-02 96.875000 100.000000
INFO:root:train 050 9.090998e-02 97.671569 99.662990
INFO:root:train 100 9.308691e-02 97.431931 99.706064
INFO:root:train 150 9.464844e-02 97.413079 99.720613
INFO:root:train 200 9.592862e-02 97.426928 99.735697
INFO:root:train 250 1.000293e-01 97.192480 99.763446
INFO:root:train 300 1.003149e-01 97.155316 99.756022
INFO:root:train 350 1.010116e-01 97.146546 99.746261
INFO:root:train 400 1.015735e-01 97.108791 99.731141
INFO:root:train 450 1.018606e-01 97.134839 99.729767
INFO:root:train 500 1.023942e-01 97.102670 99.731786
INFO:root:train 550 1.025727e-01 97.087681 99.739111
INFO:root:train 600 1.040867e-01 97.069988 99.729617
INFO:root:train 650 1.064624e-01 97.016609 99.714382
INFO:root:train 700 1.065440e-01 97.002051 99.716922
INFO:root:train 750 1.077050e-01 96.974867 99.714963
INFO:root:train 800 1.069866e-01 96.982288 99.721052
INFO:root:train 850 1.074464e-01 96.968640 99.722753
INFO:root:train 900 1.071834e-01 96.956507 99.729467
INFO:root:train 950 1.078866e-01 96.937434 99.725618
INFO:root:train 1000 1.077049e-01 96.940559 99.726836
INFO:root:train 1050 1.076016e-01 96.940414 99.724964
INFO:root:train 1100 1.075779e-01 96.938862 99.726101
INFO:root:train 1150 1.073384e-01 96.934731 99.724424
INFO:root:train 1200 1.071595e-01 96.937448 99.725489
INFO:root:train 1250 1.076315e-01 96.926209 99.715228
INFO:root:train 1300 1.074874e-01 96.930246 99.715363
INFO:root:train 1350 1.074272e-01 96.940923 99.715489
INFO:root:train 1400 1.068520e-01 96.957530 99.718951
INFO:root:train 1450 1.069886e-01 96.947149 99.720021
INFO:root:train 1500 1.072818e-01 96.941622 99.717896
INFO:root:train 1550 1.071541e-01 96.954586 99.715909
INFO:root:train 1600 1.076755e-01 96.951124 99.712094
INFO:root:train 1650 1.078548e-01 96.943141 99.712296
INFO:root:train 1700 1.074982e-01 96.953998 99.713404
INFO:root:train 1750 1.074031e-01 96.957988 99.716234
INFO:root:train 1800 1.082288e-01 96.943538 99.714568
INFO:root:train 1850 1.086319e-01 96.940843 99.710461
INFO:root:train 1900 1.091474e-01 96.929248 99.712322
INFO:root:train 1950 1.094719e-01 96.919849 99.712487
INFO:root:train 2000 1.099609e-01 96.911700 99.710301
INFO:root:train 2050 1.098457e-01 96.915377 99.712793
INFO:root:train 2100 1.099684e-01 96.910697 99.712934
INFO:root:train 2150 1.100121e-01 96.905509 99.712343
INFO:root:train 2200 1.096746e-01 96.920434 99.713908
INFO:root:train 2250 1.096417e-01 96.913872 99.713322
INFO:root:train 2300 1.098889e-01 96.906915 99.710724
INFO:root:train 2350 1.095578e-01 96.913547 99.710894
INFO:root:train 2400 1.095561e-01 96.910142 99.713010
INFO:root:train 2450 1.095865e-01 96.904325 99.712490
INFO:root:train 2500 1.098154e-01 96.898116 99.711990
INFO:root:train 2550 1.100048e-01 96.897663 99.708448
INFO:root:train 2600 1.100363e-01 96.899029 99.708045
INFO:root:train 2650 1.099718e-01 96.897987 99.708247
INFO:root:train 2700 1.098447e-01 96.903346 99.708441
INFO:root:train 2750 1.096031e-01 96.914190 99.708629
INFO:root:train 2800 1.093447e-01 96.920185 99.711041
INFO:root:train 2850 1.093959e-01 96.916652 99.710628
INFO:root:train 2900 1.092504e-01 96.916473 99.712384
INFO:root:train 2950 1.095090e-01 96.910475 99.710903
INFO:root:train 3000 1.094176e-01 96.908843 99.710513
INFO:root:train 3050 1.095688e-01 96.899070 99.710648
INFO:root:train 3100 1.099709e-01 96.885077 99.709267
INFO:root:train 3150 1.102932e-01 96.877975 99.707930
INFO:root:train 3200 1.105396e-01 96.874512 99.706147
INFO:root:train 3250 1.107872e-01 96.871636 99.705379
INFO:root:train 3300 1.107407e-01 96.868373 99.704635
INFO:root:train 3350 1.110019e-01 96.861478 99.702514
INFO:root:train 3400 1.108614e-01 96.864893 99.703212
INFO:root:train 3450 1.107376e-01 96.869567 99.703890
INFO:root:train 3500 1.107292e-01 96.871430 99.702317
INFO:root:train 3550 1.109721e-01 96.869280 99.700789
INFO:root:train 3600 1.107721e-01 96.871963 99.701038
INFO:root:train_acc 96.868289
INFO:root:valid 000 3.306535e-01 93.750000 98.437500
INFO:root:valid 050 2.511079e-01 93.750000 98.805147
INFO:root:valid 100 2.290763e-01 94.183168 98.839728
INFO:root:valid 150 2.243729e-01 94.370861 98.841060
INFO:root:valid 200 2.235773e-01 94.348570 98.919465
INFO:root:valid 250 2.277520e-01 94.372510 98.835906
INFO:root:valid 300 2.275303e-01 94.331395 98.852782
INFO:root:valid 350 2.277580e-01 94.253027 98.887108
INFO:root:valid 400 2.268692e-01 94.318890 98.905081
INFO:root:valid 450 2.294582e-01 94.214246 98.898282
INFO:root:valid 500 2.309526e-01 94.171033 98.886602
INFO:root:valid 550 2.321283e-01 94.200885 98.862863
INFO:root:valid 600 2.359105e-01 94.147775 98.845674
INFO:root:test_acc 94.147404
INFO:root:epoch 10 lr 1.201796e-03
INFO:root:train 000 1.041573e-01 96.875000 100.000000
INFO:root:train 050 9.710251e-02 97.457108 99.662990
INFO:root:train 100 1.044117e-01 97.246287 99.582302
INFO:root:train 150 9.790932e-02 97.247517 99.679222
INFO:root:train 200 9.350289e-02 97.364739 99.712376
INFO:root:train 250 9.628868e-02 97.273406 99.719871
INFO:root:train 300 9.809556e-02 97.233181 99.714493
INFO:root:train 350 9.807312e-02 97.275641 99.719551
INFO:root:train 400 1.005576e-01 97.198410 99.703865
INFO:root:train 450 9.862112e-02 97.231846 99.719374
INFO:root:train 500 9.939133e-02 97.202470 99.719311
INFO:root:train 550 9.955077e-02 97.189769 99.727768
INFO:root:train 600 9.977478e-02 97.160982 99.734817
INFO:root:train 650 9.941895e-02 97.170219 99.740783
INFO:root:train 700 9.937055e-02 97.173680 99.745899
INFO:root:train 750 9.926819e-02 97.178762 99.748252
INFO:root:train 800 9.791386e-02 97.212469 99.754213
INFO:root:train 850 9.873199e-02 97.188969 99.753966
INFO:root:train 900 9.952834e-02 97.171546 99.745075
INFO:root:train 950 1.001630e-01 97.132952 99.746977
INFO:root:train 1000 1.001316e-01 97.129433 99.745567
INFO:root:train 1050 1.005115e-01 97.112869 99.742804
INFO:root:train 1100 9.981456e-02 97.131869 99.744550
INFO:root:train 1150 9.926936e-02 97.126140 99.750217
INFO:root:train 1200 9.928864e-02 97.109180 99.751509
INFO:root:train 1250 9.960275e-02 97.102318 99.750200
INFO:root:train 1300 9.999971e-02 97.097185 99.751393
INFO:root:train 1350 1.002545e-01 97.097058 99.745559
INFO:root:train 1400 1.007364e-01 97.086902 99.742372
INFO:root:train 1450 1.008798e-01 97.090369 99.743711
INFO:root:train 1500 1.006019e-01 97.089440 99.744962
INFO:root:train 1550 1.012037e-01 97.077490 99.736057
INFO:root:train 1600 1.013247e-01 97.088734 99.737469
INFO:root:train 1650 1.013639e-01 97.092671 99.737848
INFO:root:train 1700 1.017233e-01 97.076168 99.737287
INFO:root:train 1750 1.015661e-01 97.077563 99.735865
INFO:root:train 1800 1.017414e-01 97.063263 99.737125
INFO:root:train 1850 1.020769e-01 97.048892 99.735785
INFO:root:train 1900 1.019059e-01 97.054182 99.737802
INFO:root:train 1950 1.025806e-01 97.032772 99.734912
INFO:root:train 2000 1.027009e-01 97.027268 99.733727
INFO:root:train 2050 1.023001e-01 97.038792 99.737171
INFO:root:train 2100 1.030212e-01 97.024482 99.733014
INFO:root:train 2150 1.030753e-01 97.021008 99.731956
INFO:root:train 2200 1.034205e-01 97.007752 99.733076
INFO:root:train 2250 1.034976e-01 96.996474 99.734146
INFO:root:train 2300 1.034313e-01 96.996550 99.733811
INFO:root:train 2350 1.037196e-01 96.994630 99.733491
INFO:root:train 2400 1.039658e-01 96.983028 99.733184
INFO:root:train 2450 1.041277e-01 96.978912 99.731615
INFO:root:train 2500 1.044296e-01 96.971836 99.730108
INFO:root:train 2550 1.044329e-01 96.974226 99.731110
INFO:root:train 2600 1.042809e-01 96.974121 99.732675
INFO:root:train 2650 1.039687e-01 96.982271 99.734770
INFO:root:train 2700 1.040521e-01 96.972765 99.736209
INFO:root:train 2750 1.041355e-01 96.974396 99.736459
INFO:root:train 2800 1.042029e-01 96.982105 99.736143
INFO:root:train 2850 1.042055e-01 96.982967 99.736934
INFO:root:train 2900 1.041853e-01 96.985953 99.735544
INFO:root:train 2950 1.042101e-01 96.992545 99.735259
INFO:root:train 3000 1.042362e-01 96.991628 99.734984
INFO:root:train 3050 1.045842e-01 96.987668 99.732157
INFO:root:train 3100 1.046231e-01 96.990386 99.730934
INFO:root:train 3150 1.048505e-01 96.988555 99.731732
INFO:root:train 3200 1.049247e-01 96.983853 99.731041
INFO:root:train 3250 1.049369e-01 96.983140 99.731333
INFO:root:train 3300 1.051333e-01 96.975348 99.730195
INFO:root:train 3350 1.051822e-01 96.977581 99.730491
INFO:root:train 3400 1.051338e-01 96.979289 99.729399
INFO:root:train 3450 1.052460e-01 96.974156 99.730603
INFO:root:train 3500 1.050839e-01 96.972294 99.732666
INFO:root:train 3550 1.048389e-01 96.981484 99.733790
INFO:root:train 3600 1.050665e-01 96.979138 99.730110
INFO:root:train_acc 96.984055
INFO:root:valid 000 3.708431e-01 93.750000 98.437500
INFO:root:valid 050 2.434701e-01 94.393382 98.927696
INFO:root:valid 100 2.231460e-01 94.678218 98.886139
INFO:root:valid 150 2.151419e-01 94.836507 98.892798
INFO:root:valid 200 2.101545e-01 94.768346 98.973881
INFO:root:valid 250 2.166100e-01 94.658865 98.910608
INFO:root:valid 300 2.171985e-01 94.679194 98.878738
INFO:root:valid 350 2.175707e-01 94.671474 98.900463
INFO:root:valid 400 2.165260e-01 94.669576 98.897288
INFO:root:valid 450 2.201997e-01 94.609202 98.880959
INFO:root:valid 500 2.218073e-01 94.542166 98.883483
INFO:root:valid 550 2.231541e-01 94.566697 98.865699
INFO:root:valid 600 2.270012e-01 94.506552 98.840474
INFO:root:test_acc 94.502815
INFO:root:epoch 11 lr 9.844172e-04
INFO:root:train 000 4.653120e-02 98.437500 100.000000
INFO:root:train 050 9.510291e-02 97.273284 99.785539
INFO:root:train 100 1.032503e-01 97.060644 99.752475
INFO:root:train 150 1.068005e-01 97.081954 99.699917
INFO:root:train 200 1.056422e-01 97.053794 99.712376
INFO:root:train 250 1.059743e-01 97.080428 99.719871
INFO:root:train 300 1.033778e-01 97.103405 99.714493
INFO:root:train 350 1.010258e-01 97.195513 99.732906
INFO:root:train 400 1.013474e-01 97.210100 99.711658
INFO:root:train 450 1.019810e-01 97.211059 99.702051
INFO:root:train 500 1.007763e-01 97.196233 99.706836
INFO:root:train 550 9.954958e-02 97.237976 99.710753
INFO:root:train 600 9.900550e-02 97.241577 99.703619
INFO:root:train 650 9.886039e-02 97.232623 99.714382
INFO:root:train 700 9.746405e-02 97.238320 99.732525
INFO:root:train 750 9.759511e-02 97.228695 99.729527
INFO:root:train 800 9.731209e-02 97.233926 99.738608
INFO:root:train 850 9.757721e-02 97.222018 99.739277
INFO:root:train 900 9.691535e-02 97.246115 99.746809
INFO:root:train 950 9.717068e-02 97.236462 99.740405
INFO:root:train 1000 9.674250e-02 97.255869 99.736201
INFO:root:train 1050 9.727399e-02 97.248157 99.723478
INFO:root:train 1100 9.730079e-02 97.248240 99.726101
INFO:root:train 1150 9.774298e-02 97.236099 99.731212
INFO:root:train 1200 9.781745e-02 97.226270 99.729392
INFO:root:train 1250 9.751421e-02 97.223471 99.731465
INFO:root:train 1300 9.708714e-02 97.222089 99.732177
INFO:root:train 1350 9.674759e-02 97.225435 99.733993
INFO:root:train 1400 9.646455e-02 97.231888 99.735680
INFO:root:train 1450 9.657681e-02 97.226051 99.734020
INFO:root:train 1500 9.712357e-02 97.218521 99.730388
INFO:root:train 1550 9.679793e-02 97.224573 99.727998
INFO:root:train 1600 9.680916e-02 97.231223 99.729661
INFO:root:train 1650 9.705071e-02 97.215703 99.725545
INFO:root:train 1700 9.749810e-02 97.214874 99.719834
INFO:root:train 1750 9.744047e-02 97.215877 99.720695
INFO:root:train 1800 9.736776e-02 97.216824 99.721509
INFO:root:train 1850 9.744047e-02 97.216876 99.721434
INFO:root:train 1900 9.734125e-02 97.219391 99.721364
INFO:root:train 1950 9.720464e-02 97.221777 99.722899
INFO:root:train 2000 9.715381e-02 97.224825 99.725137
INFO:root:train 2050 9.717501e-02 97.217059 99.728791
INFO:root:train 2100 9.769615e-02 97.212637 99.728552
INFO:root:train 2150 9.763475e-02 97.207694 99.730503
INFO:root:train 2200 9.772783e-02 97.207945 99.732366
INFO:root:train 2250 9.773122e-02 97.215821 99.732064
INFO:root:train 2300 9.819827e-02 97.213168 99.731095
INFO:root:train 2350 9.810156e-02 97.213287 99.733491
INFO:root:train 2400 9.798962e-02 97.209496 99.735136
INFO:root:train 2450 9.819904e-02 97.209685 99.733527
INFO:root:train 2500 9.836493e-02 97.204868 99.735106
INFO:root:train 2550 9.874462e-02 97.195953 99.735398
INFO:root:train 2600 9.871001e-02 97.198193 99.735679
INFO:root:train 2650 9.865004e-02 97.201528 99.735359
INFO:root:train 2700 9.888345e-02 97.193748 99.735052
INFO:root:train 2750 9.886139e-02 97.191362 99.733620
INFO:root:train 2800 9.922454e-02 97.180137 99.731681
INFO:root:train 2850 9.924781e-02 97.178622 99.730358
INFO:root:train 2900 9.976022e-02 97.173388 99.726387
INFO:root:train 2950 9.990099e-02 97.170451 99.723081
INFO:root:train 3000 1.000321e-01 97.169693 99.722488
INFO:root:train 3050 9.995932e-02 97.169985 99.722939
INFO:root:train 3100 1.001518e-01 97.161198 99.721864
INFO:root:train 3150 1.000201e-01 97.165582 99.723302
INFO:root:train 3200 9.963736e-02 97.171294 99.725672
INFO:root:train 3250 9.950526e-02 97.173946 99.726046
INFO:root:train 3300 9.983841e-02 97.168945 99.722622
INFO:root:train 3350 9.978095e-02 97.168289 99.723497
INFO:root:train 3400 9.997316e-02 97.159843 99.723886
INFO:root:train 3450 9.995191e-02 97.163866 99.725170
INFO:root:train 3500 9.996792e-02 97.164203 99.725079
INFO:root:train 3550 1.000483e-01 97.157491 99.724989
INFO:root:train 3600 1.001302e-01 97.163548 99.725337
INFO:root:train_acc 97.166527
INFO:root:valid 000 3.032231e-01 90.625000 98.437500
INFO:root:valid 050 2.377386e-01 94.362745 98.805147
INFO:root:valid 100 2.198360e-01 94.709158 98.839728
INFO:root:valid 150 2.140194e-01 94.846854 98.851407
INFO:root:valid 200 2.078537e-01 94.908271 98.919465
INFO:root:valid 250 2.126743e-01 94.858068 98.854582
INFO:root:valid 300 2.115132e-01 94.886836 98.868355
INFO:root:valid 350 2.126224e-01 94.853989 98.878205
INFO:root:valid 400 2.120453e-01 94.841022 98.905081
INFO:root:valid 450 2.157313e-01 94.726996 98.874030
INFO:root:valid 500 2.175305e-01 94.641966 98.874127
INFO:root:valid 550 2.195155e-01 94.634755 98.854356
INFO:root:valid 600 2.218591e-01 94.620944 98.856073
INFO:root:test_acc 94.619555
INFO:root:epoch 12 lr 7.778790e-04
INFO:root:train 000 2.006124e-01 93.750000 100.000000
INFO:root:train 050 9.758790e-02 97.303922 99.785539
INFO:root:train 100 9.487110e-02 97.308168 99.783416
INFO:root:train 150 9.328336e-02 97.433775 99.793046
INFO:root:train 200 9.392082e-02 97.341418 99.790112
INFO:root:train 250 9.036836e-02 97.410359 99.800797
INFO:root:train 300 9.083135e-02 97.388912 99.787168
INFO:root:train 350 9.320770e-02 97.364672 99.777422
INFO:root:train 400 9.443043e-02 97.315305 99.774002
INFO:root:train 450 9.480361e-02 97.256098 99.764412
INFO:root:train 500 9.275214e-02 97.280439 99.775449
INFO:root:train 550 9.160318e-02 97.314542 99.775975
INFO:root:train 600 9.067801e-02 97.363769 99.779014
INFO:root:train 650 9.041162e-02 97.347830 99.779186
INFO:root:train 700 9.117693e-02 97.311876 99.774875
INFO:root:train 750 9.104454e-02 97.324401 99.771138
INFO:root:train 800 9.164749e-02 97.333411 99.771770
INFO:root:train 850 9.048388e-02 97.367068 99.777835
INFO:root:train 900 9.061364e-02 97.353635 99.774556
INFO:root:train 950 8.999591e-02 97.361330 99.779837
INFO:root:train 1000 9.004403e-02 97.369818 99.781469
INFO:root:train 1050 9.039982e-02 97.365604 99.781458
INFO:root:train 1100 9.032961e-02 97.366031 99.780030
INFO:root:train 1150 9.086875e-02 97.348773 99.777368
INFO:root:train 1200 9.150812e-02 97.343360 99.772325
INFO:root:train 1250 9.160402e-02 97.345873 99.771433
INFO:root:train 1300 9.145123e-02 97.342189 99.776614
INFO:root:train 1350 9.100176e-02 97.345716 99.776786
INFO:root:train 1400 9.161409e-02 97.332263 99.774714
INFO:root:train 1450 9.178673e-02 97.325121 99.773863
INFO:root:train 1500 9.242824e-02 97.312209 99.773068
INFO:root:train 1550 9.253824e-02 97.314233 99.771317
INFO:root:train 1600 9.215478e-02 97.332722 99.774555
INFO:root:train 1650 9.214145e-02 97.335895 99.774758
INFO:root:train 1700 9.181088e-02 97.342556 99.776786
INFO:root:train 1750 9.190332e-02 97.347052 99.773344
INFO:root:train 1800 9.206400e-02 97.337417 99.773563
INFO:root:train 1850 9.210058e-02 97.340120 99.772083
INFO:root:train 1900 9.202016e-02 97.343503 99.772324
INFO:root:train 1950 9.219269e-02 97.340306 99.773353
INFO:root:train 2000 9.199010e-02 97.343516 99.775112
INFO:root:train 2050 9.224866e-02 97.335141 99.774500
INFO:root:train 2100 9.237187e-02 97.335346 99.773173
INFO:root:train 2150 9.232916e-02 97.337721 99.774088
INFO:root:train 2200 9.227211e-02 97.342827 99.773540
INFO:root:train 2250 9.217042e-02 97.344236 99.773018
INFO:root:train 2300 9.241241e-02 97.348300 99.771159
INFO:root:train 2350 9.219379e-02 97.351526 99.772703
INFO:root:train 2400 9.204650e-02 97.362427 99.774183
INFO:root:train 2450 9.202050e-02 97.358221 99.775602
INFO:root:train 2500 9.216500e-02 97.351684 99.777589
INFO:root:train 2550 9.234919e-02 97.339891 99.776436
INFO:root:train 2600 9.251386e-02 97.338764 99.775928
INFO:root:train 2650 9.193760e-02 97.354772 99.777796
INFO:root:train 2700 9.195302e-02 97.352832 99.777860
INFO:root:train 2750 9.206488e-02 97.348691 99.778490
INFO:root:train 2800 9.226067e-02 97.348603 99.777981
INFO:root:train 2850 9.255599e-02 97.343038 99.774750
INFO:root:train 2900 9.297028e-02 97.331739 99.771630
INFO:root:train 2950 9.327841e-02 97.321882 99.769146
INFO:root:train 3000 9.329965e-02 97.320685 99.768827
INFO:root:train 3050 9.338796e-02 97.324136 99.765958
INFO:root:train 3100 9.314851e-02 97.331506 99.766708
INFO:root:train 3150 9.315053e-02 97.330213 99.764956
INFO:root:train 3200 9.302903e-02 97.332865 99.766186
INFO:root:train 3250 9.316369e-02 97.333032 99.764976
INFO:root:train 3300 9.333168e-02 97.329408 99.763329
INFO:root:train 3350 9.339659e-02 97.331953 99.762198
INFO:root:train 3400 9.353868e-02 97.325695 99.761100
INFO:root:train 3450 9.352794e-02 97.322787 99.760939
INFO:root:train 3500 9.353489e-02 97.327996 99.759444
INFO:root:train 3550 9.349694e-02 97.326017 99.760631
INFO:root:train 3600 9.332242e-02 97.331904 99.760483
INFO:root:train_acc 97.327911
INFO:root:valid 000 3.202814e-01 92.187500 98.437500
INFO:root:valid 050 2.299945e-01 94.699755 98.897059
INFO:root:valid 100 2.108631e-01 95.034035 98.901609
INFO:root:valid 150 2.049014e-01 95.177980 98.892798
INFO:root:valid 200 2.013612e-01 95.102612 98.997201
INFO:root:valid 250 2.068743e-01 95.075946 98.947958
INFO:root:valid 300 2.058967e-01 95.026993 98.946221
INFO:root:valid 350 2.075261e-01 94.974181 98.962785
INFO:root:valid 400 2.069534e-01 94.981297 98.998597
INFO:root:valid 450 2.097824e-01 94.896757 98.960643
INFO:root:valid 500 2.110241e-01 94.860279 98.961452
INFO:root:valid 550 2.125485e-01 94.872958 98.942264
INFO:root:valid 600 2.153634e-01 94.849730 98.921069
INFO:root:test_acc 94.845254
INFO:root:epoch 13 lr 5.872667e-04
INFO:root:train 000 5.291697e-02 98.437500 100.000000
INFO:root:train 050 7.436818e-02 97.977941 99.877451
INFO:root:train 100 7.415514e-02 97.911510 99.922649
INFO:root:train 150 7.448903e-02 97.868377 99.906871
INFO:root:train 200 7.953989e-02 97.652363 99.898943
INFO:root:train 250 8.093055e-02 97.584661 99.894173
INFO:root:train 300 8.469433e-02 97.580980 99.844269
INFO:root:train 350 8.604436e-02 97.542735 99.839744
INFO:root:train 400 8.607171e-02 97.529613 99.828554
INFO:root:train 450 8.644472e-02 97.505543 99.816380
INFO:root:train 500 8.696529e-02 97.473802 99.812874
INFO:root:train 550 8.728701e-02 97.476180 99.801497
INFO:root:train 600 8.746117e-02 97.475562 99.797213
INFO:root:train 650 8.781416e-02 97.451037 99.798387
INFO:root:train 700 8.810257e-02 97.441155 99.794936
INFO:root:train 750 8.724306e-02 97.453395 99.800266
INFO:root:train 800 8.724156e-02 97.448502 99.801030
INFO:root:train 850 8.777785e-02 97.468052 99.794360
INFO:root:train 900 8.788936e-02 97.459420 99.793632
INFO:root:train 950 8.742011e-02 97.481270 99.791338
INFO:root:train 1000 8.787467e-02 97.472840 99.790834
INFO:root:train 1050 8.830193e-02 97.468185 99.785918
INFO:root:train 1100 8.842981e-02 97.463953 99.789964
INFO:root:train 1150 8.817375e-02 97.476379 99.789585
INFO:root:train 1200 8.842503e-02 97.469557 99.790539
INFO:root:train 1250 8.856830e-02 97.457034 99.791417
INFO:root:train 1300 8.847111e-02 97.455083 99.793428
INFO:root:train 1350 8.896843e-02 97.444023 99.792977
INFO:root:train 1400 8.919542e-02 97.435983 99.791444
INFO:root:train 1450 8.884733e-02 97.440343 99.792169
INFO:root:train 1500 8.889063e-02 97.437125 99.791805
INFO:root:train 1550 8.908620e-02 97.429078 99.791465
INFO:root:train 1600 8.877526e-02 97.445932 99.790170
INFO:root:train 1650 8.909910e-02 97.430535 99.791793
INFO:root:train 1700 8.932310e-02 97.429821 99.790564
INFO:root:train 1750 8.903576e-02 97.441640 99.790298
INFO:root:train 1800 8.911803e-02 97.431982 99.790047
INFO:root:train 1850 8.903525e-02 97.436352 99.790654
INFO:root:train 1900 8.915813e-02 97.428985 99.790406
INFO:root:train 1950 8.925480e-02 97.425999 99.789371
INFO:root:train 2000 8.932625e-02 97.427068 99.789168
INFO:root:train 2050 8.923991e-02 97.425798 99.787451
INFO:root:train 2100 8.887783e-02 97.443182 99.787304
INFO:root:train 2150 8.873611e-02 97.447408 99.789342
INFO:root:train 2200 8.890600e-02 97.436535 99.787739
INFO:root:train 2250 8.916167e-02 97.435168 99.785512
INFO:root:train 2300 8.973002e-02 97.427070 99.782703
INFO:root:train 2350 8.973835e-02 97.423304 99.783337
INFO:root:train 2400 8.975131e-02 97.424250 99.783293
INFO:root:train 2450 8.963012e-02 97.425796 99.785164
INFO:root:train 2500 8.936256e-02 97.431028 99.787585
INFO:root:train 2550 8.921508e-02 97.429929 99.788073
INFO:root:train 2600 8.950963e-02 97.425269 99.783136
INFO:root:train 2650 8.956418e-02 97.420195 99.784869
INFO:root:train 2700 8.926488e-02 97.424565 99.786537
INFO:root:train 2750 8.915739e-02 97.428776 99.784737
INFO:root:train 2800 8.907360e-02 97.433394 99.785233
INFO:root:train 2850 8.905517e-02 97.427986 99.786807
INFO:root:train 2900 8.917159e-02 97.427611 99.786711
INFO:root:train 2950 8.909115e-02 97.428838 99.787678
INFO:root:train 3000 8.919560e-02 97.427420 99.787571
INFO:root:train 3050 8.924780e-02 97.425025 99.787979
INFO:root:train 3100 8.937807e-02 97.424722 99.788879
INFO:root:train 3150 8.934585e-02 97.427900 99.788262
INFO:root:train 3200 8.949100e-02 97.428538 99.788640
INFO:root:train 3250 8.955851e-02 97.423870 99.786604
INFO:root:train 3300 8.941265e-02 97.425023 99.786523
INFO:root:train 3350 8.960727e-02 97.421479 99.785978
INFO:root:train 3400 8.972823e-02 97.421714 99.786368
INFO:root:train 3450 8.979870e-02 97.418321 99.786294
INFO:root:train 3500 8.977928e-02 97.417256 99.786222
INFO:root:train 3550 8.968888e-02 97.417101 99.785712
INFO:root:train 3600 8.966145e-02 97.419988 99.786952
INFO:root:train_acc 97.420007
INFO:root:valid 000 2.143254e-01 95.312500 98.437500
INFO:root:valid 050 2.244021e-01 94.914216 98.866422
INFO:root:valid 100 2.076941e-01 95.095916 98.948020
INFO:root:valid 150 2.017105e-01 95.188328 98.944536
INFO:root:valid 200 1.983177e-01 95.188122 99.028296
INFO:root:valid 250 2.023264e-01 95.231574 98.941733
INFO:root:valid 300 2.018088e-01 95.245017 98.951412
INFO:root:valid 350 2.025676e-01 95.210114 98.985043
INFO:root:valid 400 2.011503e-01 95.203398 99.006390
INFO:root:valid 450 2.038160e-01 95.108093 98.984895
INFO:root:valid 500 2.050143e-01 95.053643 98.986402
INFO:root:valid 550 2.069460e-01 95.062954 98.979129
INFO:root:valid 600 2.100193e-01 95.049917 98.983465
INFO:root:test_acc 95.047604
INFO:root:epoch 14 lr 4.172738e-04
INFO:root:train 000 2.177042e-02 100.000000 100.000000
INFO:root:train 050 8.696788e-02 97.549020 99.785539
INFO:root:train 100 8.582252e-02 97.633045 99.767946
INFO:root:train 150 8.657814e-02 97.651076 99.772351
INFO:root:train 200 8.688593e-02 97.675684 99.782338
INFO:root:train 250 8.561766e-02 97.709163 99.788347
INFO:root:train 300 8.491406e-02 97.684801 99.813123
INFO:root:train 350 8.438489e-02 97.676282 99.817486
INFO:root:train 400 8.347153e-02 97.716646 99.820761
INFO:root:train 450 8.367366e-02 97.703021 99.823309
INFO:root:train 500 8.281110e-02 97.710828 99.825349
INFO:root:train 550 8.207468e-02 97.731397 99.827019
INFO:root:train 600 8.256891e-02 97.709547 99.823211
INFO:root:train 650 8.120358e-02 97.719854 99.831989
INFO:root:train 700 8.114271e-02 97.697486 99.832828
INFO:root:train 750 8.323138e-02 97.657290 99.816911
INFO:root:train 800 8.315024e-02 97.674782 99.808833
INFO:root:train 850 8.266617e-02 97.681037 99.812720
INFO:root:train 900 8.247615e-02 97.691801 99.809240
INFO:root:train 950 8.264047e-02 97.699790 99.806125
INFO:root:train 1000 8.233609e-02 97.703859 99.806444
INFO:root:train 1050 8.234569e-02 97.704567 99.805245
INFO:root:train 1100 8.260814e-02 97.695277 99.804155
INFO:root:train 1150 8.255311e-02 97.703084 99.803160
INFO:root:train 1200 8.237071e-02 97.707639 99.804850
INFO:root:train 1250 8.245767e-02 97.700590 99.803907
INFO:root:train 1300 8.290448e-02 97.697684 99.803036
INFO:root:train 1350 8.277014e-02 97.699621 99.801073
INFO:root:train 1400 8.334015e-02 97.688035 99.799251
INFO:root:train 1450 8.334762e-02 97.683710 99.798630
INFO:root:train 1500 8.339681e-02 97.692164 99.797010
INFO:root:train 1550 8.340516e-02 97.682946 99.798517
INFO:root:train 1600 8.318475e-02 97.676257 99.798954
INFO:root:train 1650 8.345060e-02 97.671866 99.801257
INFO:root:train 1700 8.375964e-02 97.649361 99.798832
INFO:root:train 1750 8.371072e-02 97.649557 99.797437
INFO:root:train 1800 8.378726e-02 97.640200 99.795253
INFO:root:train 1850 8.366566e-02 97.644010 99.795719
INFO:root:train 1900 8.354724e-02 97.647620 99.795338
INFO:root:train 1950 8.344321e-02 97.651845 99.795778
INFO:root:train 2000 8.357585e-02 97.648051 99.794634
INFO:root:train 2050 8.332726e-02 97.649775 99.796593
INFO:root:train 2100 8.318648e-02 97.652160 99.797715
INFO:root:train 2150 8.311705e-02 97.651528 99.797333
INFO:root:train 2200 8.313566e-02 97.649506 99.796257
INFO:root:train 2250 8.370865e-02 97.642714 99.791759
INFO:root:train 2300 8.347460e-02 97.649120 99.792210
INFO:root:train 2350 8.360556e-02 97.641296 99.791977
INFO:root:train 2400 8.361479e-02 97.639655 99.792404
INFO:root:train 2450 8.324876e-02 97.653381 99.794727
INFO:root:train 2500 8.342880e-02 97.643443 99.792583
INFO:root:train 2550 8.349396e-02 97.643694 99.793586
INFO:root:train 2600 8.328797e-02 97.644536 99.795752
INFO:root:train 2650 8.345526e-02 97.640041 99.792531
INFO:root:train 2700 8.335263e-02 97.639763 99.793479
INFO:root:train 2750 8.356423e-02 97.637223 99.794961
INFO:root:train 2800 8.341114e-02 97.640352 99.797505
INFO:root:train 2850 8.385475e-02 97.630217 99.795576
INFO:root:train 2900 8.368981e-02 97.627973 99.796406
INFO:root:train 2950 8.388630e-02 97.618922 99.794561
INFO:root:train 3000 8.419787e-02 97.609651 99.792777
INFO:root:train 3050 8.396948e-02 97.612463 99.793101
INFO:root:train 3100 8.403290e-02 97.610146 99.791902
INFO:root:train 3150 8.400383e-02 97.607902 99.794212
INFO:root:train 3200 8.393327e-02 97.611098 99.794010
INFO:root:train 3250 8.389325e-02 97.608428 99.794775
INFO:root:train 3300 8.393008e-02 97.607733 99.792676
INFO:root:train 3350 8.405754e-02 97.603794 99.792506
INFO:root:train 3400 8.424096e-02 97.603646 99.792800
INFO:root:train 3450 8.435040e-02 97.602597 99.791274
INFO:root:train 3500 8.423602e-02 97.604256 99.791577
INFO:root:train 3550 8.421181e-02 97.606308 99.790552
INFO:root:train 3600 8.434982e-02 97.602229 99.791291
INFO:root:train_acc 97.599036
INFO:root:valid 000 2.886201e-01 89.062500 98.437500
INFO:root:valid 050 2.272602e-01 94.822304 98.958333
INFO:root:valid 100 2.055078e-01 95.204208 98.963490
INFO:root:valid 150 1.992811e-01 95.374586 98.975579
INFO:root:valid 200 1.956037e-01 95.320274 99.051617
INFO:root:valid 250 2.009640e-01 95.293825 98.966633
INFO:root:valid 300 2.012871e-01 95.281354 98.972176
INFO:root:valid 350 2.017891e-01 95.232372 98.993946
INFO:root:valid 400 2.004149e-01 95.215087 99.006390
INFO:root:valid 450 2.031042e-01 95.149667 99.002217
INFO:root:valid 500 2.042859e-01 95.087949 99.014471
INFO:root:valid 550 2.056174e-01 95.085640 99.007486
INFO:root:valid 600 2.088535e-01 95.039517 99.006864
INFO:root:test_acc 95.037227
INFO:root:epoch 15 lr 2.720862e-04
INFO:root:train 000 7.770529e-02 98.437500 100.000000
INFO:root:train 050 8.541971e-02 97.763480 99.785539
INFO:root:train 100 9.039235e-02 97.462871 99.721535
INFO:root:train 150 8.473323e-02 97.506209 99.772351
INFO:root:train 200 8.152330e-02 97.660137 99.790112
INFO:root:train 250 8.295960e-02 97.702938 99.788347
INFO:root:train 300 8.131108e-02 97.736711 99.787168
INFO:root:train 350 8.074117e-02 97.743056 99.790776
INFO:root:train 400 7.918954e-02 97.798473 99.809071
INFO:root:train 450 7.835456e-02 97.793099 99.812916
INFO:root:train 500 7.732184e-02 97.798154 99.806637
INFO:root:train 550 7.832247e-02 97.788113 99.792990
INFO:root:train 600 7.960920e-02 97.751144 99.784214
INFO:root:train 650 7.990769e-02 97.739055 99.793587
INFO:root:train 700 7.983548e-02 97.724233 99.799394
INFO:root:train 750 8.038670e-02 97.703063 99.789864
INFO:root:train 800 7.970265e-02 97.745006 99.793227
INFO:root:train 850 7.972688e-02 97.728775 99.790687
INFO:root:train 900 8.005766e-02 97.714345 99.790164
INFO:root:train 950 8.085517e-02 97.691575 99.788052
INFO:root:train 1000 8.145445e-02 97.674201 99.787712
INFO:root:train 1050 8.154140e-02 97.667400 99.784431
INFO:root:train 1100 8.121340e-02 97.673990 99.788545
INFO:root:train 1150 8.171500e-02 97.673219 99.789585
INFO:root:train 1200 8.142491e-02 97.669910 99.790539
INFO:root:train 1250 8.169582e-02 97.665618 99.790168
INFO:root:train 1300 8.156582e-02 97.671262 99.789825
INFO:root:train 1350 8.120414e-02 97.678803 99.794134
INFO:root:train 1400 8.059167e-02 97.694727 99.798135
INFO:root:train 1450 8.029772e-02 97.699862 99.800784
INFO:root:train 1500 8.102889e-02 97.690082 99.793887
INFO:root:train 1550 8.082164e-02 97.691006 99.795495
INFO:root:train 1600 8.049912e-02 97.688944 99.799930
INFO:root:train 1650 8.114382e-02 97.693633 99.793686
INFO:root:train 1700 8.132509e-02 97.678755 99.794239
INFO:root:train 1750 8.151130e-02 97.673651 99.792083
INFO:root:train 1800 8.162484e-02 97.664492 99.790047
INFO:root:train 1850 8.147679e-02 97.662581 99.792342
INFO:root:train 1900 8.126492e-02 97.670634 99.792050
INFO:root:train 1950 8.109982e-02 97.680677 99.788570
INFO:root:train 2000 8.091997e-02 97.683190 99.789949
INFO:root:train 2050 8.158709e-02 97.671106 99.788213
INFO:root:train 2100 8.139194e-02 97.667034 99.788047
INFO:root:train 2150 8.109262e-02 97.672594 99.790069
INFO:root:train 2200 8.097010e-02 97.672933 99.791288
INFO:root:train 2250 8.124536e-02 97.665621 99.789677
INFO:root:train 2300 8.144368e-02 97.664059 99.790852
INFO:root:train 2350 8.133080e-02 97.665887 99.792641
INFO:root:train 2400 8.152154e-02 97.662432 99.789150
INFO:root:train 2450 8.161504e-02 97.661669 99.787714
INFO:root:train 2500 8.141331e-02 97.668433 99.788834
INFO:root:train 2550 8.175495e-02 97.660231 99.786236
INFO:root:train 2600 8.165649e-02 97.661356 99.786741
INFO:root:train 2650 8.171166e-02 97.663618 99.786637
INFO:root:train 2700 8.166440e-02 97.665795 99.784223
INFO:root:train 2750 8.177714e-02 97.665054 99.784737
INFO:root:train 2800 8.154507e-02 97.668243 99.785233
INFO:root:train 2850 8.176239e-02 97.663649 99.785711
INFO:root:train 2900 8.154127e-02 97.668907 99.786173
INFO:root:train 2950 8.159635e-02 97.664986 99.785560
INFO:root:train 3000 8.169899e-02 97.663800 99.785488
INFO:root:train 3050 8.176004e-02 97.667261 99.785931
INFO:root:train 3100 8.186986e-02 97.665068 99.785351
INFO:root:train 3150 8.188135e-02 97.666415 99.785286
INFO:root:train 3200 8.181641e-02 97.667721 99.785711
INFO:root:train 3250 8.188601e-02 97.665622 99.784682
INFO:root:train 3300 8.179553e-02 97.663587 99.785103
INFO:root:train 3350 8.185511e-02 97.664410 99.784579
INFO:root:train 3400 8.185511e-02 97.667506 99.785909
INFO:root:train 3450 8.181025e-02 97.664173 99.786294
INFO:root:train 3500 8.191401e-02 97.661829 99.785329
INFO:root:train 3550 8.198710e-02 97.657790 99.785712
INFO:root:train 3600 8.207310e-02 97.654297 99.786084
INFO:root:train_acc 97.654552
INFO:root:valid 000 2.604882e-01 95.312500 98.437500
INFO:root:valid 050 2.182198e-01 95.067402 98.897059
INFO:root:valid 100 1.973891e-01 95.343441 98.932550
INFO:root:valid 150 1.919420e-01 95.478063 98.944536
INFO:root:valid 200 1.880999e-01 95.475746 99.036070
INFO:root:valid 250 1.935150e-01 95.449452 98.972859
INFO:root:valid 300 1.937818e-01 95.463040 98.977367
INFO:root:valid 350 1.944340e-01 95.401531 99.020655
INFO:root:valid 400 1.935871e-01 95.386534 99.037562
INFO:root:valid 450 1.967981e-01 95.302106 99.016075
INFO:root:valid 500 1.977413e-01 95.256362 99.023827
INFO:root:valid 550 1.994332e-01 95.241606 99.013158
INFO:root:valid 600 2.022633e-01 95.195507 99.006864
INFO:root:test_acc 95.190287
INFO:root:epoch 16 lr 1.552789e-04
INFO:root:train 000 6.618933e-02 98.437500 100.000000
INFO:root:train 050 6.811007e-02 97.886029 99.877451
INFO:root:train 100 7.250212e-02 97.942450 99.845297
INFO:root:train 150 7.714809e-02 97.858030 99.813742
INFO:root:train 200 7.688817e-02 97.916667 99.813433
INFO:root:train 250 7.847801e-02 97.839890 99.813247
INFO:root:train 300 7.757993e-02 97.856105 99.807932
INFO:root:train 350 7.839488e-02 97.854345 99.808583
INFO:root:train 400 7.883562e-02 97.829645 99.805175
INFO:root:train 450 7.601780e-02 97.886641 99.823309
INFO:root:train 500 7.665687e-02 97.876123 99.825349
INFO:root:train 550 7.701826e-02 97.870349 99.829855
INFO:root:train 600 7.617114e-02 97.888935 99.831011
INFO:root:train 650 7.597532e-02 97.909466 99.829589
INFO:root:train 700 7.717710e-02 97.878031 99.826141
INFO:root:train 750 7.709508e-02 97.875749 99.825233
INFO:root:train 800 7.750992e-02 97.858146 99.824438
INFO:root:train 850 7.838869e-02 97.831595 99.816392
INFO:root:train 900 7.822977e-02 97.851346 99.812708
INFO:root:train 950 7.831866e-02 97.842731 99.807768
INFO:root:train 1000 7.810218e-02 97.847465 99.809565
INFO:root:train 1050 7.805944e-02 97.842828 99.811192
INFO:root:train 1100 7.797652e-02 97.844289 99.815509
INFO:root:train 1150 7.804369e-02 97.819831 99.816735
INFO:root:train 1200 7.776729e-02 97.813020 99.816559
INFO:root:train 1250 7.815693e-02 97.811751 99.816397
INFO:root:train 1300 7.869004e-02 97.790161 99.816247
INFO:root:train 1350 7.887955e-02 97.793301 99.812639
INFO:root:train 1400 7.861690e-02 97.805139 99.813749
INFO:root:train 1450 7.865303e-02 97.809700 99.813706
INFO:root:train 1500 7.865262e-02 97.808752 99.814707
INFO:root:train 1550 7.846467e-02 97.815925 99.814636
INFO:root:train 1600 7.825370e-02 97.808987 99.815545
INFO:root:train 1650 7.827973e-02 97.810039 99.811667
INFO:root:train 1700 7.843506e-02 97.800926 99.812610
INFO:root:train 1750 7.861810e-02 97.796795 99.813499
INFO:root:train 1800 7.908407e-02 97.793760 99.808266
INFO:root:train 1850 7.919974e-02 97.784137 99.808381
INFO:root:train 1900 7.900894e-02 97.785705 99.810133
INFO:root:train 1950 7.858029e-02 97.791998 99.810994
INFO:root:train 2000 7.893878e-02 97.790948 99.809470
INFO:root:train 2050 7.905611e-02 97.786141 99.808782
INFO:root:train 2100 7.901105e-02 97.777844 99.806640
INFO:root:train 2150 7.895535e-02 97.775017 99.807502
INFO:root:train 2200 7.921945e-02 97.773029 99.806196
INFO:root:train 2250 7.940664e-02 97.775294 99.806336
INFO:root:train 2300 7.943121e-02 97.774745 99.803754
INFO:root:train 2350 7.915973e-02 97.778206 99.803940
INFO:root:train 2400 7.895019e-02 97.781523 99.805420
INFO:root:train 2450 7.898322e-02 97.778330 99.804927
INFO:root:train 2500 7.883327e-02 97.775265 99.806327
INFO:root:train 2550 7.876765e-02 97.779057 99.806448
INFO:root:train 2600 7.895086e-02 97.777297 99.802360
INFO:root:train 2650 7.909462e-02 97.764994 99.802551
INFO:root:train 2700 7.924765e-02 97.765295 99.803314
INFO:root:train 2750 7.926973e-02 97.759338 99.804617
INFO:root:train 2800 7.928877e-02 97.755266 99.804757
INFO:root:train 2850 7.929362e-02 97.753529 99.805441
INFO:root:train 2900 7.923700e-02 97.758316 99.806101
INFO:root:train 2950 7.930645e-02 97.753939 99.806210
INFO:root:train 3000 7.951222e-02 97.745543 99.806835
INFO:root:train 3050 7.947048e-02 97.746640 99.804367
INFO:root:train 3100 7.935844e-02 97.746695 99.805002
INFO:root:train 3150 7.939987e-02 97.745755 99.803634
INFO:root:train 3200 7.944571e-02 97.743381 99.802796
INFO:root:train 3250 7.943344e-02 97.741560 99.802465
INFO:root:train 3300 7.943268e-02 97.736008 99.803090
INFO:root:train 3350 7.961458e-02 97.728756 99.803230
INFO:root:train 3400 7.948442e-02 97.729988 99.805204
INFO:root:train 3450 7.968191e-02 97.721675 99.804405
INFO:root:train 3500 7.983997e-02 97.716277 99.804520
INFO:root:train 3550 7.999819e-02 97.711472 99.804192
INFO:root:train 3600 7.973824e-02 97.720251 99.804742
INFO:root:train_acc 97.720827
INFO:root:valid 000 2.851270e-01 92.187500 98.437500
INFO:root:valid 050 2.189021e-01 95.159314 98.927696
INFO:root:valid 100 1.981927e-01 95.451733 98.963490
INFO:root:valid 150 1.923520e-01 95.509106 98.975579
INFO:root:valid 200 1.886215e-01 95.514614 99.059391
INFO:root:valid 250 1.942978e-01 95.486803 99.003984
INFO:root:valid 300 1.943661e-01 95.509759 98.992940
INFO:root:valid 350 1.948989e-01 95.454950 99.025107
INFO:root:valid 400 1.939466e-01 95.456671 99.037562
INFO:root:valid 450 1.971802e-01 95.371397 99.016075
INFO:root:valid 500 1.983264e-01 95.349925 99.023827
INFO:root:valid 550 2.000781e-01 95.352201 99.015994
INFO:root:valid 600 2.033152e-01 95.286502 99.017263
INFO:root:test_acc 95.281085
INFO:root:epoch 17 lr 6.972792e-05
INFO:root:train 000 1.428837e-02 100.000000 100.000000
INFO:root:train 050 7.964466e-02 97.640931 99.816176
INFO:root:train 100 8.695370e-02 97.370050 99.752475
INFO:root:train 150 8.097386e-02 97.682119 99.782699
INFO:root:train 200 7.878642e-02 97.706779 99.821206
INFO:root:train 250 8.013446e-02 97.709163 99.807022
INFO:root:train 300 8.003470e-02 97.741902 99.818314
INFO:root:train 350 7.866496e-02 97.765313 99.835292
INFO:root:train 400 7.899729e-02 97.747818 99.832450
INFO:root:train 450 7.748189e-02 97.779241 99.830238
INFO:root:train 500 7.841025e-02 97.773204 99.815993
INFO:root:train 550 7.737492e-02 97.819306 99.815676
INFO:root:train 600 7.712258e-02 97.826539 99.807612
INFO:root:train 650 7.761670e-02 97.811060 99.793587
INFO:root:train 700 7.712449e-02 97.806705 99.797165
INFO:root:train 750 7.728058e-02 97.807091 99.806508
INFO:root:train 800 7.784641e-02 97.780119 99.812734
INFO:root:train 850 7.845053e-02 97.769169 99.816392
INFO:root:train 900 7.846641e-02 97.749029 99.821379
INFO:root:train 950 7.885842e-02 97.731007 99.825841
INFO:root:train 1000 7.876251e-02 97.738199 99.828297
INFO:root:train 1050 7.910190e-02 97.746194 99.827545
INFO:root:train 1100 7.901396e-02 97.750624 99.822604
INFO:root:train 1150 7.908225e-02 97.756027 99.820808
INFO:root:train 1200 7.897104e-02 97.750572 99.820462
INFO:root:train 1250 7.843842e-02 97.768036 99.822642
INFO:root:train 1300 7.827977e-02 97.768543 99.822252
INFO:root:train 1350 7.839477e-02 97.770170 99.818422
INFO:root:train 1400 7.826434e-02 97.768335 99.821556
INFO:root:train 1450 7.836582e-02 97.773088 99.820167
INFO:root:train 1500 7.841872e-02 97.773359 99.819912
INFO:root:train 1550 7.849961e-02 97.769584 99.818665
INFO:root:train 1600 7.903879e-02 97.756285 99.813593
INFO:root:train 1650 7.912259e-02 97.745684 99.815453
INFO:root:train 1700 7.984329e-02 97.738463 99.807099
INFO:root:train 1750 7.992512e-02 97.729869 99.806361
INFO:root:train 1800 8.043999e-02 97.726090 99.803061
INFO:root:train 1850 8.025659e-02 97.733489 99.801627
INFO:root:train 1900 8.040468e-02 97.731457 99.801913
INFO:root:train 1950 8.029750e-02 97.739140 99.802986
INFO:root:train 2000 8.032709e-02 97.740192 99.803223
INFO:root:train 2050 8.031391e-02 97.744241 99.804211
INFO:root:train 2100 8.015902e-02 97.747352 99.803665
INFO:root:train 2150 8.020296e-02 97.740876 99.803144
INFO:root:train 2200 8.024555e-02 97.741084 99.803356
INFO:root:train 2250 7.999960e-02 97.741282 99.803560
INFO:root:train 2300 7.976771e-02 97.745545 99.805112
INFO:root:train 2350 8.004223e-02 97.739659 99.801946
INFO:root:train 2400 8.031508e-02 97.732716 99.802166
INFO:root:train 2450 8.031552e-02 97.733068 99.803014
INFO:root:train 2500 8.042401e-02 97.730908 99.800705
INFO:root:train 2550 8.028202e-02 97.733732 99.800936
INFO:root:train 2600 8.038666e-02 97.728638 99.799957
INFO:root:train 2650 8.046859e-02 97.729041 99.797836
INFO:root:train 2700 8.020514e-02 97.730007 99.799843
INFO:root:train 2750 7.986672e-02 97.737186 99.802913
INFO:root:train 2800 8.024097e-02 97.730163 99.801968
INFO:root:train 2850 7.980184e-02 97.741472 99.803797
INFO:root:train 2900 7.991409e-02 97.744312 99.802331
INFO:root:train 2950 7.978672e-02 97.743350 99.803562
INFO:root:train 3000 7.984623e-02 97.740337 99.803711
INFO:root:train 3050 7.991562e-02 97.744592 99.803855
INFO:root:train 3100 7.992344e-02 97.743168 99.804499
INFO:root:train 3150 7.988053e-02 97.745755 99.806113
INFO:root:train 3200 7.984962e-02 97.745333 99.807677
INFO:root:train 3250 7.981322e-02 97.748289 99.807271
INFO:root:train 3300 7.996846e-02 97.741215 99.807823
INFO:root:train 3350 7.989131e-02 97.744144 99.806961
INFO:root:train 3400 7.991915e-02 97.748824 99.805204
INFO:root:train 3450 7.977017e-02 97.748841 99.807121
INFO:root:train 3500 7.974533e-02 97.746180 99.807644
INFO:root:train 3550 7.979809e-02 97.748874 99.805953
INFO:root:train 3600 7.966267e-02 97.752794 99.805176
INFO:root:train_acc 97.750522
INFO:root:valid 000 2.790222e-01 93.750000 98.437500
INFO:root:valid 050 2.190211e-01 95.098039 98.927696
INFO:root:valid 100 1.988441e-01 95.451733 98.886139
INFO:root:valid 150 1.925808e-01 95.529801 98.923841
INFO:root:valid 200 1.884706e-01 95.514614 99.028296
INFO:root:valid 250 1.944822e-01 95.511703 98.954183
INFO:root:valid 300 1.940108e-01 95.525332 98.956603
INFO:root:valid 350 1.949667e-01 95.454950 98.980591
INFO:root:valid 400 1.942519e-01 95.468360 98.990804
INFO:root:valid 450 1.972381e-01 95.378326 98.988359
INFO:root:valid 500 1.987398e-01 95.343688 98.989521
INFO:root:valid 550 2.007029e-01 95.346529 98.984800
INFO:root:valid 600 2.039281e-01 95.304700 98.983465
INFO:root:test_acc 95.301839
INFO:root:epoch 18 lr 1.753995e-05
INFO:root:train 000 2.144118e-02 100.000000 100.000000
INFO:root:train 050 6.970418e-02 98.039216 99.785539
INFO:root:train 100 7.029114e-02 98.004332 99.829827
INFO:root:train 150 7.485591e-02 97.930464 99.803394
INFO:root:train 200 7.372859e-02 97.986629 99.828980
INFO:root:train 250 7.394230e-02 98.039094 99.800797
INFO:root:train 300 7.628043e-02 97.954734 99.813123
INFO:root:train 350 7.622275e-02 97.898860 99.817486
INFO:root:train 400 7.553020e-02 97.876403 99.828554
INFO:root:train 450 7.663962e-02 97.872783 99.819845
INFO:root:train 500 7.595353e-02 97.891717 99.812874
INFO:root:train 550 7.716008e-02 97.850499 99.804333
INFO:root:train 600 7.620512e-02 97.888935 99.810212
INFO:root:train 650 7.698152e-02 97.856663 99.803187
INFO:root:train 700 7.633983e-02 97.862429 99.806081
INFO:root:train 750 7.629770e-02 97.838299 99.814830
INFO:root:train 800 7.663484e-02 97.846442 99.812734
INFO:root:train 850 7.631650e-02 97.838939 99.814556
INFO:root:train 900 7.656859e-02 97.837472 99.817911
INFO:root:train 950 7.646651e-02 97.829587 99.812697
INFO:root:train 1000 7.627506e-02 97.833417 99.814248
INFO:root:train 1050 7.602556e-02 97.841342 99.809705
INFO:root:train 1100 7.689835e-02 97.814487 99.802736
INFO:root:train 1150 7.716476e-02 97.814401 99.801803
INFO:root:train 1200 7.776396e-02 97.794806 99.802248
INFO:root:train 1250 7.815219e-02 97.783024 99.802658
INFO:root:train 1300 7.825085e-02 97.776950 99.803036
INFO:root:train 1350 7.841948e-02 97.775953 99.799917
INFO:root:train 1400 7.811830e-02 97.775027 99.800366
INFO:root:train 1450 7.787009e-02 97.789240 99.798630
INFO:root:train 1500 7.816297e-02 97.784810 99.799092
INFO:root:train 1550 7.836354e-02 97.770592 99.798517
INFO:root:train 1600 7.815048e-02 97.780684 99.800906
INFO:root:train 1650 7.775569e-02 97.787326 99.802203
INFO:root:train 1700 7.808900e-02 97.780717 99.803424
INFO:root:train 1750 7.812860e-02 97.786979 99.802791
INFO:root:train 1800 7.783692e-02 97.782482 99.805664
INFO:root:train 1850 7.832814e-02 97.770631 99.801627
INFO:root:train 1900 7.844708e-02 97.763513 99.803557
INFO:root:train 1950 7.871304e-02 97.761565 99.802185
INFO:root:train 2000 7.880319e-02 97.761276 99.804004
INFO:root:train 2050 7.919256e-02 97.757192 99.803450
INFO:root:train 2100 7.939124e-02 97.749584 99.803665
INFO:root:train 2150 7.929839e-02 97.752499 99.804597
INFO:root:train 2200 7.928899e-02 97.755992 99.801227
INFO:root:train 2250 7.920937e-02 97.760023 99.801477
INFO:root:train 2300 7.904620e-02 97.760485 99.802396
INFO:root:train 2350 7.897847e-02 97.761591 99.803275
INFO:root:train 2400 7.959144e-02 97.747032 99.799563
INFO:root:train 2450 7.942088e-02 97.753468 99.800464
INFO:root:train 2500 7.946479e-02 97.754023 99.801329
INFO:root:train 2550 7.915475e-02 97.756395 99.803998
INFO:root:train 2600 7.910957e-02 97.758675 99.805363
INFO:root:train 2650 7.888935e-02 97.766763 99.804909
INFO:root:train 2700 7.922544e-02 97.753725 99.803314
INFO:root:train 2750 7.937287e-02 97.749682 99.803481
INFO:root:train 2800 7.929151e-02 97.750803 99.805873
INFO:root:train 2850 7.947785e-02 97.745857 99.805989
INFO:root:train 2900 7.915519e-02 97.755084 99.807179
INFO:root:train 2950 7.916858e-02 97.754469 99.806739
INFO:root:train 3000 7.889870e-02 97.765328 99.808397
INFO:root:train 3050 7.901744e-02 97.761492 99.809489
INFO:root:train 3100 7.927355e-02 97.759795 99.808530
INFO:root:train 3150 7.929653e-02 97.760136 99.809584
INFO:root:train 3200 7.922106e-02 97.759001 99.812070
INFO:root:train 3250 7.918847e-02 97.762708 99.811596
INFO:root:train 3300 7.912570e-02 97.762515 99.810663
INFO:root:train 3350 7.935436e-02 97.755334 99.809758
INFO:root:train 3400 7.924633e-02 97.759391 99.809339
INFO:root:train 3450 7.922116e-02 97.753369 99.809385
INFO:root:train 3500 7.908827e-02 97.756891 99.809876
INFO:root:train 3550 7.918128e-02 97.757234 99.809913
INFO:root:train 3600 7.894685e-02 97.763208 99.811250
INFO:root:train_acc 97.760420
INFO:root:valid 000 2.827927e-01 93.750000 98.437500
INFO:root:valid 050 2.164586e-01 95.067402 98.927696
INFO:root:valid 100 1.965729e-01 95.498144 98.901609
INFO:root:valid 150 1.903617e-01 95.602235 98.934189
INFO:root:valid 200 1.863892e-01 95.592351 99.020522
INFO:root:valid 250 1.920309e-01 95.580179 98.941733
INFO:root:valid 300 1.917699e-01 95.566860 98.941030
INFO:root:valid 350 1.926689e-01 95.495014 98.962785
INFO:root:valid 400 1.918239e-01 95.487843 98.983011
INFO:root:valid 450 1.950563e-01 95.409507 98.984895
INFO:root:valid 500 1.964803e-01 95.362400 98.989521
INFO:root:valid 550 1.981667e-01 95.374887 98.987636
INFO:root:valid 600 2.012796e-01 95.320300 98.988665
INFO:root:test_acc 95.314810
INFO:root:epoch 19 lr 0.000000e+00
INFO:root:train 000 1.336834e-01 96.875000 100.000000
INFO:root:train 050 8.761285e-02 97.763480 99.724265
INFO:root:train 100 7.781707e-02 97.973391 99.798886
INFO:root:train 150 7.894914e-02 97.858030 99.813742
INFO:root:train 200 7.567816e-02 98.002177 99.821206
INFO:root:train 250 7.666890e-02 97.821215 99.825697
INFO:root:train 300 7.896533e-02 97.767857 99.807932
INFO:root:train 350 7.674407e-02 97.818732 99.826389
INFO:root:train 400 7.808433e-02 97.810162 99.809071
INFO:root:train 450 7.795279e-02 97.817350 99.812916
INFO:root:train 500 7.851268e-02 97.841816 99.809755
INFO:root:train 550 7.910153e-02 97.841992 99.815676
INFO:root:train 600 7.874639e-02 97.810940 99.818012
INFO:root:train 650 7.935418e-02 97.803859 99.807988
INFO:root:train 700 8.007589e-02 97.788873 99.803852
INFO:root:train 750 7.994283e-02 97.788366 99.798186
INFO:root:train 800 7.946362e-02 97.789872 99.804931
INFO:root:train 850 7.930849e-02 97.782021 99.803540
INFO:root:train 900 7.966945e-02 97.780244 99.798835
INFO:root:train 950 7.954086e-02 97.762224 99.802839
INFO:root:train 1000 7.977887e-02 97.744443 99.797078
INFO:root:train 1050 7.987903e-02 97.734301 99.796325
INFO:root:train 1100 7.933223e-02 97.739271 99.797059
INFO:root:train 1150 7.950147e-02 97.720732 99.799088
INFO:root:train 1200 7.959916e-02 97.714144 99.795743
INFO:root:train 1250 7.980554e-02 97.700590 99.797662
INFO:root:train 1300 8.042113e-02 97.679669 99.797031
INFO:root:train 1350 8.100536e-02 97.661454 99.796447
INFO:root:train 1400 8.073503e-02 97.677998 99.794789
INFO:root:train 1450 8.070680e-02 97.672941 99.796477
INFO:root:train 1500 8.078699e-02 97.665098 99.802215
INFO:root:train 1550 8.082253e-02 97.661791 99.805569
INFO:root:train 1600 8.068568e-02 97.663570 99.806761
INFO:root:train 1650 8.062633e-02 97.665241 99.805042
INFO:root:train 1700 8.033759e-02 97.672325 99.809854
INFO:root:train 1750 8.041354e-02 97.668297 99.809930
INFO:root:train 1800 8.089218e-02 97.661889 99.805664
INFO:root:train 1850 8.108798e-02 97.648231 99.806692
INFO:root:train 1900 8.100874e-02 97.653373 99.806023
INFO:root:train 1950 8.139526e-02 97.648642 99.803787
INFO:root:train 2000 8.123755e-02 97.650394 99.807128
INFO:root:train 2050 8.146135e-02 97.639109 99.808782
INFO:root:train 2100 8.121969e-02 97.643979 99.811102
INFO:root:train 2150 8.111856e-02 97.650076 99.808955
INFO:root:train 2200 8.111006e-02 97.653765 99.809746
INFO:root:train 2250 8.082439e-02 97.657291 99.812583
INFO:root:train 2300 8.078104e-02 97.655910 99.812581
INFO:root:train 2350 8.114881e-02 97.645284 99.809921
INFO:root:train 2400 8.109425e-02 97.642909 99.809975
INFO:root:train 2450 8.103873e-02 97.648919 99.810664
INFO:root:train 2500 8.085849e-02 97.652814 99.810076
INFO:root:train 2550 8.123431e-02 97.645531 99.807061
INFO:root:train 2600 8.119431e-02 97.648140 99.806565
INFO:root:train 2650 8.123328e-02 97.655955 99.806087
INFO:root:train 2700 8.112357e-02 97.655961 99.806785
INFO:root:train 2750 8.137451e-02 97.653694 99.805184
INFO:root:train 2800 8.137442e-02 97.652624 99.805873
INFO:root:train 2850 8.132724e-02 97.647207 99.805441
INFO:root:train 2900 8.160011e-02 97.641438 99.802870
INFO:root:train 2950 8.179734e-02 97.636924 99.801974
INFO:root:train 3000 8.217195e-02 97.630477 99.800587
INFO:root:train 3050 8.213614e-02 97.630388 99.800270
INFO:root:train 3100 8.199164e-02 97.632820 99.802483
INFO:root:train 3150 8.219494e-02 97.626745 99.804130
INFO:root:train 3200 8.211269e-02 97.627206 99.804260
INFO:root:train 3250 8.199997e-02 97.631979 99.804387
INFO:root:train 3300 8.184356e-02 97.640393 99.804983
INFO:root:train 3350 8.178693e-02 97.642028 99.806028
INFO:root:train 3400 8.179235e-02 97.643156 99.804286
INFO:root:train 3450 8.168167e-02 97.644252 99.805763
INFO:root:train 3500 8.173830e-02 97.646655 99.806305
INFO:root:train 3550 8.174138e-02 97.651630 99.806393
INFO:root:train 3600 8.166727e-02 97.652562 99.806477
INFO:root:train_acc 97.651970
INFO:root:valid 000 2.768960e-01 92.187500 98.437500
INFO:root:valid 050 2.151958e-01 95.128676 98.897059
INFO:root:valid 100 1.953253e-01 95.467203 98.901609
INFO:root:valid 150 1.893582e-01 95.622930 98.923841
INFO:root:valid 200 1.861408e-01 95.545709 99.004975
INFO:root:valid 250 1.918809e-01 95.517928 98.941733
INFO:root:valid 300 1.920028e-01 95.525332 98.951412
INFO:root:valid 350 1.929902e-01 95.446047 98.980591
INFO:root:valid 400 1.919344e-01 95.456671 99.002494
INFO:root:valid 450 1.950336e-01 95.378326 99.002217
INFO:root:valid 500 1.964272e-01 95.334331 99.001996
INFO:root:valid 550 1.981751e-01 95.332350 98.996143
INFO:root:valid 600 2.012056e-01 95.289101 98.996464
INFO:root:test_acc 95.288868
INFO:solver.bo_hb:Evaluate: [3.86173484e-01 5.08479855e+00 7.77975413e-02 1.10296446e+00
 4.59085032e+00 1.31274305e+00 5.15586931e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.15446939352913233, 'grad_clip_value': 8, 'initial_lr': 2.448993918369908e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adad', 'weight_decay': 1.0023771866015065e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 2.215136e-06
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486538e+00 9.436275 49.816176
INFO:root:train 100 2.478550e+00 10.009282 49.675124
INFO:root:train 150 2.479980e+00 10.016556 49.596440
INFO:root:train 200 2.482485e+00 9.856965 49.175995
INFO:root:train 250 2.482384e+00 9.779631 49.234313
INFO:root:train 300 2.482098e+00 9.805855 49.117525
INFO:root:train 350 2.482968e+00 9.793447 49.100783
INFO:root:train 400 2.482956e+00 9.776340 49.185630
INFO:root:train 450 2.480947e+00 9.821924 49.341741
INFO:root:train 500 2.480415e+00 9.808508 49.401198
INFO:root:train 550 2.479573e+00 9.800363 49.526429
INFO:root:train 600 2.481462e+00 9.746776 49.435836
INFO:root:train 650 2.483055e+00 9.660618 49.313556
INFO:root:train 700 2.483552e+00 9.617956 49.262215
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.384380e+00 14.062500 53.125000
INFO:root:valid 050 2.482524e+00 9.191176 49.816176
INFO:root:valid 100 2.477941e+00 9.576114 49.922649
INFO:root:valid 150 2.479729e+00 9.147351 49.772351
INFO:root:valid_acc 9.216667
INFO:root:epoch 1 lr 1.602887e-06
INFO:root:train 000 2.481083e+00 10.937500 51.562500
INFO:root:train 050 2.506043e+00 9.803922 48.314951
INFO:root:train 100 2.501403e+00 9.220297 49.025371
INFO:root:train 150 2.501287e+00 9.281871 48.985927
INFO:root:train 200 2.501688e+00 9.118470 49.051617
INFO:root:train 250 2.498159e+00 9.082420 49.433516
INFO:root:train 300 2.498277e+00 9.214078 49.340739
INFO:root:train 350 2.497670e+00 9.250356 49.296652
INFO:root:train 400 2.495119e+00 9.332138 49.392145
INFO:root:train 450 2.495568e+00 9.243348 49.386779
INFO:root:train 500 2.497286e+00 9.234656 49.242141
INFO:root:train 550 2.495515e+00 9.366493 49.370463
INFO:root:train 600 2.494170e+00 9.458195 49.472234
INFO:root:train 650 2.493662e+00 9.509409 49.536770
INFO:root:train 700 2.493170e+00 9.506509 49.578727
INFO:root:train_acc 9.514583
INFO:root:valid 000 2.554349e+00 6.250000 45.312500
INFO:root:valid 050 2.491351e+00 9.160539 49.877451
INFO:root:valid 100 2.485679e+00 9.498762 49.845297
INFO:root:valid 150 2.486884e+00 9.364652 49.772351
INFO:root:valid_acc 9.658333
INFO:root:epoch 2 lr 8.461066e-07
INFO:root:train 000 2.582036e+00 6.250000 45.312500
INFO:root:train 050 2.508925e+00 9.344363 48.927696
INFO:root:train 100 2.510492e+00 9.498762 48.746906
INFO:root:train 150 2.501493e+00 9.840646 49.327401
INFO:root:train 200 2.500160e+00 9.787002 49.518035
INFO:root:train 250 2.501428e+00 9.742281 49.377490
INFO:root:train 300 2.503791e+00 9.639743 49.340739
INFO:root:train 350 2.503095e+00 9.606481 49.590456
INFO:root:train 400 2.503726e+00 9.499688 49.664900
INFO:root:train 450 2.503869e+00 9.478936 49.715909
INFO:root:train 500 2.503388e+00 9.515344 49.750499
INFO:root:train 550 2.502916e+00 9.633054 49.741946
INFO:root:train 600 2.502429e+00 9.668781 49.820611
INFO:root:train 650 2.502746e+00 9.691820 49.829589
INFO:root:train 700 2.502644e+00 9.711573 49.866262
INFO:root:train_acc 9.668750
INFO:root:valid 000 2.544911e+00 9.375000 48.437500
INFO:root:valid 050 2.486453e+00 9.895833 50.735294
INFO:root:valid 100 2.491060e+00 9.684406 50.479579
INFO:root:valid 150 2.493118e+00 9.706126 50.320778
INFO:root:valid_acc 9.725000
INFO:root:epoch 3 lr 2.338581e-07
INFO:root:train 000 2.458576e+00 10.937500 43.750000
INFO:root:train 050 2.494112e+00 10.232843 50.337010
INFO:root:train 100 2.510945e+00 9.653465 50.046411
INFO:root:train 150 2.509952e+00 9.737169 50.165563
INFO:root:train 200 2.510539e+00 9.763682 50.178794
INFO:root:train 250 2.508025e+00 9.829432 50.392181
INFO:root:train 300 2.507363e+00 9.774709 50.384136
INFO:root:train 350 2.509166e+00 9.771189 50.209224
INFO:root:train 400 2.510004e+00 9.780237 50.120792
INFO:root:train 450 2.510036e+00 9.738775 50.069290
INFO:root:train 500 2.510916e+00 9.724301 50.046781
INFO:root:train 550 2.511438e+00 9.737976 50.022686
INFO:root:train 600 2.511787e+00 9.762375 49.961002
INFO:root:train 650 2.512309e+00 9.723022 49.884793
INFO:root:train 700 2.511994e+00 9.753923 49.879636
INFO:root:train_acc 9.718750
INFO:root:valid 000 2.541083e+00 12.500000 46.875000
INFO:root:valid 050 2.515707e+00 8.946078 49.785539
INFO:root:valid 100 2.494934e+00 10.133045 50.495050
INFO:root:valid 150 2.499676e+00 10.006209 50.745033
INFO:root:valid_acc 9.725000
INFO:root:epoch 4 lr 0.000000e+00
INFO:root:train 000 2.515491e+00 12.500000 43.750000
INFO:root:train 050 2.528526e+00 10.294118 50.153186
INFO:root:train 100 2.529657e+00 9.761757 49.814356
INFO:root:train 150 2.529111e+00 9.644040 49.937914
INFO:root:train 200 2.526075e+00 9.701493 50.077736
INFO:root:train 250 2.521325e+00 9.829432 50.305030
INFO:root:train 300 2.519899e+00 9.883721 50.306271
INFO:root:train 350 2.520702e+00 9.926994 50.200321
INFO:root:train 400 2.522218e+00 9.924408 49.976621
INFO:root:train 450 2.521962e+00 9.884285 50.045039
INFO:root:train 500 2.523873e+00 9.861527 49.862774
INFO:root:train 550 2.524748e+00 9.868421 49.912092
INFO:root:train 600 2.525217e+00 9.887167 49.896007
INFO:root:train 650 2.524183e+00 9.869432 49.959197
INFO:root:train 700 2.523646e+00 9.878745 49.988855
INFO:root:train_acc 9.927083
INFO:root:valid 000 2.603901e+00 9.375000 40.625000
INFO:root:valid 050 2.522578e+00 10.049020 49.908088
INFO:root:valid 100 2.520003e+00 9.746287 50.030941
INFO:root:valid 150 2.516025e+00 9.788907 50.341474
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.514756 in 226.619429 seconds
INFO:solver.bo_hb:Evaluate: [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.26461231e+00
 4.77263794e+00 1.95909429e+00 4.76943238e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.039514186522966156, 'grad_clip_value': 8, 'initial_lr': 1.4538147935760433e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0021988186450858e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 1.314988e-06
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.485812e+00 9.436275 49.816176
INFO:root:train 100 2.476937e+00 10.009282 49.721535
INFO:root:train 150 2.477472e+00 10.016556 49.627483
INFO:root:train 200 2.479020e+00 9.841418 49.222637
INFO:root:train 250 2.477985e+00 9.754731 49.284114
INFO:root:train 300 2.476784e+00 9.790282 49.174626
INFO:root:train 350 2.476695e+00 9.780093 49.176460
INFO:root:train 400 2.475786e+00 9.752961 49.279146
INFO:root:train 450 2.472951e+00 9.780349 49.435283
INFO:root:train 500 2.471534e+00 9.758608 49.500998
INFO:root:train 550 2.469829e+00 9.760662 49.639859
INFO:root:train 600 2.470762e+00 9.715578 49.552829
INFO:root:train 650 2.471369e+00 9.634217 49.440764
INFO:root:train 700 2.470936e+00 9.600125 49.409326
INFO:root:train_acc 9.585417
INFO:root:valid 000 2.361783e+00 15.625000 54.687500
INFO:root:valid 050 2.455494e+00 9.099265 50.061275
INFO:root:valid 100 2.451474e+00 9.436881 50.216584
INFO:root:valid 150 2.453181e+00 9.012831 50.103477
INFO:root:valid_acc 9.125000
INFO:root:epoch 1 lr 9.515341e-07
INFO:root:train 000 2.475636e+00 10.937500 48.437500
INFO:root:train 050 2.473748e+00 9.712010 47.886029
INFO:root:train 100 2.468533e+00 9.344059 48.917079
INFO:root:train 150 2.466319e+00 9.281871 48.903146
INFO:root:train 200 2.464620e+00 9.211754 49.036070
INFO:root:train 250 2.461325e+00 9.150896 49.178287
INFO:root:train 300 2.460593e+00 9.302326 49.278447
INFO:root:train 350 2.459358e+00 9.294872 49.327813
INFO:root:train 400 2.456916e+00 9.464620 49.376559
INFO:root:train 450 2.456534e+00 9.357677 49.490715
INFO:root:train 500 2.457087e+00 9.356287 49.388723
INFO:root:train 550 2.454498e+00 9.488430 49.546279
INFO:root:train 600 2.452360e+00 9.546589 49.711418
INFO:root:train 650 2.451405e+00 9.567012 49.786386
INFO:root:train 700 2.450503e+00 9.560004 49.864034
INFO:root:train_acc 9.566667
INFO:root:valid 000 2.489544e+00 6.250000 43.750000
INFO:root:valid 050 2.439186e+00 9.068627 50.367647
INFO:root:valid 100 2.434762e+00 9.282178 50.804455
INFO:root:valid 150 2.436194e+00 9.033526 50.838162
INFO:root:valid_acc 9.308333
INFO:root:epoch 2 lr 5.022807e-07
INFO:root:train 000 2.543736e+00 7.812500 42.187500
INFO:root:train 050 2.446881e+00 9.160539 49.417892
INFO:root:train 100 2.448315e+00 9.313119 49.474010
INFO:root:train 150 2.442994e+00 9.571606 50.010348
INFO:root:train 200 2.440917e+00 9.608209 50.069963
INFO:root:train 250 2.440558e+00 9.605329 49.975100
INFO:root:train 300 2.441360e+00 9.504776 50.077865
INFO:root:train 350 2.441991e+00 9.455128 50.115741
INFO:root:train 400 2.442317e+00 9.410069 50.202618
INFO:root:train 450 2.443309e+00 9.350748 50.103936
INFO:root:train 500 2.442524e+00 9.390594 50.155938
INFO:root:train 550 2.441631e+00 9.502609 50.181488
INFO:root:train 600 2.440416e+00 9.476394 50.314580
INFO:root:train 650 2.440370e+00 9.526210 50.302419
INFO:root:train 700 2.439689e+00 9.551088 50.338802
INFO:root:train_acc 9.493750
INFO:root:valid 000 2.483427e+00 7.812500 46.875000
INFO:root:valid 050 2.425989e+00 9.405637 51.041667
INFO:root:valid 100 2.427473e+00 9.297649 50.758045
INFO:root:valid 150 2.428792e+00 9.271523 50.610513
INFO:root:valid_acc 9.325000
INFO:root:epoch 3 lr 1.388270e-07
INFO:root:train 000 2.308400e+00 14.062500 54.687500
INFO:root:train 050 2.425452e+00 10.386029 50.888480
INFO:root:train 100 2.437280e+00 9.699876 50.030941
INFO:root:train 150 2.435225e+00 9.602649 50.382864
INFO:root:train 200 2.435896e+00 9.507152 50.590796
INFO:root:train 250 2.433653e+00 9.692480 50.690986
INFO:root:train 300 2.432580e+00 9.816238 50.695598
INFO:root:train 350 2.433843e+00 9.775641 50.592058
INFO:root:train 400 2.434462e+00 9.784133 50.537718
INFO:root:train 450 2.435680e+00 9.693736 50.478104
INFO:root:train 500 2.437364e+00 9.640095 50.414795
INFO:root:train 550 2.437597e+00 9.650068 50.357305
INFO:root:train 600 2.437501e+00 9.629784 50.405574
INFO:root:train 650 2.437655e+00 9.617416 50.391225
INFO:root:train 700 2.437157e+00 9.615728 50.459165
INFO:root:train_acc 9.579167
INFO:root:valid 000 2.472605e+00 12.500000 42.187500
INFO:root:valid 050 2.435957e+00 8.670343 50.643382
INFO:root:valid 100 2.418519e+00 9.916460 51.253094
INFO:root:valid 150 2.422115e+00 9.737169 51.127897
INFO:root:valid_acc 9.433333
INFO:root:epoch 4 lr 0.000000e+00
INFO:root:train 000 2.416252e+00 10.937500 51.562500
INFO:root:train 050 2.439208e+00 10.018382 50.919118
INFO:root:train 100 2.446053e+00 9.313119 50.092822
INFO:root:train 150 2.443480e+00 9.509520 50.051738
INFO:root:train 200 2.441809e+00 9.491604 50.388682
INFO:root:train 250 2.439158e+00 9.704930 50.647410
INFO:root:train 300 2.438518e+00 9.733181 50.685216
INFO:root:train 350 2.438718e+00 9.820157 50.667735
INFO:root:train 400 2.439202e+00 9.830892 50.572787
INFO:root:train 450 2.439154e+00 9.801136 50.557788
INFO:root:train 500 2.440330e+00 9.721183 50.302520
INFO:root:train 550 2.440981e+00 9.726633 50.189995
INFO:root:train 600 2.440322e+00 9.718178 50.262583
INFO:root:train 650 2.439917e+00 9.703821 50.309620
INFO:root:train 700 2.439299e+00 9.722718 50.316512
INFO:root:train_acc 9.750000
INFO:root:valid 000 2.496244e+00 7.812500 39.062500
INFO:root:valid 050 2.434923e+00 9.926471 50.367647
INFO:root:valid 100 2.434002e+00 9.746287 50.402228
INFO:root:valid 150 2.431461e+00 9.623344 50.693295
INFO:root:valid_acc 9.550000
INFO:solver.bo_hb:Configuration achieved a performance of 2.430626 in 179.989823 seconds
INFO:solver.bo_hb:Evaluate: [1.66082825e-01 6.97328841e+00 1.36329774e-02 1.37932027e+00
 4.48702076e+00 2.32289561e+00 3.40176373e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.06643313002328846, 'grad_clip_value': 8, 'initial_lr': 1.1699434945750075e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0015677978034067e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 1.058224e-06
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.485956e+00 9.436275 49.816176
INFO:root:train 100 2.477255e+00 10.009282 49.706064
INFO:root:train 150 2.477964e+00 10.016556 49.606788
INFO:root:train 200 2.479699e+00 9.833644 49.191542
INFO:root:train 250 2.478845e+00 9.748506 49.259213
INFO:root:train 300 2.477819e+00 9.785091 49.148671
INFO:root:train 350 2.477914e+00 9.775641 49.145299
INFO:root:train 400 2.477177e+00 9.752961 49.251870
INFO:root:train 450 2.474498e+00 9.790743 49.404102
INFO:root:train 500 2.473249e+00 9.771083 49.476048
INFO:root:train 550 2.471706e+00 9.769170 49.620009
INFO:root:train 600 2.472818e+00 9.720778 49.532030
INFO:root:train 650 2.473609e+00 9.639017 49.416763
INFO:root:train 700 2.473349e+00 9.606812 49.382578
INFO:root:train_acc 9.589583
INFO:root:valid 000 2.366000e+00 14.062500 54.687500
INFO:root:valid 050 2.460563e+00 9.068627 49.816176
INFO:root:valid 100 2.456437e+00 9.452351 50.046411
INFO:root:valid 150 2.458159e+00 9.033526 49.989652
INFO:root:valid_acc 9.133333
INFO:root:epoch 1 lr 7.657380e-07
INFO:root:train 000 2.479164e+00 10.937500 46.875000
INFO:root:train 050 2.480116e+00 9.681373 47.855392
INFO:root:train 100 2.474991e+00 9.251238 48.932550
INFO:root:train 150 2.472929e+00 9.281871 48.768626
INFO:root:train 200 2.471589e+00 9.157338 48.896144
INFO:root:train 250 2.468420e+00 9.150896 49.203187
INFO:root:train 300 2.468172e+00 9.276370 49.283638
INFO:root:train 350 2.466942e+00 9.277066 49.336717
INFO:root:train 400 2.464354e+00 9.445137 49.470075
INFO:root:train 450 2.464214e+00 9.343819 49.511502
INFO:root:train 500 2.464823e+00 9.346931 49.385604
INFO:root:train 550 2.462409e+00 9.474251 49.529265
INFO:root:train 600 2.460496e+00 9.530990 49.662022
INFO:root:train 650 2.459700e+00 9.540611 49.723982
INFO:root:train 700 2.458892e+00 9.526569 49.754815
INFO:root:train_acc 9.541667
INFO:root:valid 000 2.501343e+00 6.250000 43.750000
INFO:root:valid 050 2.448376e+00 9.068627 50.091912
INFO:root:valid 100 2.443723e+00 9.220297 50.433168
INFO:root:valid 150 2.445102e+00 9.074917 50.434603
INFO:root:valid_acc 9.400000
INFO:root:epoch 2 lr 4.042055e-07
INFO:root:train 000 2.574777e+00 7.812500 39.062500
INFO:root:train 050 2.457576e+00 9.283088 49.325980
INFO:root:train 100 2.459730e+00 9.421411 49.087252
INFO:root:train 150 2.453151e+00 9.840646 49.637831
INFO:root:train 200 2.451388e+00 9.802550 49.720149
INFO:root:train 250 2.451438e+00 9.742281 49.813247
INFO:root:train 300 2.452602e+00 9.650125 49.896179
INFO:root:train 350 2.453200e+00 9.570869 49.915420
INFO:root:train 400 2.453743e+00 9.491895 49.972724
INFO:root:train 450 2.454803e+00 9.454684 49.878742
INFO:root:train 500 2.454401e+00 9.477919 49.862774
INFO:root:train 550 2.453639e+00 9.590517 49.863884
INFO:root:train 600 2.452373e+00 9.551789 50.036398
INFO:root:train 650 2.452144e+00 9.619816 50.009601
INFO:root:train 700 2.451651e+00 9.635788 49.984397
INFO:root:train_acc 9.579167
INFO:root:valid 000 2.493602e+00 7.812500 46.875000
INFO:root:valid 050 2.436740e+00 9.497549 50.796569
INFO:root:valid 100 2.439070e+00 9.359530 50.525990
INFO:root:valid 150 2.440577e+00 9.385348 50.424255
INFO:root:valid_acc 9.441667
INFO:root:epoch 3 lr 1.117197e-07
INFO:root:train 000 2.316633e+00 12.500000 59.375000
INFO:root:train 050 2.442691e+00 10.386029 50.674020
INFO:root:train 100 2.455669e+00 9.761757 50.061881
INFO:root:train 150 2.451936e+00 9.675083 50.486341
INFO:root:train 200 2.452004e+00 9.569341 50.513060
INFO:root:train 250 2.449579e+00 9.704930 50.554034
INFO:root:train 300 2.449142e+00 9.743563 50.462002
INFO:root:train 350 2.449977e+00 9.713319 50.431802
INFO:root:train 400 2.450894e+00 9.698410 50.424719
INFO:root:train 450 2.451400e+00 9.638304 50.415743
INFO:root:train 500 2.452570e+00 9.640095 50.455339
INFO:root:train 550 2.452909e+00 9.664247 50.397005
INFO:root:train 600 2.452885e+00 9.634983 50.356177
INFO:root:train 650 2.452927e+00 9.634217 50.276018
INFO:root:train 700 2.452781e+00 9.644704 50.314283
INFO:root:train_acc 9.608333
INFO:root:valid 000 2.486034e+00 12.500000 43.750000
INFO:root:valid 050 2.450355e+00 8.854167 50.490196
INFO:root:valid 100 2.432226e+00 9.993812 51.082921
INFO:root:valid 150 2.436116e+00 9.923427 51.003725
INFO:root:valid_acc 9.600000
INFO:root:epoch 4 lr 0.000000e+00
INFO:root:train 000 2.452647e+00 12.500000 40.625000
INFO:root:train 050 2.459785e+00 9.834559 51.164216
INFO:root:train 100 2.465690e+00 9.297649 50.046411
INFO:root:train 150 2.461630e+00 9.468129 49.958609
INFO:root:train 200 2.460083e+00 9.398321 50.357587
INFO:root:train 250 2.456837e+00 9.599104 50.610060
INFO:root:train 300 2.455767e+00 9.660507 50.705980
INFO:root:train 350 2.456132e+00 9.695513 50.667735
INFO:root:train 400 2.456050e+00 9.772444 50.600062
INFO:root:train 450 2.455920e+00 9.776885 50.692905
INFO:root:train 500 2.457261e+00 9.689995 50.417914
INFO:root:train 550 2.457966e+00 9.709619 50.388498
INFO:root:train 600 2.457597e+00 9.684380 50.431572
INFO:root:train 650 2.457178e+00 9.675019 50.417627
INFO:root:train 700 2.456314e+00 9.713802 50.421273
INFO:root:train_acc 9.760417
INFO:root:valid 000 2.516795e+00 7.812500 35.937500
INFO:root:valid 050 2.450918e+00 9.865196 50.275735
INFO:root:valid 100 2.449661e+00 9.730817 50.495050
INFO:root:valid 150 2.446807e+00 9.695778 50.879553
INFO:root:valid_acc 9.658333
INFO:solver.bo_hb:Configuration achieved a performance of 2.445809 in 180.378739 seconds
INFO:solver.bo_hb:Start iteration 3 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/aml-project/src/models/DNGO.py:157: RuntimeWarning: invalid value encountered in true_divide
  x_nom=(x-mx_mat)/sx_mat
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/rkohli/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py:2093: RuntimeWarning: invalid value encountered in det
  r = _umath_linalg.det(a, signature=signature)
INFO:solver.bo_hb:Time to train the model: 16.202965
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.368780
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537378e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.907361 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 4 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.302334
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.358711
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474960e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368844e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.815462 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 5 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.320982
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.358324
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368844e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.835574 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 6 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.395536
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.360953
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 71.061161 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 7 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.473780
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.359150
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474960e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.819931 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 8 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.515202
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.358684
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.888908 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 9 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.489916
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.563226
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.974843 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 10 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.567271
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.359224
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537378e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 71.726924 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 11 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.614899
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.358718
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474960e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.966836 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 12 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.669424
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.366119
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.852993 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 13 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.705514
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.360107
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474960e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.860549 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 14 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.724175
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.358309
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474960e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.829099 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 15 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.699298
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.358167
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537378e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.919900 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 16 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.916678
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.360727
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.950122 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 17 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.923579
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.360602
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474960e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480448e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537378e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.891143 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 18 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.839382
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.358199
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474960e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537378e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.939670 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 19 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([19])) that is different to the input size (torch.Size([19, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.946841
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.358427
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368844e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480448e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537378e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 71.171714 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 20 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 14.990901
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.360096
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537378e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 71.011533 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 21 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([21])) that is different to the input size (torch.Size([21, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.027542
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.370154
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368844e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537378e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.971910 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 22 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([22])) that is different to the input size (torch.Size([22, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.096809
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.358983
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.930478 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 23 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([23])) that is different to the input size (torch.Size([23, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.188138
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.364312
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368844e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.898561 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 24 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.140098
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.362088
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.928341 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 25 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([25])) that is different to the input size (torch.Size([25, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.202104
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.364677
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 71.039457 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 26 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([26])) that is different to the input size (torch.Size([26, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.265598
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.362932
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474960e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.904417 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 27 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.293499
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.364638
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474960e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.950278 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 28 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.323810
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.364959
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368844e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.947873 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 29 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.392160
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.362718
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537378e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.860660 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 30 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.346107
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.362665
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.968073 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 31 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([31])) that is different to the input size (torch.Size([31, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.470141
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.364567
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368844e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 71.821007 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 32 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.530200
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.364844
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537378e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.813402 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 33 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([33])) that is different to the input size (torch.Size([33, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.523999
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.364612
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368844e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.997599 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 34 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([34])) that is different to the input size (torch.Size([34, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.536423
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.363257
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368844e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.916941 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 35 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([35])) that is different to the input size (torch.Size([35, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.646885
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.362805
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474960e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495738e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.881346 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 36 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([36])) that is different to the input size (torch.Size([36, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.579247
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.362668
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537378e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 71.027632 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 37 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([37])) that is different to the input size (torch.Size([37, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.633325
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.362663
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474959e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368843e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 71.082968 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 38 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([38])) that is different to the input size (torch.Size([38, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 15.661170
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.361271
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474960e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368844e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478429e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.946524 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Start iteration 39 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([39])) that is different to the input size (torch.Size([39, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 19.329299
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.361345
INFO:solver.bo_hb:Next candidate [2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00 1.50000e+00
 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.034034MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.455928e+00 7.812500 53.125000
INFO:root:train 050 2.486050e+00 9.436275 49.816176
INFO:root:train 100 2.477465e+00 10.009282 49.690594
INFO:root:train 150 2.478290e+00 10.016556 49.596440
INFO:root:train 200 2.480148e+00 9.841418 49.183769
INFO:root:train 250 2.479414e+00 9.760956 49.246763
INFO:root:train 300 2.478506e+00 9.795473 49.143480
INFO:root:train 350 2.478723e+00 9.784544 49.140848
INFO:root:train 400 2.478100e+00 9.760754 49.240181
INFO:root:train 450 2.475526e+00 9.801136 49.397173
INFO:root:train 500 2.474389e+00 9.786677 49.469810
INFO:root:train 550 2.472955e+00 9.783348 49.611502
INFO:root:train 600 2.474188e+00 9.733777 49.519031
INFO:root:train 650 2.475103e+00 9.655818 49.402362
INFO:root:train 700 2.474960e+00 9.622414 49.366976
INFO:root:train_acc 9.606250
INFO:root:valid 000 2.368844e+00 14.062500 54.687500
INFO:root:valid 050 2.463973e+00 9.099265 49.908088
INFO:root:valid 100 2.459777e+00 9.498762 50.046411
INFO:root:valid 150 2.461510e+00 9.074917 49.968957
INFO:root:valid_acc 9.166667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.495739e+00 9.375000 50.000000
INFO:root:train 050 2.487994e+00 10.018382 48.713235
INFO:root:train 100 2.484815e+00 9.313119 49.427599
INFO:root:train 150 2.485542e+00 9.406043 49.441225
INFO:root:train 200 2.486234e+00 9.289490 49.393657
INFO:root:train 250 2.482675e+00 9.238048 49.688745
INFO:root:train 300 2.482667e+00 9.317899 49.595100
INFO:root:train 350 2.482209e+00 9.317130 49.626068
INFO:root:train 400 2.480237e+00 9.398379 49.731141
INFO:root:train 450 2.480513e+00 9.336890 49.809451
INFO:root:train 500 2.482422e+00 9.312625 49.675649
INFO:root:train 550 2.480449e+00 9.443058 49.770304
INFO:root:train 600 2.479010e+00 9.536190 49.867408
INFO:root:train 650 2.478428e+00 9.595814 49.918395
INFO:root:train 700 2.478119e+00 9.584522 49.948734
INFO:root:train_acc 9.597917
INFO:root:valid 000 2.537377e+00 6.250000 43.750000
INFO:root:valid 050 2.474574e+00 9.191176 50.000000
INFO:root:valid 100 2.469269e+00 9.498762 50.247525
INFO:root:valid 150 2.470355e+00 9.385348 50.165563
INFO:root:valid_acc 9.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.469484 
INFO:solver.bo_hb:Evaluation of this configuration took 70.950181 seconds
INFO:solver.bo_hb:Current incumbent [9.87854663e-02 4.07305960e+00 3.25018168e-02 1.00000000e+00
 4.77263794e+00 2.00000000e+00 4.76943238e-04] with estimated performance 2.430626
INFO:solver.bo_hb:Return [0.09878546630741539, 4.073059595565921, 0.03250181675837139, 1.0, 4.772637940878366, 2.0, 0.00047694323824041846] as incumbent with error 2.430626 
INFO:root:gpu device = cuda:0
INFO:root:config = {'drop_path_prob': 0.039514186522966156, 'grad_clip_value': 8, 'initial_lr': 1.4538147935760433e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0021988186450858e-05}
INFO:root:param size = 0.039065MB
INFO:root:epoch 0 lr 1.444865e-06
INFO:root:train 000 3.930316e+00 3.125000 10.937500
INFO:root:train 050 3.957617e+00 2.573529 11.979167
INFO:root:train 100 3.953499e+00 2.923886 11.834777
INFO:root:train 150 3.950199e+00 2.887003 11.899834
INFO:root:train 200 3.945172e+00 3.055037 12.010261
INFO:root:train 250 3.942389e+00 3.112550 12.008217
INFO:root:train 300 3.942570e+00 3.078281 11.918605
INFO:root:train 350 3.944479e+00 3.120548 11.845620
INFO:root:train 400 3.946359e+00 3.066552 11.790835
INFO:root:train 450 3.946568e+00 3.066103 11.762057
INFO:root:train 500 3.945089e+00 3.081337 11.792041
INFO:root:train 550 3.944858e+00 3.068285 11.830762
INFO:root:train 600 3.945074e+00 3.041805 11.774646
INFO:root:train 650 3.944134e+00 3.052995 11.777554
INFO:root:train 700 3.945006e+00 3.042529 11.690888
INFO:root:train 750 3.945120e+00 3.031375 11.680260
INFO:root:train 800 3.944977e+00 3.023564 11.647550
INFO:root:train 850 3.943558e+00 3.051557 11.712324
INFO:root:train 900 3.943346e+00 3.008810 11.681465
INFO:root:train 950 3.943261e+00 2.988630 11.721215
INFO:root:train 1000 3.942649e+00 3.018856 11.746066
INFO:root:train 1050 3.942510e+00 3.025392 11.741794
INFO:root:train 1100 3.941082e+00 3.045527 11.793256
INFO:root:train 1150 3.940559e+00 3.053052 11.803595
INFO:root:train 1200 3.941112e+00 3.040435 11.745420
INFO:root:train 1250 3.940760e+00 3.048811 11.753098
INFO:root:train 1300 3.940679e+00 3.048136 11.762586
INFO:root:train 1350 3.940223e+00 3.031319 11.759808
INFO:root:train 1400 3.940374e+00 3.020164 11.751651
INFO:root:train 1450 3.939359e+00 3.060389 11.769900
INFO:root:train 1500 3.938727e+00 3.076074 11.801507
INFO:root:train 1550 3.938183e+00 3.081681 11.822010
INFO:root:train 1600 3.938203e+00 3.067419 11.796338
INFO:root:train 1650 3.938088e+00 3.072948 11.783578
INFO:root:train 1700 3.937637e+00 3.080908 11.799126
INFO:root:train 1750 3.937519e+00 3.061643 11.777199
INFO:root:train 1800 3.937439e+00 3.064270 11.773841
INFO:root:train 1850 3.937192e+00 3.065910 11.770665
INFO:root:train 1900 3.937083e+00 3.064177 11.771765
INFO:root:train 1950 3.936985e+00 3.065736 11.768804
INFO:root:train 2000 3.936523e+00 3.085176 11.814405
INFO:root:train 2050 3.936448e+00 3.093003 11.806741
INFO:root:train 2100 3.936146e+00 3.098227 11.823983
INFO:root:train 2150 3.935968e+00 3.103934 11.820810
INFO:root:train 2200 3.935706e+00 3.108672 11.820621
INFO:root:train 2250 3.935535e+00 3.109729 11.808641
INFO:root:train 2300 3.935217e+00 3.122284 11.834528
INFO:root:train 2350 3.934818e+00 3.124335 11.846023
INFO:root:train 2400 3.934607e+00 3.127603 11.861594
INFO:root:train 2450 3.934551e+00 3.119263 11.852943
INFO:root:train 2500 3.934384e+00 3.123126 11.856507
INFO:root:train 2550 3.934254e+00 3.133575 11.855032
INFO:root:train 2600 3.934010e+00 3.141220 11.852413
INFO:root:train 2650 3.933608e+00 3.145040 11.864037
INFO:root:train 2700 3.933362e+00 3.154503 11.874653
INFO:root:train 2750 3.933204e+00 3.164190 11.880339
INFO:root:train 2800 3.933039e+00 3.171858 11.881917
INFO:root:train 2850 3.932968e+00 3.169940 11.873027
INFO:root:train 2900 3.932804e+00 3.176168 11.892451
INFO:root:train 2950 3.932851e+00 3.174771 11.874153
INFO:root:train 3000 3.932623e+00 3.178107 11.885621
INFO:root:train 3050 3.932595e+00 3.177749 11.893641
INFO:root:train 3100 3.932302e+00 3.180426 11.905434
INFO:root:train 3150 3.932209e+00 3.177563 11.911397
INFO:root:train 3200 3.931962e+00 3.183087 11.922056
INFO:root:train 3250 3.931738e+00 3.179791 11.923735
INFO:root:train 3300 3.931288e+00 3.184168 11.935304
INFO:root:train 3350 3.931056e+00 3.191678 11.944196
INFO:root:train 3400 3.930867e+00 3.194373 11.949610
INFO:root:train 3450 3.930603e+00 3.196990 11.953057
INFO:root:train 3500 3.930240e+00 3.197747 11.965778
INFO:root:train 3550 3.930075e+00 3.199803 11.982980
INFO:root:train 3600 3.929861e+00 3.202235 11.984952
INFO:root:train_acc 3.209175
INFO:root:valid 000 4.005069e+00 7.812500 12.500000
INFO:root:valid 050 3.926544e+00 3.125000 12.040441
INFO:root:valid 100 3.927840e+00 3.047649 12.144183
INFO:root:valid 150 3.927908e+00 2.907699 11.982616
INFO:root:valid 200 3.926560e+00 2.938433 12.095771
INFO:root:valid 250 3.925994e+00 2.894671 12.145169
INFO:root:valid 300 3.924928e+00 2.974460 12.307932
INFO:root:valid 350 3.924099e+00 3.013711 12.348647
INFO:root:valid 400 3.922929e+00 3.058759 12.488310
INFO:root:valid 450 3.923275e+00 3.021064 12.482677
INFO:root:valid 500 3.923233e+00 3.028318 12.440744
INFO:root:valid 550 3.923978e+00 3.017241 12.465971
INFO:root:valid 600 3.924613e+00 3.026206 12.437604
INFO:root:test_acc 3.022284
INFO:root:epoch 1 lr 1.418237e-06
INFO:root:train 000 3.983857e+00 0.000000 14.062500
INFO:root:train 050 3.928298e+00 3.400735 12.040441
INFO:root:train 100 3.919528e+00 3.264233 12.515470
INFO:root:train 150 3.912073e+00 3.249172 12.810430
INFO:root:train 200 3.909492e+00 3.303794 12.810945
INFO:root:train 250 3.909548e+00 3.380229 12.892181
INFO:root:train 300 3.909140e+00 3.462417 12.868563
INFO:root:train 350 3.909017e+00 3.485577 12.896189
INFO:root:train 400 3.909067e+00 3.479582 12.835100
INFO:root:train 450 3.909818e+00 3.467988 12.797949
INFO:root:train 500 3.909859e+00 3.464945 12.808757
INFO:root:train 550 3.909177e+00 3.522005 12.857305
INFO:root:train 600 3.907622e+00 3.514975 12.895175
INFO:root:train 650 3.907391e+00 3.506624 12.848022
INFO:root:train 700 3.907570e+00 3.492778 12.816512
INFO:root:train 750 3.907006e+00 3.480776 12.824567
INFO:root:train 800 3.906388e+00 3.489778 12.870630
INFO:root:train 850 3.906538e+00 3.473854 12.865379
INFO:root:train 900 3.905971e+00 3.497850 12.891926
INFO:root:train 950 3.905483e+00 3.502892 12.889393
INFO:root:train 1000 3.905739e+00 3.505869 12.885552
INFO:root:train 1050 3.905457e+00 3.524917 12.904377
INFO:root:train 1100 3.905398e+00 3.520947 12.897366
INFO:root:train 1150 3.905672e+00 3.520037 12.892322
INFO:root:train 1200 3.905117e+00 3.523106 12.896805
INFO:root:train 1250 3.905031e+00 3.529676 12.919664
INFO:root:train 1300 3.904403e+00 3.522531 12.950375
INFO:root:train 1350 3.904533e+00 3.521697 12.911732
INFO:root:train 1400 3.904522e+00 3.516461 12.910421
INFO:root:train 1450 3.904524e+00 3.508356 12.912431
INFO:root:train 1500 3.904431e+00 3.502873 12.915348
INFO:root:train 1550 3.904186e+00 3.508825 12.933188
INFO:root:train 1600 3.903985e+00 3.498790 12.930395
INFO:root:train 1650 3.903868e+00 3.506398 12.953324
INFO:root:train 1700 3.903669e+00 3.505291 12.927138
INFO:root:train 1750 3.903323e+00 3.500678 12.929219
INFO:root:train 1800 3.903113e+00 3.501527 12.932052
INFO:root:train 1850 3.902603e+00 3.520057 12.949082
INFO:root:train 1900 3.902142e+00 3.527749 12.983298
INFO:root:train 1950 3.902043e+00 3.531042 12.982125
INFO:root:train 2000 3.901680e+00 3.531828 12.990380
INFO:root:train 2050 3.901909e+00 3.530290 12.959380
INFO:root:train 2100 3.901730e+00 3.525851 12.964808
INFO:root:train 2150 3.901435e+00 3.523071 12.988145
INFO:root:train 2200 3.901229e+00 3.526806 12.980605
INFO:root:train 2250 3.901163e+00 3.525516 12.958824
INFO:root:train 2300 3.901528e+00 3.519530 12.942742
INFO:root:train 2350 3.901458e+00 3.525096 12.947948
INFO:root:train 2400 3.901241e+00 3.533684 12.945778
INFO:root:train 2450 3.901275e+00 3.532359 12.944334
INFO:root:train 2500 3.900956e+00 3.537960 12.961066
INFO:root:train 2550 3.900388e+00 3.539666 12.973466
INFO:root:train 2600 3.900310e+00 3.535299 12.960760
INFO:root:train 2650 3.900211e+00 3.528739 12.959143
INFO:root:train 2700 3.900156e+00 3.532257 12.965106
INFO:root:train 2750 3.900099e+00 3.520311 12.956652
INFO:root:train 2800 3.899956e+00 3.523853 12.967467
INFO:root:train 2850 3.899773e+00 3.527271 12.973518
INFO:root:train 2900 3.899401e+00 3.537573 13.006291
INFO:root:train 2950 3.899259e+00 3.544879 13.013597
INFO:root:train 3000 3.899173e+00 3.550900 13.005561
INFO:root:train 3050 3.898992e+00 3.551602 13.011615
INFO:root:train 3100 3.898873e+00 3.551778 13.005885
INFO:root:train 3150 3.898733e+00 3.558394 13.008271
INFO:root:train 3200 3.898539e+00 3.552113 13.002284
INFO:root:train 3250 3.898449e+00 3.553234 13.009939
INFO:root:train 3300 3.898240e+00 3.556687 13.018309
INFO:root:train 3350 3.897855e+00 3.556774 13.029693
INFO:root:train 3400 3.897780e+00 3.558696 13.030634
INFO:root:train 3450 3.897512e+00 3.550149 13.038793
INFO:root:train 3500 3.897400e+00 3.553895 13.058323
INFO:root:train 3550 3.897245e+00 3.552696 13.068502
INFO:root:train 3600 3.896971e+00 3.560209 13.079700
INFO:root:train_acc 3.561638
INFO:root:valid 000 3.963344e+00 7.812500 12.500000
INFO:root:valid 050 3.895996e+00 3.400735 12.745098
INFO:root:valid 100 3.897161e+00 3.295173 12.840347
INFO:root:valid 150 3.897170e+00 3.145695 12.789735
INFO:root:valid 200 3.896195e+00 3.156095 12.943097
INFO:root:valid 250 3.895618e+00 3.143675 13.066484
INFO:root:valid 300 3.894610e+00 3.249585 13.164452
INFO:root:valid 350 3.893959e+00 3.271902 13.292379
INFO:root:valid 400 3.893119e+00 3.350998 13.411783
INFO:root:valid 450 3.893308e+00 3.298226 13.369595
INFO:root:valid 500 3.893084e+00 3.309007 13.295284
INFO:root:valid 550 3.893779e+00 3.295145 13.316697
INFO:root:valid 600 3.894442e+00 3.304389 13.279950
INFO:root:test_acc 3.302462
INFO:root:epoch 2 lr 1.374587e-06
INFO:root:train 000 3.975617e+00 3.125000 7.812500
INFO:root:train 050 3.882761e+00 3.400735 13.265931
INFO:root:train 100 3.880886e+00 3.387995 13.242574
INFO:root:train 150 3.877825e+00 3.683775 13.379553
INFO:root:train 200 3.875069e+00 3.708022 13.603856
INFO:root:train 250 3.878176e+00 3.598108 13.402639
INFO:root:train 300 3.879967e+00 3.659676 13.346138
INFO:root:train 350 3.879450e+00 3.712607 13.363604
INFO:root:train 400 3.881096e+00 3.689994 13.357232
INFO:root:train 450 3.881905e+00 3.748614 13.373060
INFO:root:train 500 3.880891e+00 3.779940 13.463698
INFO:root:train 550 3.880018e+00 3.797074 13.509528
INFO:root:train 600 3.880109e+00 3.777558 13.428141
INFO:root:train 650 3.879522e+00 3.763441 13.452861
INFO:root:train 700 3.880367e+00 3.775856 13.416102
INFO:root:train 750 3.880533e+00 3.749168 13.419607
INFO:root:train 800 3.879883e+00 3.799938 13.453886
INFO:root:train 850 3.879956e+00 3.808020 13.427218
INFO:root:train 900 3.879682e+00 3.790927 13.485017
INFO:root:train 950 3.879826e+00 3.775631 13.484161
INFO:root:train 1000 3.879614e+00 3.786838 13.502123
INFO:root:train 1050 3.878922e+00 3.774679 13.507969
INFO:root:train 1100 3.879326e+00 3.756528 13.483481
INFO:root:train 1150 3.879054e+00 3.767105 13.476053
INFO:root:train 1200 3.878804e+00 3.772898 13.516080
INFO:root:train 1250 3.878738e+00 3.769484 13.520434
INFO:root:train 1300 3.878133e+00 3.792756 13.571291
INFO:root:train 1350 3.877323e+00 3.805052 13.609132
INFO:root:train 1400 3.877460e+00 3.804202 13.604122
INFO:root:train 1450 3.877414e+00 3.812026 13.602688
INFO:root:train 1500 3.877163e+00 3.827657 13.606554
INFO:root:train 1550 3.876636e+00 3.842279 13.643416
INFO:root:train 1600 3.876448e+00 3.849157 13.646744
INFO:root:train 1650 3.876553e+00 3.855618 13.635675
INFO:root:train 1700 3.875800e+00 3.866292 13.675779
INFO:root:train 1750 3.875267e+00 3.875464 13.682360
INFO:root:train 1800 3.875274e+00 3.882392 13.694649
INFO:root:train 1850 3.875268e+00 3.877971 13.695300
INFO:root:train 1900 3.874790e+00 3.881181 13.718109
INFO:root:train 1950 3.874811e+00 3.873014 13.708515
INFO:root:train 2000 3.874777e+00 3.879310 13.704085
INFO:root:train 2050 3.874195e+00 3.886824 13.732630
INFO:root:train 2100 3.874105e+00 3.888773 13.730813
INFO:root:train 2150 3.874181e+00 3.888453 13.760315
INFO:root:train 2200 3.874158e+00 3.893117 13.770019
INFO:root:train 2250 3.874033e+00 3.889938 13.779293
INFO:root:train 2300 3.874070e+00 3.882144 13.786126
INFO:root:train 2350 3.873676e+00 3.889967 13.798650
INFO:root:train 2400 3.873531e+00 3.887703 13.793732
INFO:root:train 2450 3.873481e+00 3.888081 13.788377
INFO:root:train 2500 3.873253e+00 3.887195 13.788859
INFO:root:train 2550 3.872932e+00 3.904719 13.812598
INFO:root:train 2600 3.872523e+00 3.913158 13.828215
INFO:root:train 2650 3.872000e+00 3.923637 13.847958
INFO:root:train 2700 3.871774e+00 3.930836 13.853665
INFO:root:train 2750 3.871378e+00 3.938909 13.873364
INFO:root:train 2800 3.871090e+00 3.953945 13.880087
INFO:root:train 2850 3.871073e+00 3.955849 13.881094
INFO:root:train 2900 3.871186e+00 3.956071 13.878835
INFO:root:train 2950 3.871030e+00 3.959463 13.879299
INFO:root:train 3000 3.870994e+00 3.960659 13.878186
INFO:root:train 3050 3.870794e+00 3.961816 13.892474
INFO:root:train 3100 3.870703e+00 3.963943 13.884634
INFO:root:train 3150 3.870407e+00 3.968482 13.903324
INFO:root:train 3200 3.870156e+00 3.963117 13.904834
INFO:root:train 3250 3.869863e+00 3.974739 13.917833
INFO:root:train 3300 3.869808e+00 3.974174 13.934698
INFO:root:train 3350 3.869568e+00 3.977824 13.948728
INFO:root:train 3400 3.869507e+00 3.978609 13.958211
INFO:root:train 3450 3.869449e+00 3.972580 13.956552
INFO:root:train 3500 3.869230e+00 3.981452 13.974132
INFO:root:train 3550 3.869045e+00 3.992713 13.993857
INFO:root:train 3600 3.868876e+00 3.996286 14.001753
INFO:root:train_acc 3.997590
INFO:root:valid 000 3.929374e+00 7.812500 12.500000
INFO:root:valid 050 3.870806e+00 3.553922 13.572304
INFO:root:valid 100 3.871829e+00 3.527228 13.660272
INFO:root:valid 150 3.871804e+00 3.352649 13.658940
INFO:root:valid 200 3.870828e+00 3.435945 13.704913
INFO:root:valid 250 3.870121e+00 3.467380 13.857072
INFO:root:valid 300 3.869076e+00 3.592193 13.974252
INFO:root:valid 350 3.868589e+00 3.623575 14.111467
INFO:root:valid 400 3.867987e+00 3.689994 14.237843
INFO:root:valid 450 3.868134e+00 3.648143 14.201081
INFO:root:valid 500 3.867879e+00 3.655190 14.131113
INFO:root:valid 550 3.868556e+00 3.629764 14.150408
INFO:root:valid 600 3.869188e+00 3.637167 14.101498
INFO:root:test_acc 3.634524
INFO:root:epoch 3 lr 1.314988e-06
INFO:root:train 000 3.787815e+00 9.375000 23.437500
INFO:root:train 050 3.866824e+00 4.074755 13.909314
INFO:root:train 100 3.861619e+00 4.238861 13.753094
INFO:root:train 150 3.861638e+00 4.221854 13.824503
INFO:root:train 200 3.859468e+00 4.057836 13.891480
INFO:root:train 250 3.859114e+00 4.064990 13.937998
INFO:root:train 300 3.859357e+00 4.074958 13.932724
INFO:root:train 350 3.859341e+00 4.082087 13.982372
INFO:root:train 400 3.858461e+00 4.106920 14.132637
INFO:root:train 450 3.859184e+00 4.070815 14.142184
INFO:root:train 500 3.858384e+00 4.104291 14.221557
INFO:root:train 550 3.858654e+00 4.137364 14.266674
INFO:root:train 600 3.857274e+00 4.175333 14.351082
INFO:root:train 650 3.857282e+00 4.173867 14.386521
INFO:root:train 700 3.857418e+00 4.165924 14.428049
INFO:root:train 750 3.857702e+00 4.146555 14.426598
INFO:root:train 800 3.857734e+00 4.139357 14.386314
INFO:root:train 850 3.857801e+00 4.125661 14.372797
INFO:root:train 900 3.857738e+00 4.151637 14.398932
INFO:root:train 950 3.858006e+00 4.174882 14.412461
INFO:root:train 1000 3.856989e+00 4.188000 14.480832
INFO:root:train 1050 3.856468e+00 4.208789 14.505530
INFO:root:train 1100 3.855773e+00 4.212080 14.520890
INFO:root:train 1150 3.855319e+00 4.201510 14.528128
INFO:root:train 1200 3.855257e+00 4.199625 14.564686
INFO:root:train 1250 3.855159e+00 4.192896 14.563349
INFO:root:train 1300 3.854971e+00 4.199894 14.590940
INFO:root:train 1350 3.854339e+00 4.209845 14.610705
INFO:root:train 1400 3.854156e+00 4.214623 14.610100
INFO:root:train 1450 3.854200e+00 4.201844 14.608460
INFO:root:train 1500 3.853853e+00 4.197202 14.617338
INFO:root:train 1550 3.853727e+00 4.201926 14.614563
INFO:root:train 1600 3.853358e+00 4.203428 14.638312
INFO:root:train 1650 3.853177e+00 4.200106 14.649266
INFO:root:train 1700 3.852897e+00 4.222700 14.664168
INFO:root:train 1750 3.852702e+00 4.222587 14.681789
INFO:root:train 1800 3.852197e+00 4.229421 14.723591
INFO:root:train 1850 3.851985e+00 4.238418 14.738655
INFO:root:train 1900 3.851757e+00 4.251052 14.744707
INFO:root:train 1950 3.851783e+00 4.261436 14.752851
INFO:root:train 2000 3.851312e+00 4.271302 14.780891
INFO:root:train 2050 3.850829e+00 4.276877 14.799183
INFO:root:train 2100 3.850419e+00 4.289624 14.807681
INFO:root:train 2150 3.850274e+00 4.292335 14.809972
INFO:root:train 2200 3.850139e+00 4.284984 14.806480
INFO:root:train 2250 3.849954e+00 4.290454 14.810084
INFO:root:train 2300 3.849810e+00 4.301119 14.804704
INFO:root:train 2350 3.849550e+00 4.315318 14.812846
INFO:root:train 2400 3.849158e+00 4.336735 14.830409
INFO:root:train 2450 3.849076e+00 4.343253 14.836419
INFO:root:train 2500 3.849064e+00 4.350135 14.826569
INFO:root:train 2550 3.848987e+00 4.360422 14.831194
INFO:root:train 2600 3.848888e+00 4.354695 14.824827
INFO:root:train 2650 3.848822e+00 4.353899 14.831667
INFO:root:train 2700 3.848684e+00 4.350819 14.850981
INFO:root:train 2750 3.848590e+00 4.349555 14.854258
INFO:root:train 2800 3.848367e+00 4.350009 14.852954
INFO:root:train 2850 3.848139e+00 4.360312 14.874167
INFO:root:train 2900 3.847992e+00 4.363258 14.882260
INFO:root:train 2950 3.847842e+00 4.361869 14.876313
INFO:root:train 3000 3.847601e+00 4.363650 14.883580
INFO:root:train 3050 3.847182e+00 4.373566 14.916728
INFO:root:train 3100 3.847147e+00 4.374093 14.904466
INFO:root:train 3150 3.846898e+00 4.378074 14.901519
INFO:root:train 3200 3.846890e+00 4.372657 14.920142
INFO:root:train 3250 3.846662e+00 4.374616 14.933867
INFO:root:train 3300 3.846552e+00 4.378408 14.934395
INFO:root:train 3350 3.846295e+00 4.387217 14.942368
INFO:root:train 3400 3.846257e+00 4.379686 14.939540
INFO:root:train 3450 3.846264e+00 4.379165 14.944943
INFO:root:train 3500 3.846267e+00 4.377767 14.939482
INFO:root:train 3550 3.846046e+00 4.388289 14.953534
INFO:root:train 3600 3.845743e+00 4.390707 14.968064
INFO:root:train_acc 4.396531
INFO:root:valid 000 3.899356e+00 7.812500 14.062500
INFO:root:valid 050 3.848699e+00 3.982843 14.613971
INFO:root:valid 100 3.849732e+00 3.975866 14.758663
INFO:root:valid 150 3.849641e+00 3.963162 14.641970
INFO:root:valid 200 3.848656e+00 3.964552 14.707711
INFO:root:valid 250 3.847817e+00 3.971614 14.790837
INFO:root:valid 300 3.846681e+00 4.049003 14.908638
INFO:root:valid 350 3.846313e+00 4.050926 15.032942
INFO:root:valid 400 3.845980e+00 4.103024 15.118454
INFO:root:valid 450 3.846126e+00 4.063886 15.108786
INFO:root:valid 500 3.845846e+00 4.060629 15.032435
INFO:root:valid 550 3.846508e+00 4.006919 15.023820
INFO:root:valid 600 3.847095e+00 4.003744 14.995840
INFO:root:test_acc 4.000311
INFO:root:epoch 4 lr 1.240909e-06
INFO:root:train 000 3.883379e+00 7.812500 12.500000
INFO:root:train 050 3.841562e+00 4.687500 15.165441
INFO:root:train 100 3.833653e+00 4.733911 15.918936
INFO:root:train 150 3.836576e+00 4.470199 15.531871
INFO:root:train 200 3.831841e+00 4.539801 15.834888
INFO:root:train 250 3.831784e+00 4.531873 15.600100
INFO:root:train 300 3.830797e+00 4.557724 15.604236
INFO:root:train 350 3.829021e+00 4.611823 15.682870
INFO:root:train 400 3.830435e+00 4.590087 15.706827
INFO:root:train 450 3.831502e+00 4.538525 15.683897
INFO:root:train 500 3.832705e+00 4.556512 15.603169
INFO:root:train 550 3.832195e+00 4.568398 15.599478
INFO:root:train 600 3.831435e+00 4.562708 15.674397
INFO:root:train 650 3.830909e+00 4.577093 15.716206
INFO:root:train 700 3.830609e+00 4.593884 15.731990
INFO:root:train 750 3.830466e+00 4.595955 15.770639
INFO:root:train 800 3.830080e+00 4.568508 15.771301
INFO:root:train 850 3.830465e+00 4.549794 15.678246
INFO:root:train 900 3.830581e+00 4.547031 15.677026
INFO:root:train 950 3.830815e+00 4.562631 15.675933
INFO:root:train 1000 3.830765e+00 4.547015 15.667145
INFO:root:train 1050 3.830635e+00 4.553699 15.693387
INFO:root:train 1100 3.830543e+00 4.566871 15.659060
INFO:root:train 1150 3.830573e+00 4.561251 15.639933
INFO:root:train 1200 3.831193e+00 4.544390 15.596378
INFO:root:train 1250 3.831608e+00 4.561351 15.568795
INFO:root:train 1300 3.831457e+00 4.580611 15.579362
INFO:root:train 1350 3.831081e+00 4.607698 15.614591
INFO:root:train 1400 3.830981e+00 4.625045 15.621654
INFO:root:train 1450 3.830889e+00 4.620736 15.641153
INFO:root:train 1500 3.830307e+00 4.642738 15.664557
INFO:root:train 1550 3.829675e+00 4.644181 15.687460
INFO:root:train 1600 3.829406e+00 4.660173 15.724547
INFO:root:train 1650 3.829315e+00 4.693178 15.741407
INFO:root:train 1700 3.829045e+00 4.696686 15.766461
INFO:root:train 1750 3.829111e+00 4.681254 15.768668
INFO:root:train 1800 3.828931e+00 4.690103 15.772488
INFO:root:train 1850 3.829090e+00 4.684123 15.754997
INFO:root:train 1900 3.829093e+00 4.662842 15.744181
INFO:root:train 1950 3.829041e+00 4.661071 15.752339
INFO:root:train 2000 3.829020e+00 4.670321 15.772582
INFO:root:train 2050 3.829103e+00 4.668454 15.764414
INFO:root:train 2100 3.828861e+00 4.686013 15.791587
INFO:root:train 2150 3.828791e+00 4.681689 15.800790
INFO:root:train 2200 3.828887e+00 4.679691 15.779759
INFO:root:train 2250 3.828774e+00 4.679865 15.768686
INFO:root:train 2300 3.828769e+00 4.673919 15.764206
INFO:root:train 2350 3.828613e+00 4.671549 15.758587
INFO:root:train 2400 3.828456e+00 4.657565 15.751249
INFO:root:train 2450 3.828260e+00 4.669650 15.779274
INFO:root:train 2500 3.828325e+00 4.673131 15.780563
INFO:root:train 2550 3.828114e+00 4.675250 15.784251
INFO:root:train 2600 3.828001e+00 4.677888 15.790802
INFO:root:train 2650 3.827728e+00 4.679838 15.805356
INFO:root:train 2700 3.827556e+00 4.683451 15.800861
INFO:root:train 2750 3.827348e+00 4.694316 15.821519
INFO:root:train 2800 3.827122e+00 4.704235 15.829726
INFO:root:train 2850 3.827016e+00 4.715451 15.855730
INFO:root:train 2900 3.826834e+00 4.721432 15.861449
INFO:root:train 2950 3.826768e+00 4.715563 15.869620
INFO:root:train 3000 3.826676e+00 4.706764 15.888454
INFO:root:train 3050 3.826435e+00 4.710034 15.898988
INFO:root:train 3100 3.826158e+00 4.715213 15.910694
INFO:root:train 3150 3.825993e+00 4.721715 15.923516
INFO:root:train 3200 3.825815e+00 4.727038 15.943748
INFO:root:train 3250 3.825700e+00 4.725950 15.949900
INFO:root:train 3300 3.825502e+00 4.731994 15.956812
INFO:root:train 3350 3.825266e+00 4.731330 15.959788
INFO:root:train 3400 3.825239e+00 4.733442 15.963136
INFO:root:train 3450 3.825234e+00 4.734135 15.961406
INFO:root:train 3500 3.824986e+00 4.721865 15.958387
INFO:root:train 3550 3.824919e+00 4.726662 15.955453
INFO:root:train 3600 3.824724e+00 4.732626 15.952600
INFO:root:train_acc 4.730058
INFO:root:valid 000 3.873357e+00 7.812500 12.500000
INFO:root:valid 050 3.829536e+00 4.473039 15.410539
INFO:root:valid 100 3.830239e+00 4.424505 15.547649
INFO:root:valid 150 3.830039e+00 4.325331 15.552566
INFO:root:valid 200 3.829089e+00 4.267724 15.492848
INFO:root:valid 250 3.828157e+00 4.301544 15.575199
INFO:root:valid 300 3.826950e+00 4.324128 15.723630
INFO:root:valid 350 3.826708e+00 4.326923 15.887642
slurmstepd-metagpu5: error: *** JOB 3427331 ON metagpu5 CANCELLED AT 2019-09-12T23:09:25 ***
