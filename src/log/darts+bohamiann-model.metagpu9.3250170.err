INFO:solver.bo_hb:Evaluate: [3.09249961e+01 3.47313702e-01 7.26430496e+00 4.59781648e-02
 1.31903288e+00 5.72072865e+00 2.60297694e+00 8.99381446e-04]
INFO:solver.bo_hb:Configuration achieved a performance of 100.000000 in 0.000751 seconds
INFO:solver.bo_hb:Evaluate: [2.59022427e+01 2.62368452e-01 6.89851960e+00 3.34638255e-02
 1.18826397e+00 5.75381139e+00 1.10617436e+00 2.14301331e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.10494738074970683, 'grad_clip_value': 8, 'initial_lr': 1.4700060679063494e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adad', 'weight_decay': 1.0009873812428262e-05}
INFO:root:param size = 0.255930MB
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
INFO:root:epoch 0 lr 1.329633e-06
/home/rkohli/aml-project/PC-DARTS/train.py:131: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(model.parameters(), grad_clip)
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351185e+00 7.322304 48.897059
INFO:root:train 100 2.352851e+00 7.193688 47.617574
INFO:root:train 150 2.353500e+00 6.943295 47.299255
INFO:root:train 200 2.355030e+00 6.864117 46.944963
INFO:root:train 250 2.353527e+00 6.940986 47.292082
INFO:root:train 300 2.352453e+00 7.013081 47.606935
INFO:root:train 350 2.351748e+00 7.015670 47.845442
INFO:root:train 400 2.351234e+00 7.021509 47.973815
INFO:root:train 450 2.350843e+00 7.074557 48.108370
INFO:root:train 500 2.350801e+00 7.060878 48.156811
INFO:root:train 550 2.350482e+00 7.029832 48.077359
INFO:root:train 600 2.350162e+00 7.061148 48.055324
INFO:root:train 650 2.350665e+00 7.025250 48.043875
INFO:root:train 700 2.350629e+00 6.992243 48.034058
INFO:root:train_acc 7.029167
INFO:root:valid 000 2.387062e+00 4.687500 35.937500
INFO:root:valid 050 2.350660e+00 7.475490 47.334559
INFO:root:valid 100 2.352223e+00 7.209158 47.663985
INFO:root:valid 150 2.349544e+00 7.367550 48.220199
INFO:root:valid_acc 7.283333
INFO:root:epoch 1 lr 9.621315e-07
INFO:root:train 000 2.356113e+00 3.125000 46.875000
INFO:root:train 050 2.350697e+00 8.241422 48.590686
INFO:root:train 100 2.350319e+00 7.688738 48.715965
INFO:root:train 150 2.351863e+00 7.553808 47.837334
INFO:root:train 200 2.352568e+00 7.501555 47.932214
INFO:root:train 250 2.352405e+00 7.501245 47.989293
INFO:root:train 300 2.351931e+00 7.521802 48.058555
INFO:root:train 350 2.351688e+00 7.589922 48.023504
INFO:root:train 400 2.351356e+00 7.668329 48.180330
INFO:root:train 450 2.351728e+00 7.673919 48.201912
INFO:root:train 500 2.352041e+00 7.603543 48.272206
INFO:root:train 550 2.352435e+00 7.579968 48.136910
INFO:root:train 600 2.352287e+00 7.594114 48.169717
INFO:root:train 650 2.352036e+00 7.596486 48.245488
INFO:root:train 700 2.352148e+00 7.602978 48.283702
INFO:root:train_acc 7.525000
INFO:root:valid 000 2.356154e+00 3.125000 40.625000
INFO:root:valid 050 2.334005e+00 7.965686 48.713235
INFO:root:valid 100 2.338793e+00 7.534035 48.004332
INFO:root:valid 150 2.338058e+00 7.543460 47.795944
INFO:root:valid_acc 7.400000
INFO:root:epoch 2 lr 5.078746e-07
INFO:root:train 000 2.320044e+00 10.937500 56.250000
INFO:root:train 050 2.358731e+00 8.210784 47.334559
INFO:root:train 100 2.357499e+00 7.874381 48.267327
INFO:root:train 150 2.355065e+00 8.071192 48.468543
INFO:root:train 200 2.355093e+00 7.944652 48.616294
INFO:root:train 250 2.354472e+00 8.042829 48.661604
INFO:root:train 300 2.354548e+00 8.092816 48.681478
INFO:root:train 350 2.355978e+00 7.941595 48.446403
INFO:root:train 400 2.357080e+00 7.882637 48.355673
INFO:root:train 450 2.357519e+00 7.867932 48.288525
INFO:root:train 500 2.357921e+00 7.834331 48.125624
INFO:root:train 550 2.357903e+00 7.826679 48.162432
INFO:root:train 600 2.357600e+00 7.817700 48.278910
INFO:root:train 650 2.357447e+00 7.877304 48.262289
INFO:root:train 700 2.357442e+00 7.890514 48.236894
INFO:root:train_acc 7.870833
INFO:root:valid 000 2.338570e+00 9.375000 39.062500
INFO:root:valid 050 2.343363e+00 7.751225 44.332108
INFO:root:valid 100 2.335772e+00 8.199257 46.209777
INFO:root:valid 150 2.333560e+00 8.423013 47.102649
INFO:root:valid_acc 8.441667
INFO:root:epoch 3 lr 1.403731e-07
INFO:root:train 000 2.323344e+00 14.062500 51.562500
INFO:root:train 050 2.360811e+00 8.486520 48.560049
INFO:root:train 100 2.361694e+00 7.858911 48.406559
INFO:root:train 150 2.359770e+00 7.947020 48.799669
INFO:root:train 200 2.356674e+00 8.061256 48.826182
INFO:root:train 250 2.356894e+00 8.049054 48.811006
INFO:root:train 300 2.357345e+00 8.087625 48.842400
INFO:root:train 350 2.357906e+00 8.026175 48.798077
INFO:root:train 400 2.358541e+00 8.011222 48.577774
INFO:root:train 450 2.359270e+00 7.944152 48.479074
INFO:root:train 500 2.359474e+00 7.984032 48.384481
INFO:root:train 550 2.359099e+00 7.962795 48.471529
INFO:root:train 600 2.358676e+00 8.023087 48.593490
INFO:root:train 650 2.358454e+00 8.040515 48.703917
INFO:root:train 700 2.357801e+00 8.095578 48.881063
INFO:root:train_acc 8.125000
INFO:root:valid 000 2.319118e+00 14.062500 45.312500
INFO:root:valid 050 2.329947e+00 9.528186 46.752451
INFO:root:valid 100 2.330048e+00 9.854579 46.952351
INFO:root:valid 150 2.331921e+00 9.861341 46.512831
INFO:root:valid_acc 9.950000
INFO:root:epoch 4 lr 0.000000e+00
INFO:root:train 000 2.404269e+00 1.562500 40.625000
INFO:root:train 050 2.357052e+00 8.762255 48.866422
INFO:root:train 100 2.358973e+00 8.616955 48.994431
INFO:root:train 150 2.358199e+00 8.588576 49.079056
INFO:root:train 200 2.358180e+00 8.620958 49.160448
INFO:root:train 250 2.359695e+00 8.360309 48.947958
INFO:root:train 300 2.359586e+00 8.508098 49.091570
INFO:root:train 350 2.360479e+00 8.435719 48.904915
INFO:root:train 400 2.360601e+00 8.474906 48.928460
INFO:root:train 450 2.360726e+00 8.501940 48.936391
INFO:root:train 500 2.360725e+00 8.523578 49.026946
INFO:root:train 550 2.361643e+00 8.541289 48.825998
INFO:root:train 600 2.361676e+00 8.595050 48.780678
INFO:root:train 650 2.361852e+00 8.551747 48.775922
INFO:root:train 700 2.362232e+00 8.476730 48.702746
INFO:root:train_acc 8.514583
INFO:root:valid 000 2.337858e+00 10.937500 37.500000
INFO:root:valid 050 2.329826e+00 10.232843 46.966912
INFO:root:valid 100 2.331798e+00 10.380569 46.024134
INFO:root:valid 150 2.333536e+00 10.264901 45.871275
INFO:root:valid_acc 10.408333
INFO:solver.bo_hb:Configuration achieved a performance of 2.332933 in 1343.589246 seconds
INFO:solver.bo_hb:Evaluate: [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.07050234e+00 4.34399304e+00 8.81319187e-02 6.01281106e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.11500850447919678, 'grad_clip_value': 8, 'initial_lr': 2.0853321148649926e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 1.0027728390489452e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 1.886201e-06
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.348537e+00 7.689951 49.264706
INFO:root:train 100 2.347499e+00 7.564975 48.452970
INFO:root:train 150 2.345417e+00 7.574503 48.396109
INFO:root:train 200 2.344059e+00 7.633706 48.243159
INFO:root:train 250 2.339929e+00 7.961902 48.923058
INFO:root:train 300 2.336257e+00 8.258929 49.454942
INFO:root:train 350 2.333087e+00 8.587073 49.915420
INFO:root:train 400 2.330233e+00 8.887936 50.214308
INFO:root:train 450 2.327416e+00 9.232955 50.592433
INFO:root:train 500 2.324798e+00 9.540294 50.938748
INFO:root:train 550 2.322128e+00 9.857078 51.256239
INFO:root:train 600 2.319594e+00 10.162750 51.492304
INFO:root:train 650 2.317553e+00 10.421467 51.795315
INFO:root:train 700 2.315290e+00 10.667796 52.081847
INFO:root:train_acc 10.956250
INFO:root:valid 000 2.302255e+00 10.937500 40.625000
INFO:root:valid 050 2.277716e+00 16.053922 55.575980
INFO:root:valid 100 2.277954e+00 15.717822 56.079827
INFO:root:valid 150 2.274895e+00 16.049255 56.757036
INFO:root:valid_acc 16.158333
INFO:root:epoch 1 lr 1.364868e-06
INFO:root:train 000 2.294455e+00 9.375000 54.687500
INFO:root:train 050 2.276939e+00 15.042892 56.893382
INFO:root:train 100 2.276540e+00 14.897896 57.209158
INFO:root:train 150 2.278241e+00 14.890315 56.674255
INFO:root:train 200 2.276835e+00 15.174129 56.871891
INFO:root:train 250 2.275691e+00 15.257719 57.140189
INFO:root:train 300 2.273830e+00 15.464078 57.179194
INFO:root:train 350 2.272488e+00 15.576033 57.380698
INFO:root:train 400 2.270587e+00 15.792550 57.656640
INFO:root:train 450 2.269259e+00 15.999169 57.774390
INFO:root:train 500 2.268357e+00 16.036677 57.955963
INFO:root:train 550 2.267574e+00 16.092899 58.152790
INFO:root:train 600 2.266029e+00 16.157966 58.491057
INFO:root:train 650 2.264601e+00 16.349846 58.717358
INFO:root:train 700 2.263681e+00 16.391762 58.926979
INFO:root:train_acc 16.506250
INFO:root:valid 000 2.284837e+00 14.062500 59.375000
INFO:root:valid 050 2.246844e+00 15.747549 61.305147
INFO:root:valid 100 2.246821e+00 16.444926 61.432550
INFO:root:valid 150 2.247257e+00 16.618377 61.268626
INFO:root:valid_acc 16.658333
INFO:root:epoch 2 lr 7.204645e-07
INFO:root:train 000 2.277139e+00 18.750000 59.375000
INFO:root:train 050 2.258562e+00 16.942402 58.884804
INFO:root:train 100 2.251628e+00 17.280322 59.978342
INFO:root:train 150 2.248865e+00 17.901490 60.233858
INFO:root:train 200 2.249254e+00 17.708333 60.354478
INFO:root:train 250 2.248996e+00 17.536106 60.427042
INFO:root:train 300 2.248312e+00 17.587209 60.392442
INFO:root:train 350 2.249730e+00 17.418981 60.269765
INFO:root:train 400 2.250209e+00 17.308292 60.294576
INFO:root:train 450 2.250237e+00 17.208287 60.203021
INFO:root:train 500 2.250313e+00 17.084581 60.188997
INFO:root:train 550 2.249531e+00 17.125113 60.256919
INFO:root:train 600 2.249087e+00 17.153702 60.240745
INFO:root:train 650 2.248357e+00 17.273906 60.320661
INFO:root:train 700 2.248216e+00 17.287803 60.346826
INFO:root:train_acc 17.316667
INFO:root:valid 000 2.233691e+00 14.062500 57.812500
INFO:root:valid 050 2.271565e+00 12.408088 56.893382
INFO:root:valid 100 2.259646e+00 13.180693 59.127475
INFO:root:valid 150 2.259154e+00 13.027732 59.395695
INFO:root:valid_acc 13.133333
INFO:root:epoch 3 lr 1.991315e-07
INFO:root:train 000 2.263326e+00 17.187500 54.687500
INFO:root:train 050 2.252494e+00 16.513480 60.202206
INFO:root:train 100 2.251568e+00 16.800743 60.179455
INFO:root:train 150 2.248530e+00 16.618377 60.606374
INFO:root:train 200 2.247113e+00 16.728856 60.867537
INFO:root:train 250 2.246461e+00 16.745518 60.912600
INFO:root:train 300 2.246802e+00 16.741071 61.150332
INFO:root:train 350 2.247513e+00 16.742343 61.053241
INFO:root:train 400 2.248631e+00 16.638092 60.918017
INFO:root:train 450 2.248988e+00 16.612389 60.888997
INFO:root:train 500 2.249320e+00 16.613648 60.828343
INFO:root:train 550 2.249153e+00 16.679900 60.835413
INFO:root:train 600 2.248328e+00 16.750728 60.877704
INFO:root:train 650 2.248019e+00 16.841878 60.807892
INFO:root:train 700 2.247216e+00 16.960146 60.955332
INFO:root:train_acc 16.975000
INFO:root:valid 000 2.210577e+00 15.625000 60.937500
INFO:root:valid 050 2.267857e+00 12.162990 56.924020
INFO:root:valid 100 2.270580e+00 11.618193 56.698639
INFO:root:valid 150 2.272698e+00 11.351407 57.026076
INFO:root:valid_acc 11.400000
INFO:root:epoch 4 lr 0.000000e+00
INFO:root:train 000 2.283333e+00 14.062500 51.562500
INFO:root:train 050 2.248314e+00 16.544118 59.436275
INFO:root:train 100 2.251167e+00 16.630569 59.854579
INFO:root:train 150 2.251473e+00 16.659768 59.933775
INFO:root:train 200 2.252407e+00 16.627799 59.996891
INFO:root:train 250 2.252814e+00 16.459163 59.873008
INFO:root:train 300 2.252709e+00 16.533430 59.987542
INFO:root:train 350 2.252987e+00 16.631054 59.824608
INFO:root:train 400 2.252270e+00 16.801746 59.901029
INFO:root:train 450 2.252215e+00 16.851441 59.960504
INFO:root:train 500 2.252432e+00 16.863149 60.020584
INFO:root:train 550 2.253719e+00 16.762137 59.854242
INFO:root:train 600 2.254083e+00 16.748128 59.848170
INFO:root:train 650 2.255124e+00 16.575461 59.718222
INFO:root:train 700 2.254882e+00 16.576765 59.745007
INFO:root:train_acc 16.518750
INFO:root:valid 000 2.272024e+00 12.500000 50.000000
INFO:root:valid 050 2.286796e+00 11.060049 55.238971
INFO:root:valid 100 2.294586e+00 10.102104 55.043317
INFO:root:valid 150 2.293473e+00 10.347682 55.411838
INFO:root:valid_acc 10.450000
INFO:solver.bo_hb:Configuration achieved a performance of 2.292099 in 1104.431350 seconds
INFO:solver.bo_hb:Start iteration 3 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/aml-project/PC-DARTS/models/DNGO.py:157: RuntimeWarning: invalid value encountered in true_divide
  x_nom=(x-mx_mat)/sx_mat
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.
  warnings.warn(warning.format(ret))
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/home/rkohli/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py:2093: RuntimeWarning: invalid value encountered in det
  r = _umath_linalg.det(a, signature=signature)
INFO:solver.bo_hb:Time to train the model: 41.832344
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 1.017678
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351063e+00 7.383578 48.866422
INFO:root:train 100 2.352568e+00 7.255569 47.633045
INFO:root:train 150 2.353081e+00 7.046772 47.257864
INFO:root:train 200 2.354450e+00 6.934080 46.867226
INFO:root:train 250 2.352816e+00 6.978337 47.236056
INFO:root:train 300 2.351591e+00 7.059801 47.586171
INFO:root:train 350 2.350733e+00 7.055734 47.858796
INFO:root:train 400 2.350089e+00 7.056577 48.004988
INFO:root:train 450 2.349537e+00 7.091879 48.129157
INFO:root:train 500 2.349332e+00 7.076472 48.159930
INFO:root:train 550 2.348893e+00 7.072368 48.128403
INFO:root:train 600 2.348459e+00 7.115745 48.073523
INFO:root:train 650 2.348802e+00 7.070853 48.058276
INFO:root:train 700 2.348614e+00 7.047967 48.087553
INFO:root:train_acc 7.087500
INFO:root:valid 000 2.380721e+00 6.250000 32.812500
INFO:root:valid 050 2.346317e+00 7.598039 48.039216
INFO:root:valid 100 2.347741e+00 7.240099 48.004332
INFO:root:valid 150 2.345104e+00 7.367550 48.489238
INFO:root:valid_acc 7.333333
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357476e+00 10.937500 51.562500
INFO:root:train 050 2.349706e+00 8.118873 49.325980
INFO:root:train 100 2.350467e+00 7.920792 49.102723
INFO:root:train 150 2.350751e+00 7.926325 48.561672
INFO:root:train 200 2.350569e+00 7.960199 48.662935
INFO:root:train 250 2.350051e+00 8.011703 48.605578
INFO:root:train 300 2.350138e+00 8.098007 48.686669
INFO:root:train 350 2.349772e+00 8.137464 48.660078
INFO:root:train 400 2.349552e+00 8.194358 48.741428
INFO:root:train 450 2.349887e+00 8.155488 48.742378
INFO:root:train 500 2.349966e+00 8.074476 48.902196
INFO:root:train 550 2.350175e+00 8.047868 48.800476
INFO:root:train 600 2.349767e+00 8.129680 48.913270
INFO:root:train 650 2.349614e+00 8.090918 48.941532
INFO:root:train 700 2.349527e+00 8.091120 48.956847
INFO:root:train_acc 8.070833
INFO:root:valid 000 2.346884e+00 6.250000 45.312500
INFO:root:valid 050 2.325966e+00 9.313725 49.142157
INFO:root:valid 100 2.331984e+00 8.787129 47.633045
INFO:root:valid 150 2.331284e+00 8.847268 47.847682
INFO:root:valid_acc 8.691667
INFO:solver.bo_hb:Configuration achieved a performance of 2.331947 
INFO:solver.bo_hb:Evaluation of this configuration took 384.153588 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 4 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 39.240924
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.648300
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351063e+00 7.383578 48.866422
INFO:root:train 100 2.352568e+00 7.255569 47.633045
INFO:root:train 150 2.353081e+00 7.057119 47.247517
INFO:root:train 200 2.354451e+00 6.941853 46.867226
INFO:root:train 250 2.352816e+00 6.984562 47.236056
INFO:root:train 300 2.351593e+00 7.070183 47.575789
INFO:root:train 350 2.350734e+00 7.064637 47.858796
INFO:root:train 400 2.350090e+00 7.064370 48.008884
INFO:root:train 450 2.349539e+00 7.098808 48.122228
INFO:root:train 500 2.349333e+00 7.088947 48.153693
INFO:root:train 550 2.348894e+00 7.083711 48.122731
INFO:root:train 600 2.348460e+00 7.126144 48.070923
INFO:root:train 650 2.348802e+00 7.078053 48.055876
INFO:root:train 700 2.348614e+00 7.054654 48.074180
INFO:root:train_acc 7.093750
INFO:root:valid 000 2.380492e+00 6.250000 32.812500
INFO:root:valid 050 2.346299e+00 7.628676 48.008578
INFO:root:valid 100 2.347720e+00 7.271040 48.035272
INFO:root:valid 150 2.345089e+00 7.398593 48.458195
INFO:root:valid_acc 7.350000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357436e+00 9.375000 51.562500
INFO:root:train 050 2.349701e+00 8.088235 49.325980
INFO:root:train 100 2.350472e+00 7.920792 49.087252
INFO:root:train 150 2.350750e+00 7.957368 48.561672
INFO:root:train 200 2.350566e+00 7.967973 48.670709
INFO:root:train 250 2.350046e+00 8.017928 48.568227
INFO:root:train 300 2.350136e+00 8.103198 48.645141
INFO:root:train 350 2.349769e+00 8.137464 48.628917
INFO:root:train 400 2.349552e+00 8.194358 48.698566
INFO:root:train 450 2.349887e+00 8.141630 48.704268
INFO:root:train 500 2.349964e+00 8.058882 48.852295
INFO:root:train 550 2.350176e+00 8.028017 48.757940
INFO:root:train 600 2.349769e+00 8.108881 48.876872
INFO:root:train 650 2.349615e+00 8.071717 48.903130
INFO:root:train 700 2.349527e+00 8.075517 48.925642
INFO:root:train_acc 8.056250
INFO:root:valid 000 2.346928e+00 6.250000 43.750000
INFO:root:valid 050 2.325969e+00 9.313725 49.019608
INFO:root:valid 100 2.332001e+00 8.787129 47.524752
INFO:root:valid 150 2.331295e+00 8.857616 47.733858
INFO:root:valid_acc 8.725000
INFO:solver.bo_hb:Configuration achieved a performance of 2.331952 
INFO:solver.bo_hb:Evaluation of this configuration took 383.743464 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 5 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 39.696871
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.651177
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351063e+00 7.383578 48.866422
INFO:root:train 100 2.352567e+00 7.255569 47.633045
INFO:root:train 150 2.353080e+00 7.046772 47.247517
INFO:root:train 200 2.354450e+00 6.934080 46.859453
INFO:root:train 250 2.352814e+00 6.978337 47.229831
INFO:root:train 300 2.351590e+00 7.070183 47.570598
INFO:root:train 350 2.350732e+00 7.064637 47.854345
INFO:root:train 400 2.350088e+00 7.060474 48.001091
INFO:root:train 450 2.349536e+00 7.102273 48.118764
INFO:root:train 500 2.349330e+00 7.085828 48.153693
INFO:root:train 550 2.348892e+00 7.078040 48.122731
INFO:root:train 600 2.348458e+00 7.118344 48.065724
INFO:root:train 650 2.348802e+00 7.073253 48.046275
INFO:root:train 700 2.348615e+00 7.047967 48.060806
INFO:root:train_acc 7.085417
INFO:root:valid 000 2.380898e+00 6.250000 32.812500
INFO:root:valid 050 2.346348e+00 7.628676 47.947304
INFO:root:valid 100 2.347745e+00 7.286510 47.865099
INFO:root:valid 150 2.345106e+00 7.398593 48.385762
INFO:root:valid_acc 7.366667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357613e+00 10.937500 51.562500
INFO:root:train 050 2.349719e+00 8.026961 49.448529
INFO:root:train 100 2.350473e+00 7.889851 49.211015
INFO:root:train 150 2.350755e+00 7.926325 48.613411
INFO:root:train 200 2.350576e+00 7.983520 48.678483
INFO:root:train 250 2.350067e+00 8.042829 48.624253
INFO:root:train 300 2.350157e+00 8.123962 48.681478
INFO:root:train 350 2.349794e+00 8.150819 48.664530
INFO:root:train 400 2.349570e+00 8.198254 48.721945
INFO:root:train 450 2.349909e+00 8.152023 48.721591
INFO:root:train 500 2.349990e+00 8.071357 48.874127
INFO:root:train 550 2.350194e+00 8.042196 48.772119
INFO:root:train 600 2.349783e+00 8.124480 48.887271
INFO:root:train 650 2.349628e+00 8.098118 48.907930
INFO:root:train 700 2.349540e+00 8.097807 48.932329
INFO:root:train_acc 8.077083
INFO:root:valid 000 2.346917e+00 6.250000 45.312500
INFO:root:valid 050 2.325959e+00 9.344363 49.050245
INFO:root:valid 100 2.331966e+00 8.787129 47.431931
INFO:root:valid 150 2.331273e+00 8.836921 47.651076
INFO:root:valid_acc 8.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.331944 
INFO:solver.bo_hb:Evaluation of this configuration took 383.720708 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 6 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 39.897612
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.728632
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351063e+00 7.383578 48.866422
INFO:root:train 100 2.352568e+00 7.255569 47.633045
INFO:root:train 150 2.353080e+00 7.046772 47.247517
INFO:root:train 200 2.354450e+00 6.934080 46.867226
INFO:root:train 250 2.352815e+00 6.978337 47.229831
INFO:root:train 300 2.351590e+00 7.070183 47.575789
INFO:root:train 350 2.350729e+00 7.064637 47.858796
INFO:root:train 400 2.350086e+00 7.068267 48.004988
INFO:root:train 450 2.349533e+00 7.109202 48.122228
INFO:root:train 500 2.349328e+00 7.085828 48.147455
INFO:root:train 550 2.348889e+00 7.078040 48.119896
INFO:root:train 600 2.348457e+00 7.118344 48.068324
INFO:root:train 650 2.348803e+00 7.068452 48.051075
INFO:root:train 700 2.348616e+00 7.043509 48.067493
INFO:root:train_acc 7.085417
INFO:root:valid 000 2.380766e+00 6.250000 32.812500
INFO:root:valid 050 2.346309e+00 7.628676 47.977941
INFO:root:valid 100 2.347737e+00 7.255569 47.973391
INFO:root:valid 150 2.345112e+00 7.388245 48.458195
INFO:root:valid_acc 7.400000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357603e+00 9.375000 50.000000
INFO:root:train 050 2.349670e+00 8.057598 49.387255
INFO:root:train 100 2.350439e+00 7.889851 49.087252
INFO:root:train 150 2.350744e+00 7.936672 48.540977
INFO:root:train 200 2.350570e+00 7.967973 48.608520
INFO:root:train 250 2.350048e+00 8.017928 48.555777
INFO:root:train 300 2.350139e+00 8.103198 48.639950
INFO:root:train 350 2.349776e+00 8.146368 48.637821
INFO:root:train 400 2.349554e+00 8.194358 48.718049
INFO:root:train 450 2.349891e+00 8.152023 48.711197
INFO:root:train 500 2.349974e+00 8.071357 48.864770
INFO:root:train 550 2.350180e+00 8.036525 48.763612
INFO:root:train 600 2.349770e+00 8.121880 48.876872
INFO:root:train 650 2.349620e+00 8.088518 48.883929
INFO:root:train 700 2.349531e+00 8.091120 48.898894
INFO:root:train_acc 8.070833
INFO:root:valid 000 2.346910e+00 6.250000 45.312500
INFO:root:valid 050 2.325966e+00 9.313725 49.080882
INFO:root:valid 100 2.331978e+00 8.787129 47.555693
INFO:root:valid 150 2.331276e+00 8.867964 47.764901
INFO:root:valid_acc 8.708333
INFO:solver.bo_hb:Configuration achieved a performance of 2.331943 
INFO:solver.bo_hb:Evaluation of this configuration took 384.496136 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 7 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 40.149245
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.640686
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351062e+00 7.383578 48.866422
INFO:root:train 100 2.352567e+00 7.255569 47.617574
INFO:root:train 150 2.353081e+00 7.057119 47.237169
INFO:root:train 200 2.354450e+00 6.941853 46.859453
INFO:root:train 250 2.352815e+00 6.984562 47.223606
INFO:root:train 300 2.351590e+00 7.080565 47.555025
INFO:root:train 350 2.350733e+00 7.073540 47.832087
INFO:root:train 400 2.350089e+00 7.072163 47.985505
INFO:root:train 450 2.349539e+00 7.116131 48.108370
INFO:root:train 500 2.349333e+00 7.092066 48.138099
INFO:root:train 550 2.348896e+00 7.086547 48.105717
INFO:root:train 600 2.348462e+00 7.123544 48.060524
INFO:root:train 650 2.348806e+00 7.075653 48.036674
INFO:root:train 700 2.348618e+00 7.050196 48.051890
INFO:root:train_acc 7.089583
INFO:root:valid 000 2.380826e+00 6.250000 32.812500
INFO:root:valid 050 2.346325e+00 7.628676 48.039216
INFO:root:valid 100 2.347747e+00 7.255569 47.957921
INFO:root:valid 150 2.345110e+00 7.377897 48.458195
INFO:root:valid_acc 7.383333
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357414e+00 10.937500 51.562500
INFO:root:train 050 2.349700e+00 8.088235 49.356618
INFO:root:train 100 2.350462e+00 7.905322 49.149134
INFO:root:train 150 2.350755e+00 7.936672 48.582368
INFO:root:train 200 2.350576e+00 7.975746 48.655162
INFO:root:train 250 2.350063e+00 8.030378 48.568227
INFO:root:train 300 2.350152e+00 8.108389 48.660714
INFO:root:train 350 2.349786e+00 8.141916 48.633369
INFO:root:train 400 2.349566e+00 8.194358 48.718049
INFO:root:train 450 2.349903e+00 8.152023 48.721591
INFO:root:train 500 2.349983e+00 8.071357 48.889721
INFO:root:train 550 2.350189e+00 8.036525 48.777790
INFO:root:train 600 2.349779e+00 8.121880 48.889871
INFO:root:train 650 2.349626e+00 8.086118 48.907930
INFO:root:train 700 2.349538e+00 8.084433 48.925642
INFO:root:train_acc 8.064583
INFO:root:valid 000 2.346903e+00 6.250000 45.312500
INFO:root:valid 050 2.325956e+00 9.344363 49.050245
INFO:root:valid 100 2.331966e+00 8.818069 47.524752
INFO:root:valid 150 2.331278e+00 8.878311 47.733858
INFO:root:valid_acc 8.700000
INFO:solver.bo_hb:Configuration achieved a performance of 2.331947 
INFO:solver.bo_hb:Evaluation of this configuration took 385.170450 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 8 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 43.099477
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.681679
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351062e+00 7.383578 48.866422
INFO:root:train 100 2.352568e+00 7.255569 47.617574
INFO:root:train 150 2.353080e+00 7.046772 47.237169
INFO:root:train 200 2.354449e+00 6.934080 46.851679
INFO:root:train 250 2.352815e+00 6.978337 47.229831
INFO:root:train 300 2.351590e+00 7.064992 47.586171
INFO:root:train 350 2.350731e+00 7.064637 47.858796
INFO:root:train 400 2.350087e+00 7.068267 48.012781
INFO:root:train 450 2.349535e+00 7.102273 48.125693
INFO:root:train 500 2.349331e+00 7.076472 48.150574
INFO:root:train 550 2.348893e+00 7.072368 48.119896
INFO:root:train 600 2.348459e+00 7.120944 48.065724
INFO:root:train 650 2.348803e+00 7.075653 48.046275
INFO:root:train 700 2.348617e+00 7.047967 48.071951
INFO:root:train_acc 7.087500
INFO:root:valid 000 2.380942e+00 6.250000 32.812500
INFO:root:valid 050 2.346351e+00 7.628676 48.008578
INFO:root:valid 100 2.347743e+00 7.255569 47.911510
INFO:root:valid 150 2.345100e+00 7.377897 48.406457
INFO:root:valid_acc 7.366667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357486e+00 10.937500 51.562500
INFO:root:train 050 2.349694e+00 8.026961 49.325980
INFO:root:train 100 2.350468e+00 7.905322 49.102723
INFO:root:train 150 2.350745e+00 7.926325 48.551325
INFO:root:train 200 2.350568e+00 7.960199 48.647388
INFO:root:train 250 2.350055e+00 8.017928 48.580677
INFO:root:train 300 2.350148e+00 8.087625 48.665905
INFO:root:train 350 2.349782e+00 8.119658 48.642272
INFO:root:train 400 2.349562e+00 8.170979 48.714152
INFO:root:train 450 2.349900e+00 8.127772 48.731984
INFO:root:train 500 2.349981e+00 8.049526 48.864770
INFO:root:train 550 2.350188e+00 8.019510 48.772119
INFO:root:train 600 2.349780e+00 8.106281 48.887271
INFO:root:train 650 2.349627e+00 8.074117 48.907930
INFO:root:train 700 2.349539e+00 8.075517 48.934558
INFO:root:train_acc 8.056250
INFO:root:valid 000 2.346797e+00 6.250000 45.312500
INFO:root:valid 050 2.325950e+00 9.344363 49.111520
INFO:root:valid 100 2.331968e+00 8.818069 47.540223
INFO:root:valid 150 2.331275e+00 8.867964 47.754553
INFO:root:valid_acc 8.708333
INFO:solver.bo_hb:Configuration achieved a performance of 2.331943 
INFO:solver.bo_hb:Evaluation of this configuration took 382.481895 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 9 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 42.185523
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.786642
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351063e+00 7.383578 48.866422
INFO:root:train 100 2.352568e+00 7.255569 47.602104
INFO:root:train 150 2.353080e+00 7.057119 47.226821
INFO:root:train 200 2.354449e+00 6.941853 46.843905
INFO:root:train 250 2.352814e+00 6.984562 47.198705
INFO:root:train 300 2.351590e+00 7.070183 47.560216
INFO:root:train 350 2.350734e+00 7.064637 47.840990
INFO:root:train 400 2.350090e+00 7.060474 47.985505
INFO:root:train 450 2.349538e+00 7.095344 48.097977
INFO:root:train 500 2.349332e+00 7.073353 48.128743
INFO:root:train 550 2.348894e+00 7.069533 48.097210
INFO:root:train 600 2.348460e+00 7.113145 48.047525
INFO:root:train 650 2.348802e+00 7.066052 48.031874
INFO:root:train 700 2.348614e+00 7.043509 48.058577
INFO:root:train_acc 7.089583
INFO:root:valid 000 2.380615e+00 6.250000 32.812500
INFO:root:valid 050 2.346284e+00 7.628676 48.069853
INFO:root:valid 100 2.347714e+00 7.255569 48.019802
INFO:root:valid 150 2.345087e+00 7.377897 48.447848
INFO:root:valid_acc 7.341667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357533e+00 10.937500 51.562500
INFO:root:train 050 2.349707e+00 8.057598 49.509804
INFO:root:train 100 2.350465e+00 7.905322 49.195545
INFO:root:train 150 2.350747e+00 7.947020 48.613411
INFO:root:train 200 2.350565e+00 7.975746 48.662935
INFO:root:train 250 2.350051e+00 8.030378 48.580677
INFO:root:train 300 2.350141e+00 8.108389 48.671096
INFO:root:train 350 2.349774e+00 8.137464 48.651175
INFO:root:train 400 2.349553e+00 8.194358 48.721945
INFO:root:train 450 2.349887e+00 8.145094 48.718126
INFO:root:train 500 2.349967e+00 8.065120 48.867889
INFO:root:train 550 2.350177e+00 8.036525 48.774955
INFO:root:train 600 2.349768e+00 8.119280 48.889871
INFO:root:train 650 2.349616e+00 8.086118 48.915131
INFO:root:train 700 2.349527e+00 8.088891 48.932329
INFO:root:train_acc 8.068750
INFO:root:valid 000 2.347037e+00 6.250000 43.750000
INFO:root:valid 050 2.325980e+00 9.313725 48.927696
INFO:root:valid 100 2.331993e+00 8.771658 47.478342
INFO:root:valid 150 2.331286e+00 8.847268 47.713162
INFO:root:valid_acc 8.683333
INFO:solver.bo_hb:Configuration achieved a performance of 2.331951 
INFO:solver.bo_hb:Evaluation of this configuration took 382.141204 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 10 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 42.937550
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.733595
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351063e+00 7.383578 48.866422
INFO:root:train 100 2.352568e+00 7.255569 47.633045
INFO:root:train 150 2.353080e+00 7.057119 47.247517
INFO:root:train 200 2.354450e+00 6.941853 46.859453
INFO:root:train 250 2.352816e+00 6.984562 47.236056
INFO:root:train 300 2.351592e+00 7.075374 47.575789
INFO:root:train 350 2.350733e+00 7.069088 47.849893
INFO:root:train 400 2.350090e+00 7.068267 48.001091
INFO:root:train 450 2.349537e+00 7.105737 48.129157
INFO:root:train 500 2.349331e+00 7.085828 48.153693
INFO:root:train 550 2.348893e+00 7.078040 48.119896
INFO:root:train 600 2.348458e+00 7.123544 48.065724
INFO:root:train 650 2.348802e+00 7.075653 48.046275
INFO:root:train 700 2.348613e+00 7.052425 48.067493
INFO:root:train_acc 7.093750
INFO:root:valid 000 2.380784e+00 6.250000 32.812500
INFO:root:valid 050 2.346326e+00 7.689951 48.039216
INFO:root:valid 100 2.347746e+00 7.301980 47.926980
INFO:root:valid 150 2.345114e+00 7.408940 48.385762
INFO:root:valid_acc 7.400000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357490e+00 10.937500 51.562500
INFO:root:train 050 2.349697e+00 7.935049 49.387255
INFO:root:train 100 2.350459e+00 7.843441 49.071782
INFO:root:train 150 2.350746e+00 7.884934 48.509934
INFO:root:train 200 2.350568e+00 7.944652 48.592973
INFO:root:train 250 2.350055e+00 7.993028 48.537102
INFO:root:train 300 2.350143e+00 8.077243 48.624377
INFO:root:train 350 2.349777e+00 8.101852 48.615563
INFO:root:train 400 2.349556e+00 8.159289 48.690773
INFO:root:train 450 2.349893e+00 8.117378 48.686946
INFO:root:train 500 2.349975e+00 8.040170 48.852295
INFO:root:train 550 2.350181e+00 8.011003 48.752269
INFO:root:train 600 2.349771e+00 8.098482 48.869072
INFO:root:train 650 2.349619e+00 8.069316 48.888729
INFO:root:train 700 2.349531e+00 8.071059 48.912268
INFO:root:train_acc 8.052083
INFO:root:valid 000 2.346901e+00 6.250000 45.312500
INFO:root:valid 050 2.325964e+00 9.313725 49.019608
INFO:root:valid 100 2.331974e+00 8.787129 47.540223
INFO:root:valid 150 2.331274e+00 8.857616 47.744205
INFO:root:valid_acc 8.691667
INFO:solver.bo_hb:Configuration achieved a performance of 2.331947 
INFO:solver.bo_hb:Evaluation of this configuration took 382.687708 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 11 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 99.479262
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.647223
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351063e+00 7.352941 48.866422
INFO:root:train 100 2.352568e+00 7.240099 47.617574
INFO:root:train 150 2.353081e+00 7.046772 47.237169
INFO:root:train 200 2.354451e+00 6.934080 46.851679
INFO:root:train 250 2.352815e+00 6.978337 47.223606
INFO:root:train 300 2.351591e+00 7.070183 47.570598
INFO:root:train 350 2.350732e+00 7.064637 47.849893
INFO:root:train 400 2.350089e+00 7.064370 48.001091
INFO:root:train 450 2.349537e+00 7.102273 48.125693
INFO:root:train 500 2.349331e+00 7.085828 48.150574
INFO:root:train 550 2.348893e+00 7.080876 48.117060
INFO:root:train 600 2.348460e+00 7.120944 48.063124
INFO:root:train 650 2.348803e+00 7.073253 48.041475
INFO:root:train 700 2.348615e+00 7.045738 48.063035
INFO:root:train_acc 7.083333
INFO:root:valid 000 2.380636e+00 6.250000 32.812500
INFO:root:valid 050 2.346304e+00 7.628676 47.977941
INFO:root:valid 100 2.347733e+00 7.240099 47.973391
INFO:root:valid 150 2.345102e+00 7.367550 48.447848
INFO:root:valid_acc 7.350000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357379e+00 9.375000 51.562500
INFO:root:train 050 2.349675e+00 8.057598 49.387255
INFO:root:train 100 2.350451e+00 7.889851 49.133663
INFO:root:train 150 2.350746e+00 7.936672 48.572020
INFO:root:train 200 2.350567e+00 7.967973 48.655162
INFO:root:train 250 2.350047e+00 8.024153 48.568227
INFO:root:train 300 2.350137e+00 8.103198 48.665905
INFO:root:train 350 2.349773e+00 8.141916 48.655627
INFO:root:train 400 2.349552e+00 8.198254 48.721945
INFO:root:train 450 2.349889e+00 8.152023 48.721591
INFO:root:train 500 2.349968e+00 8.071357 48.867889
INFO:root:train 550 2.350176e+00 8.039360 48.769283
INFO:root:train 600 2.349768e+00 8.121880 48.892471
INFO:root:train 650 2.349616e+00 8.083717 48.919931
INFO:root:train 700 2.349528e+00 8.086662 48.936787
INFO:root:train_acc 8.066667
INFO:root:valid 000 2.346914e+00 6.250000 43.750000
INFO:root:valid 050 2.325956e+00 9.283088 49.050245
INFO:root:valid 100 2.331984e+00 8.740718 47.540223
INFO:root:valid 150 2.331285e+00 8.816225 47.744205
INFO:root:valid_acc 8.675000
INFO:solver.bo_hb:Configuration achieved a performance of 2.331945 
INFO:solver.bo_hb:Evaluation of this configuration took 399.967836 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 12 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 48.162626
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.771616
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351063e+00 7.383578 48.866422
INFO:root:train 100 2.352568e+00 7.255569 47.617574
INFO:root:train 150 2.353080e+00 7.046772 47.247517
INFO:root:train 200 2.354450e+00 6.934080 46.859453
INFO:root:train 250 2.352816e+00 6.978337 47.229831
INFO:root:train 300 2.351590e+00 7.070183 47.570598
INFO:root:train 350 2.350732e+00 7.060185 47.849893
INFO:root:train 400 2.350089e+00 7.056577 47.997195
INFO:root:train 450 2.349536e+00 7.098808 48.122228
INFO:root:train 500 2.349330e+00 7.076472 48.147455
INFO:root:train 550 2.348891e+00 7.072368 48.111388
INFO:root:train 600 2.348458e+00 7.113145 48.060524
INFO:root:train 650 2.348802e+00 7.063652 48.036674
INFO:root:train 700 2.348614e+00 7.039051 48.060806
INFO:root:train_acc 7.081250
INFO:root:valid 000 2.380635e+00 6.250000 32.812500
INFO:root:valid 050 2.346299e+00 7.659314 47.947304
INFO:root:valid 100 2.347734e+00 7.255569 47.911510
INFO:root:valid 150 2.345112e+00 7.377897 48.396109
INFO:root:valid_acc 7.375000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357554e+00 9.375000 51.562500
INFO:root:train 050 2.349690e+00 8.026961 49.417892
INFO:root:train 100 2.350451e+00 7.889851 49.087252
INFO:root:train 150 2.350743e+00 7.915977 48.530629
INFO:root:train 200 2.350565e+00 7.960199 48.616294
INFO:root:train 250 2.350047e+00 8.017928 48.562002
INFO:root:train 300 2.350137e+00 8.108389 48.655523
INFO:root:train 350 2.349770e+00 8.141916 48.642272
INFO:root:train 400 2.349547e+00 8.190461 48.725842
INFO:root:train 450 2.349885e+00 8.141630 48.721591
INFO:root:train 500 2.349967e+00 8.062001 48.883483
INFO:root:train 550 2.350174e+00 8.033689 48.786298
INFO:root:train 600 2.349764e+00 8.119280 48.900270
INFO:root:train 650 2.349612e+00 8.081317 48.912730
INFO:root:train 700 2.349525e+00 8.082204 48.925642
INFO:root:train_acc 8.062500
INFO:root:valid 000 2.346964e+00 6.250000 45.312500
INFO:root:valid 050 2.325971e+00 9.344363 49.050245
INFO:root:valid 100 2.331982e+00 8.802599 47.555693
INFO:root:valid 150 2.331282e+00 8.878311 47.754553
INFO:root:valid_acc 8.716667
INFO:solver.bo_hb:Configuration achieved a performance of 2.331947 
INFO:solver.bo_hb:Evaluation of this configuration took 381.718559 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 13 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 65.870921
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.647449
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351063e+00 7.383578 48.866422
INFO:root:train 100 2.352568e+00 7.255569 47.633045
INFO:root:train 150 2.353081e+00 7.057119 47.237169
INFO:root:train 200 2.354449e+00 6.941853 46.851679
INFO:root:train 250 2.352815e+00 6.984562 47.223606
INFO:root:train 300 2.351590e+00 7.070183 47.575789
INFO:root:train 350 2.350730e+00 7.064637 47.854345
INFO:root:train 400 2.350087e+00 7.060474 48.004988
INFO:root:train 450 2.349535e+00 7.091879 48.125693
INFO:root:train 500 2.349331e+00 7.073353 48.153693
INFO:root:train 550 2.348892e+00 7.069533 48.125567
INFO:root:train 600 2.348459e+00 7.113145 48.070923
INFO:root:train 650 2.348803e+00 7.068452 48.053475
INFO:root:train 700 2.348616e+00 7.045738 48.071951
INFO:root:train_acc 7.085417
INFO:root:valid 000 2.380900e+00 6.250000 32.812500
INFO:root:valid 050 2.346330e+00 7.628676 47.947304
INFO:root:valid 100 2.347740e+00 7.255569 47.896040
INFO:root:valid 150 2.345109e+00 7.377897 48.416805
INFO:root:valid_acc 7.366667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357480e+00 9.375000 51.562500
INFO:root:train 050 2.349680e+00 8.026961 49.295343
INFO:root:train 100 2.350464e+00 7.874381 49.056312
INFO:root:train 150 2.350752e+00 7.905629 48.520281
INFO:root:train 200 2.350570e+00 7.944652 48.639614
INFO:root:train 250 2.350053e+00 7.993028 48.562002
INFO:root:train 300 2.350146e+00 8.077243 48.655523
INFO:root:train 350 2.349781e+00 8.124110 48.628917
INFO:root:train 400 2.349558e+00 8.174875 48.714152
INFO:root:train 450 2.349897e+00 8.127772 48.728520
INFO:root:train 500 2.349978e+00 8.049526 48.874127
INFO:root:train 550 2.350183e+00 8.013838 48.777790
INFO:root:train 600 2.349772e+00 8.098482 48.887271
INFO:root:train 650 2.349620e+00 8.062116 48.912730
INFO:root:train 700 2.349533e+00 8.064372 48.930100
INFO:root:train_acc 8.045833
INFO:root:valid 000 2.346809e+00 6.250000 45.312500
INFO:root:valid 050 2.325936e+00 9.283088 49.080882
INFO:root:valid 100 2.331966e+00 8.787129 47.524752
INFO:root:valid 150 2.331272e+00 8.867964 47.764901
INFO:root:valid_acc 8.691667
INFO:solver.bo_hb:Configuration achieved a performance of 2.331941 
INFO:solver.bo_hb:Evaluation of this configuration took 381.429898 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 14 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 48.156388
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 1.202044
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351063e+00 7.383578 48.866422
INFO:root:train 100 2.352568e+00 7.255569 47.617574
INFO:root:train 150 2.353081e+00 7.046772 47.247517
INFO:root:train 200 2.354449e+00 6.934080 46.875000
INFO:root:train 250 2.352816e+00 6.978337 47.223606
INFO:root:train 300 2.351590e+00 7.075374 47.570598
INFO:root:train 350 2.350731e+00 7.069088 47.840990
INFO:root:train 400 2.350089e+00 7.068267 47.985505
INFO:root:train 450 2.349537e+00 7.109202 48.111835
INFO:root:train 500 2.349331e+00 7.079591 48.144336
INFO:root:train 550 2.348893e+00 7.069533 48.111388
INFO:root:train 600 2.348459e+00 7.110545 48.060524
INFO:root:train 650 2.348803e+00 7.061252 48.046275
INFO:root:train 700 2.348614e+00 7.036822 48.069722
INFO:root:train_acc 7.081250
INFO:root:valid 000 2.380506e+00 6.250000 32.812500
INFO:root:valid 050 2.346256e+00 7.628676 48.008578
INFO:root:valid 100 2.347704e+00 7.255569 48.004332
INFO:root:valid 150 2.345088e+00 7.398593 48.468543
INFO:root:valid_acc 7.391667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357460e+00 9.375000 50.000000
INFO:root:train 050 2.349699e+00 7.996324 49.387255
INFO:root:train 100 2.350468e+00 7.874381 49.164604
INFO:root:train 150 2.350751e+00 7.895281 48.603063
INFO:root:train 200 2.350575e+00 7.929104 48.647388
INFO:root:train 250 2.350049e+00 7.993028 48.562002
INFO:root:train 300 2.350137e+00 8.082434 48.639950
INFO:root:train 350 2.349772e+00 8.119658 48.633369
INFO:root:train 400 2.349551e+00 8.178772 48.725842
INFO:root:train 450 2.349886e+00 8.131236 48.725055
INFO:root:train 500 2.349969e+00 8.052645 48.877246
INFO:root:train 550 2.350177e+00 8.025181 48.786298
INFO:root:train 600 2.349768e+00 8.108881 48.900270
INFO:root:train 650 2.349616e+00 8.074117 48.919931
INFO:root:train 700 2.349528e+00 8.077746 48.936787
INFO:root:train_acc 8.058333
INFO:root:valid 000 2.347088e+00 6.250000 45.312500
INFO:root:valid 050 2.325979e+00 9.375000 48.988971
INFO:root:valid 100 2.331987e+00 8.787129 47.493812
INFO:root:valid 150 2.331287e+00 8.867964 47.713162
INFO:root:valid_acc 8.708333
INFO:solver.bo_hb:Configuration achieved a performance of 2.331947 
INFO:solver.bo_hb:Evaluation of this configuration took 380.494022 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 15 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 42.785883
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.687815
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351063e+00 7.383578 48.866422
INFO:root:train 100 2.352568e+00 7.255569 47.633045
INFO:root:train 150 2.353081e+00 7.046772 47.257864
INFO:root:train 200 2.354450e+00 6.934080 46.867226
INFO:root:train 250 2.352814e+00 6.978337 47.229831
INFO:root:train 300 2.351590e+00 7.070183 47.570598
INFO:root:train 350 2.350733e+00 7.064637 47.858796
INFO:root:train 400 2.350090e+00 7.064370 48.004988
INFO:root:train 450 2.349540e+00 7.105737 48.129157
INFO:root:train 500 2.349334e+00 7.082710 48.153693
INFO:root:train 550 2.348896e+00 7.078040 48.125567
INFO:root:train 600 2.348463e+00 7.118344 48.081323
INFO:root:train 650 2.348807e+00 7.070853 48.065476
INFO:root:train 700 2.348617e+00 7.045738 48.074180
INFO:root:train_acc 7.085417
INFO:root:valid 000 2.380505e+00 6.250000 32.812500
INFO:root:valid 050 2.346304e+00 7.628676 47.947304
INFO:root:valid 100 2.347727e+00 7.271040 47.973391
INFO:root:valid 150 2.345092e+00 7.388245 48.447848
INFO:root:valid_acc 7.358333
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357436e+00 10.937500 51.562500
INFO:root:train 050 2.349716e+00 8.118873 49.417892
INFO:root:train 100 2.350473e+00 7.936262 49.180074
INFO:root:train 150 2.350761e+00 7.967715 48.623758
INFO:root:train 200 2.350577e+00 7.983520 48.701803
INFO:root:train 250 2.350054e+00 8.024153 48.630478
INFO:root:train 300 2.350144e+00 8.098007 48.697051
INFO:root:train 350 2.349777e+00 8.137464 48.677885
INFO:root:train 400 2.349558e+00 8.190461 48.749221
INFO:root:train 450 2.349894e+00 8.148559 48.752772
INFO:root:train 500 2.349971e+00 8.068239 48.902196
INFO:root:train 550 2.350179e+00 8.042196 48.797641
INFO:root:train 600 2.349771e+00 8.124480 48.918469
INFO:root:train 650 2.349616e+00 8.090918 48.943932
INFO:root:train 700 2.349528e+00 8.088891 48.965763
INFO:root:train_acc 8.066667
INFO:root:valid 000 2.346956e+00 6.250000 43.750000
INFO:root:valid 050 2.325950e+00 9.344363 49.050245
INFO:root:valid 100 2.331974e+00 8.833540 47.571163
INFO:root:valid 150 2.331283e+00 8.857616 47.764901
INFO:root:valid_acc 8.725000
INFO:solver.bo_hb:Configuration achieved a performance of 2.331945 
INFO:solver.bo_hb:Evaluation of this configuration took 379.908364 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 16 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 104.924273
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.778309
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351063e+00 7.383578 48.866422
INFO:root:train 100 2.352567e+00 7.255569 47.617574
INFO:root:train 150 2.353080e+00 7.046772 47.247517
INFO:root:train 200 2.354450e+00 6.934080 46.859453
INFO:root:train 250 2.352815e+00 6.978337 47.236056
INFO:root:train 300 2.351590e+00 7.070183 47.591362
INFO:root:train 350 2.350730e+00 7.064637 47.863248
INFO:root:train 400 2.350087e+00 7.064370 48.008884
INFO:root:train 450 2.349536e+00 7.105737 48.132622
INFO:root:train 500 2.349331e+00 7.076472 48.166168
INFO:root:train 550 2.348892e+00 7.069533 48.139746
INFO:root:train 600 2.348459e+00 7.110545 48.086522
INFO:root:train 650 2.348804e+00 7.061252 48.065476
INFO:root:train 700 2.348616e+00 7.036822 48.078638
INFO:root:train_acc 7.079167
INFO:root:valid 000 2.380760e+00 6.250000 32.812500
INFO:root:valid 050 2.346293e+00 7.628676 47.916667
INFO:root:valid 100 2.347725e+00 7.240099 47.973391
INFO:root:valid 150 2.345097e+00 7.367550 48.416805
INFO:root:valid_acc 7.366667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357488e+00 10.937500 51.562500
INFO:root:train 050 2.349698e+00 8.088235 49.417892
INFO:root:train 100 2.350471e+00 7.920792 49.102723
INFO:root:train 150 2.350760e+00 7.947020 48.572020
INFO:root:train 200 2.350578e+00 7.991294 48.639614
INFO:root:train 250 2.350054e+00 8.042829 48.593127
INFO:root:train 300 2.350146e+00 8.118771 48.681478
INFO:root:train 350 2.349778e+00 8.150819 48.664530
INFO:root:train 400 2.349558e+00 8.198254 48.753117
INFO:root:train 450 2.349895e+00 8.158952 48.752772
INFO:root:train 500 2.349977e+00 8.077595 48.902196
INFO:root:train 550 2.350184e+00 8.047868 48.800476
INFO:root:train 600 2.349774e+00 8.132280 48.915869
INFO:root:train 650 2.349622e+00 8.093318 48.931932
INFO:root:train 700 2.349533e+00 8.097807 48.947932
INFO:root:train_acc 8.077083
INFO:root:valid 000 2.347009e+00 6.250000 45.312500
INFO:root:valid 050 2.325955e+00 9.375000 49.050245
INFO:root:valid 100 2.331977e+00 8.818069 47.555693
INFO:root:valid 150 2.331277e+00 8.878311 47.764901
INFO:root:valid_acc 8.725000
INFO:solver.bo_hb:Configuration achieved a performance of 2.331941 
INFO:solver.bo_hb:Evaluation of this configuration took 378.648866 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 17 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 42.920867
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.656766
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351063e+00 7.383578 48.866422
INFO:root:train 100 2.352568e+00 7.255569 47.633045
INFO:root:train 150 2.353080e+00 7.057119 47.257864
INFO:root:train 200 2.354449e+00 6.941853 46.875000
INFO:root:train 250 2.352815e+00 6.984562 47.229831
INFO:root:train 300 2.351589e+00 7.075374 47.586171
INFO:root:train 350 2.350731e+00 7.069088 47.863248
INFO:root:train 400 2.350089e+00 7.068267 48.008884
INFO:root:train 450 2.349535e+00 7.109202 48.129157
INFO:root:train 500 2.349331e+00 7.085828 48.156811
INFO:root:train 550 2.348891e+00 7.078040 48.128403
INFO:root:train 600 2.348460e+00 7.118344 48.078723
INFO:root:train 650 2.348805e+00 7.070853 48.058276
INFO:root:train 700 2.348617e+00 7.047967 48.076409
INFO:root:train_acc 7.087500
INFO:root:valid 000 2.380667e+00 6.250000 32.812500
INFO:root:valid 050 2.346292e+00 7.598039 47.947304
INFO:root:valid 100 2.347732e+00 7.240099 47.957921
INFO:root:valid 150 2.345101e+00 7.367550 48.406457
INFO:root:valid_acc 7.350000
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357504e+00 10.937500 51.562500
INFO:root:train 050 2.349706e+00 8.057598 49.387255
INFO:root:train 100 2.350466e+00 7.874381 49.133663
INFO:root:train 150 2.350759e+00 7.915977 48.572020
INFO:root:train 200 2.350576e+00 7.960199 48.631841
INFO:root:train 250 2.350052e+00 8.017928 48.568227
INFO:root:train 300 2.350141e+00 8.103198 48.681478
INFO:root:train 350 2.349775e+00 8.137464 48.673433
INFO:root:train 400 2.349553e+00 8.186565 48.753117
INFO:root:train 450 2.349890e+00 8.145094 48.745843
INFO:root:train 500 2.349970e+00 8.065120 48.905314
INFO:root:train 550 2.350177e+00 8.036525 48.814655
INFO:root:train 600 2.349767e+00 8.121880 48.926269
INFO:root:train 650 2.349617e+00 8.086118 48.943932
INFO:root:train 700 2.349528e+00 8.086662 48.961305
INFO:root:train_acc 8.066667
INFO:root:valid 000 2.346964e+00 6.250000 45.312500
INFO:root:valid 050 2.325970e+00 9.283088 49.019608
INFO:root:valid 100 2.331989e+00 8.787129 47.540223
INFO:root:valid 150 2.331288e+00 8.867964 47.702815
INFO:root:valid_acc 8.708333
INFO:solver.bo_hb:Configuration achieved a performance of 2.331950 
INFO:solver.bo_hb:Evaluation of this configuration took 379.620096 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 18 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 46.370520
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.660775
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351062e+00 7.383578 48.866422
INFO:root:train 100 2.352567e+00 7.255569 47.617574
INFO:root:train 150 2.353079e+00 7.046772 47.237169
INFO:root:train 200 2.354448e+00 6.934080 46.859453
INFO:root:train 250 2.352814e+00 6.978337 47.211155
INFO:root:train 300 2.351588e+00 7.070183 47.570598
INFO:root:train 350 2.350728e+00 7.060185 47.854345
INFO:root:train 400 2.350086e+00 7.064370 48.001091
INFO:root:train 450 2.349533e+00 7.105737 48.125693
INFO:root:train 500 2.349327e+00 7.082710 48.159930
INFO:root:train 550 2.348890e+00 7.075204 48.122731
INFO:root:train 600 2.348457e+00 7.115745 48.070923
INFO:root:train 650 2.348802e+00 7.070853 48.053475
INFO:root:train 700 2.348614e+00 7.050196 48.069722
INFO:root:train_acc 7.089583
INFO:root:valid 000 2.380828e+00 6.250000 32.812500
INFO:root:valid 050 2.346301e+00 7.598039 47.947304
INFO:root:valid 100 2.347732e+00 7.224629 47.942450
INFO:root:valid 150 2.345102e+00 7.367550 48.406457
INFO:root:valid_acc 7.366667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357556e+00 9.375000 51.562500
INFO:root:train 050 2.349680e+00 7.965686 49.387255
INFO:root:train 100 2.350460e+00 7.858911 49.071782
INFO:root:train 150 2.350751e+00 7.895281 48.551325
INFO:root:train 200 2.350573e+00 7.944652 48.639614
INFO:root:train 250 2.350050e+00 8.005478 48.574452
INFO:root:train 300 2.350142e+00 8.077243 48.655523
INFO:root:train 350 2.349781e+00 8.115207 48.646724
INFO:root:train 400 2.349558e+00 8.163186 48.725842
INFO:root:train 450 2.349896e+00 8.127772 48.728520
INFO:root:train 500 2.349978e+00 8.046407 48.892839
INFO:root:train 550 2.350184e+00 8.019510 48.791969
INFO:root:train 600 2.349773e+00 8.106281 48.908070
INFO:root:train 650 2.349624e+00 8.071717 48.919931
INFO:root:train 700 2.349535e+00 8.075517 48.939016
INFO:root:train_acc 8.056250
INFO:root:valid 000 2.347044e+00 6.250000 45.312500
INFO:root:valid 050 2.325950e+00 9.344363 49.019608
INFO:root:valid 100 2.331966e+00 8.802599 47.540223
INFO:root:valid 150 2.331274e+00 8.857616 47.744205
INFO:root:valid_acc 8.708333
INFO:solver.bo_hb:Configuration achieved a performance of 2.331939 
INFO:solver.bo_hb:Evaluation of this configuration took 379.537476 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Start iteration 19 ... 
INFO:solver.bo_hb:Train model...
/home/rkohli/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([19])) that is different to the input size (torch.Size([19, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:solver.bo_hb:Time to train the model: 43.154223
INFO:solver.bo_hb:Maximize acquisition function...
INFO:solver.bo_hb:Time to maximize the acquisition function: 0.634541
INFO:solver.bo_hb:Next candidate [2.40000e+01 2.00000e-01 6.00000e+00 5.00005e-02 1.00000e+00 4.50000e+00
 1.50000e+00 5.05000e-04]
INFO:root:gpu device = cuda:0
INFO:root:config = {'batch_size': 32, 'drop_path_prob': 0.08000000000000002, 'grad_clip_value': 8, 'initial_lr': 1.7782896466675373e-06, 'lr_scheduler': 'Cosine', 'n_conv_layers': 6, 'optimizer': 'sgd', 'weight_decay': 1.0023283172746057e-05}
INFO:root:param size = 0.255930MB
INFO:root:epoch 0 lr 8.891448e-07
INFO:root:train 000 2.394835e+00 12.500000 40.625000
INFO:root:train 050 2.351062e+00 7.383578 48.866422
INFO:root:train 100 2.352567e+00 7.255569 47.617574
INFO:root:train 150 2.353080e+00 7.057119 47.226821
INFO:root:train 200 2.354450e+00 6.941853 46.851679
INFO:root:train 250 2.352815e+00 6.984562 47.211155
INFO:root:train 300 2.351590e+00 7.075374 47.555025
INFO:root:train 350 2.350732e+00 7.069088 47.836538
INFO:root:train 400 2.350089e+00 7.068267 47.985505
INFO:root:train 450 2.349537e+00 7.102273 48.111835
INFO:root:train 500 2.349332e+00 7.082710 48.138099
INFO:root:train 550 2.348893e+00 7.078040 48.108553
INFO:root:train 600 2.348459e+00 7.120944 48.055324
INFO:root:train 650 2.348803e+00 7.070853 48.039075
INFO:root:train 700 2.348614e+00 7.045738 48.060806
INFO:root:train_acc 7.085417
INFO:root:valid 000 2.380594e+00 6.250000 32.812500
INFO:root:valid 050 2.346300e+00 7.567402 47.947304
INFO:root:valid 100 2.347727e+00 7.224629 47.957921
INFO:root:valid 150 2.345094e+00 7.357202 48.437500
INFO:root:valid_acc 7.341667
INFO:root:epoch 1 lr 0.000000e+00
INFO:root:train 000 2.357465e+00 10.937500 51.562500
INFO:root:train 050 2.349692e+00 7.996324 49.479167
INFO:root:train 100 2.350465e+00 7.874381 49.149134
INFO:root:train 150 2.350747e+00 7.915977 48.623758
INFO:root:train 200 2.350565e+00 7.952425 48.694030
INFO:root:train 250 2.350043e+00 8.011703 48.636703
INFO:root:train 300 2.350135e+00 8.092816 48.728198
INFO:root:train 350 2.349767e+00 8.124110 48.717949
INFO:root:train 400 2.349548e+00 8.182668 48.788186
INFO:root:train 450 2.349883e+00 8.141630 48.783952
INFO:root:train 500 2.349963e+00 8.062001 48.930264
INFO:root:train 550 2.350171e+00 8.039360 48.825998
INFO:root:train 600 2.349763e+00 8.124480 48.949667
INFO:root:train 650 2.349611e+00 8.083717 48.967934
INFO:root:train 700 2.349523e+00 8.084433 48.979137
INFO:root:train_acc 8.064583
INFO:root:valid 000 2.346991e+00 6.250000 45.312500
INFO:root:valid 050 2.325958e+00 9.313725 49.111520
INFO:root:valid 100 2.331983e+00 8.787129 47.555693
INFO:root:valid 150 2.331282e+00 8.857616 47.754553
INFO:root:valid_acc 8.708333
INFO:solver.bo_hb:Configuration achieved a performance of 2.331945 
INFO:solver.bo_hb:Evaluation of this configuration took 379.814842 seconds
INFO:solver.bo_hb:Current incumbent [2.28305501e+01 2.87521261e-01 5.34103323e+00 6.38350463e-02
 1.00000000e+00 4.34399304e+00 0.00000000e+00 6.01281106e-04] with estimated performance 2.292099
INFO:solver.bo_hb:Return [22.8305500683827, 0.28752126119799193, 5.34103323383389, 0.06383504631442148, 1.0, 4.3439930433373695, 0.0, 0.0006012811059360664] as incumbent with error 2.292099 
Traceback (most recent call last):
  File "darts-bohamiann.py", line 172, in <module>
    worker.run_bohamiann()
  File "darts-bohamiann.py", line 164, in run_bohamiann
    os.mkdir(log_dir)
FileNotFoundError: [Errno 2] No such file or directory: './bohamiann/EXP0'
