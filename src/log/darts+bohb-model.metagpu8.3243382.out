Workingdir: /home/rkohli/aml-project/PC-DARTS
Started at Mon Sep  9 15:54:34 CEST 2019
Running job darts+bohb-model using 2 cpus per node with given JID 3243382 on queue meta_gpu-black
Experiment dir : eval-EXP-20190909-160350
Worker running
09/09 04:04:52 PM DISPATCHER: started the 'discover_worker' thread
09/09 04:04:52 PM WORKER: start listening for jobs
09/09 04:04:53 PM DISPATCHER: started the 'job_runner' thread
09/09 04:04:53 PM DISPATCHER: Pyro daemon running on 127.0.0.1:33743
09/09 04:04:53 PM DISPATCHER: discovered new worker, hpbandster.run_01.worker.metagpu8.17919139963371587392
09/09 04:04:53 PM HBMASTER: adjusted queue size to (0, 1)
09/09 04:04:53 PM DISPATCHER: A new worker triggered discover_worker
09/09 04:04:53 PM HBMASTER: starting run at 1568037893.153693
09/09 04:04:53 PM WORKER: start processing job (0, 0, 0)
09/09 04:04:54 PM gpu device = cuda:0
09/09 04:04:54 PM config = {'drop_path_prob': 0.12227569891433619, 'grad_clip_value': 4, 'initial_lr': 2.154885743532238e-06, 'lr_scheduler': 'Plateau', 'n_conv_layers': 6, 'optimizer': 'adam', 'weight_decay': 0.0008538256081348577}
48 48 16
48 64 16
64 64 32
64 128 32
128 128 64
128 256 64
09/09 04:05:21 PM param size = 0.255930MB
Training size= 48000
09/09 04:05:21 PM WORKER: registered result for job (0, 0, 0) with dispatcher
09/09 04:05:22 PM job (0, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 117, in compute
    lr_scheduler.step()
TypeError: step() missing 1 required positional argument: 'metrics'

09/09 04:05:22 PM WORKER: start processing job (1, 0, 0)
09/09 04:05:22 PM gpu device = cuda:0
09/09 04:05:22 PM config = {'drop_path_prob': 0.1845860078956414, 'grad_clip_value': 5, 'initial_lr': 0.005120593129238132, 'lr_scheduler': 'Cosine', 'n_conv_layers': 3, 'optimizer': 'sgd', 'weight_decay': 4.241376263065748e-05, 'nesterov': 'False', 'sgd_momentum': 0.9352732812800111}
48 48 16
48 64 32
64 128 64
09/09 04:05:22 PM param size = 0.118122MB
Training size= 48000
09/09 04:05:22 PM epoch 0 lr 4.631620e-03
09/09 04:05:26 PM train 000 2.308738e+00 14.062500 43.750000
09/09 04:05:33 PM train 050 2.057791e+00 29.074755 77.389706
09/09 04:05:39 PM train 100 1.683892e+00 43.131188 84.900990
09/09 04:05:46 PM train 150 1.385441e+00 53.704470 89.072848
09/09 04:05:52 PM train 200 1.164032e+00 61.333955 91.495647
09/09 04:05:58 PM train 250 1.005359e+00 66.832669 93.052789
09/09 04:06:04 PM train 300 8.909064e-01 70.779693 94.113372
09/09 04:06:11 PM train 350 7.995285e-01 73.891560 94.880698
09/09 04:06:17 PM train 400 7.265945e-01 76.371571 95.472257
09/09 04:06:23 PM train 450 6.673693e-01 78.350194 95.963830
09/09 04:06:29 PM train 500 6.171636e-01 80.049276 96.341692
09/09 04:06:36 PM train 550 5.762866e-01 81.414474 96.648140
09/09 04:06:42 PM train 600 5.416523e-01 82.547317 96.908798
09/09 04:06:48 PM train 650 5.113608e-01 83.544547 97.124616
09/09 04:06:54 PM train 700 4.849300e-01 84.395061 97.320792
09/09 04:07:01 PM train_acc 85.152083
09/09 04:07:01 PM valid 000 8.613573e-02 96.875000 100.000000
09/09 04:07:02 PM valid 050 1.406585e-01 96.323529 99.816176
09/09 04:07:03 PM valid 100 1.335048e-01 96.364480 99.814356
09/09 04:07:04 PM valid 150 1.313778e-01 96.233444 99.803394
09/09 04:07:05 PM valid_acc 96.075000
09/09 04:07:05 PM epoch 1 lr 3.351472e-03
09/09 04:07:05 PM train 000 1.859654e-01 93.750000 100.000000
09/09 04:07:12 PM train 050 2.684400e-01 91.421569 99.479167
09/09 04:07:18 PM train 100 2.333927e-01 92.930074 99.582302
09/09 04:07:25 PM train 150 2.168462e-01 93.439570 99.648179
09/09 04:07:31 PM train 200 2.029195e-01 93.851057 99.665734
09/09 04:07:38 PM train 250 1.898625e-01 94.173307 99.726096
09/09 04:07:44 PM train 300 1.802248e-01 94.466362 99.735257
09/09 04:07:50 PM train 350 1.750926e-01 94.649217 99.746261
09/09 04:07:57 PM train 400 1.685812e-01 94.841022 99.758416
09/09 04:08:04 PM train 450 1.643880e-01 95.035338 99.760948
09/09 04:08:10 PM train 500 1.605004e-01 95.162799 99.762974
09/09 04:08:17 PM train 550 1.569951e-01 95.255785 99.773140
09/09 04:08:23 PM train 600 1.561613e-01 95.294301 99.771215
09/09 04:08:30 PM train 650 1.527111e-01 95.403706 99.783986
09/09 04:08:36 PM train 700 1.502059e-01 95.448466 99.794936
09/09 04:08:43 PM train_acc 95.504167
09/09 04:08:43 PM valid 000 6.039707e-02 98.437500 100.000000
09/09 04:08:44 PM valid 050 9.650914e-02 96.875000 99.938725
09/09 04:08:45 PM valid 100 9.712505e-02 97.230817 99.922649
09/09 04:08:46 PM valid 150 9.902812e-02 97.123344 99.865480
09/09 04:08:47 PM valid_acc 97.033333
09/09 04:08:47 PM epoch 2 lr 1.769121e-03
09/09 04:08:47 PM train 000 2.279550e-01 92.187500 100.000000
09/09 04:08:54 PM train 050 1.492028e-01 95.036765 99.846814
09/09 04:09:00 PM train 100 1.478122e-01 95.374381 99.845297
09/09 04:09:07 PM train 150 1.433536e-01 95.560844 99.834437
09/09 04:09:14 PM train 200 1.412789e-01 95.724502 99.836754
09/09 04:09:20 PM train 250 1.400570e-01 95.810508 99.825697
09/09 04:09:27 PM train 300 1.386909e-01 95.831603 99.833887
09/09 04:09:33 PM train 350 1.353918e-01 95.957977 99.839744
09/09 04:09:40 PM train 400 1.317331e-01 96.080112 99.848036
09/09 04:09:46 PM train 450 1.279578e-01 96.195953 99.851025
09/09 04:09:53 PM train 500 1.268502e-01 96.245010 99.840943
09/09 04:09:59 PM train 550 1.250892e-01 96.305014 99.841198
09/09 04:10:06 PM train 600 1.254521e-01 96.321235 99.825811
09/09 04:10:12 PM train 650 1.237375e-01 96.356567 99.829589
09/09 04:10:19 PM train 700 1.223921e-01 96.404690 99.830599
09/09 04:10:25 PM train_acc 96.433333
09/09 04:10:25 PM valid 000 3.652608e-02 98.437500 100.000000
09/09 04:10:27 PM valid 050 7.377873e-02 97.977941 99.908088
09/09 04:10:28 PM valid 100 6.797054e-02 98.050743 99.907178
09/09 04:10:29 PM valid 150 6.603369e-02 98.085679 99.896523
09/09 04:10:30 PM valid_acc 98.016667
09/09 04:10:30 PM epoch 3 lr 4.889731e-04
09/09 04:10:30 PM train 000 1.369852e-01 96.875000 100.000000
09/09 04:10:37 PM train 050 1.297431e-01 95.955882 99.785539
09/09 04:10:43 PM train 100 1.421985e-01 95.807550 99.752475
09/09 04:10:50 PM train 150 1.441622e-01 95.736755 99.751656
09/09 04:10:56 PM train 200 1.446667e-01 95.763371 99.751244
09/09 04:11:03 PM train 250 1.395175e-01 95.903884 99.769671
09/09 04:11:10 PM train 300 1.361475e-01 96.002907 99.792359
09/09 04:11:16 PM train 350 1.349568e-01 95.993590 99.799679
09/09 04:11:23 PM train 400 1.334961e-01 96.021665 99.789589
09/09 04:11:30 PM train 450 1.334841e-01 96.098947 99.781735
09/09 04:11:36 PM train 500 1.306943e-01 96.207585 99.794162
09/09 04:11:43 PM train 550 1.300464e-01 96.211434 99.804333
09/09 04:11:49 PM train 600 1.284459e-01 96.256240 99.812812
09/09 04:11:56 PM train 650 1.276253e-01 96.282162 99.807988
09/09 04:12:03 PM train 700 1.265744e-01 96.299929 99.812767
09/09 04:12:09 PM train_acc 96.333333
09/09 04:12:09 PM valid 000 1.944356e-02 100.000000 100.000000
09/09 04:12:10 PM valid 050 6.062660e-02 98.253676 99.908088
09/09 04:12:11 PM valid 100 5.476418e-02 98.437500 99.922649
09/09 04:12:13 PM valid 150 5.325884e-02 98.499586 99.927566
09/09 04:12:14 PM valid_acc 98.458333
09/09 04:12:14 PM epoch 4 lr 0.000000e+00
09/09 04:12:14 PM train 000 8.510994e-02 100.000000 100.000000
09/09 04:12:20 PM train 050 1.635247e-01 95.281863 99.785539
09/09 04:12:27 PM train 100 1.682479e-01 95.080446 99.706064
09/09 04:12:33 PM train 150 1.701765e-01 95.002070 99.689570
09/09 04:12:40 PM train 200 1.685438e-01 94.916045 99.673507
09/09 04:12:47 PM train 250 1.663013e-01 94.970120 99.682520
09/09 04:12:53 PM train 300 1.681985e-01 94.917982 99.672965
09/09 04:13:00 PM train 350 1.702844e-01 94.907407 99.670584
09/09 04:13:06 PM train 400 1.704499e-01 94.938435 99.664900
09/09 04:13:13 PM train 450 1.710659e-01 94.910615 99.667406
09/09 04:13:19 PM train 500 1.717286e-01 94.907061 99.672530
09/09 04:13:26 PM train 550 1.711550e-01 94.904152 99.673888
09/09 04:13:32 PM train 600 1.721343e-01 94.886127 99.672421
09/09 04:13:39 PM train 650 1.709003e-01 94.954877 99.680780
09/09 04:13:46 PM train 700 1.695019e-01 95.018277 99.683488
09/09 04:13:52 PM train_acc 95.039583
09/09 04:13:52 PM valid 000 4.952304e-02 98.437500 100.000000
09/09 04:13:53 PM valid 050 5.392423e-02 98.406863 99.908088
09/09 04:13:55 PM valid 100 5.162374e-02 98.561262 99.891708
09/09 04:13:56 PM valid 150 5.658521e-02 98.427152 99.875828
09/09 04:13:57 PM valid_acc 98.383333
09/09 04:13:57 PM WORKER: registered result for job (1, 0, 0) with dispatcher
09/09 04:13:57 PM WORKER: start processing job (2, 0, 0)
09/09 04:13:57 PM gpu device = cuda:0
09/09 04:13:57 PM config = {'drop_path_prob': 0.11289344663290116, 'grad_clip_value': 7, 'initial_lr': 0.0005977229568984026, 'lr_scheduler': 'Plateau', 'n_conv_layers': 4, 'optimizer': 'adad', 'weight_decay': 0.00012018715553328046}
48 48 16
48 64 32
64 128 64
128 256 64
09/09 04:13:57 PM param size = 0.213546MB
Training size= 48000
09/09 04:13:58 PM WORKER: registered result for job (2, 0, 0) with dispatcher
09/09 04:13:58 PM job (2, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 117, in compute
    lr_scheduler.step()
TypeError: step() missing 1 required positional argument: 'metrics'

09/09 04:13:58 PM WORKER: start processing job (3, 0, 0)
09/09 04:13:58 PM gpu device = cuda:0
09/09 04:13:58 PM config = {'drop_path_prob': 0.1755925203064179, 'grad_clip_value': 7, 'initial_lr': 0.025312649697615434, 'lr_scheduler': 'Exponential', 'n_conv_layers': 4, 'optimizer': 'adad', 'weight_decay': 0.0001320835650197275}
48 48 16
48 64 32
64 128 64
128 256 64
09/09 04:13:58 PM param size = 0.213546MB
09/09 04:13:58 PM WORKER: registered result for job (3, 0, 0) with dispatcher
09/09 04:13:58 PM job (3, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 98, in compute
    lr_scheduler = torch.optim.lr_scheduler.ExponnetialLR(optimizer, gamma=0.1)
AttributeError: module 'torch.optim.lr_scheduler' has no attribute 'ExponnetialLR'

09/09 04:13:58 PM WORKER: start processing job (4, 0, 0)
09/09 04:13:58 PM gpu device = cuda:0
09/09 04:13:58 PM config = {'drop_path_prob': 0.1825894652158632, 'grad_clip_value': 5, 'initial_lr': 8.612803171288942e-06, 'lr_scheduler': 'Plateau', 'n_conv_layers': 5, 'optimizer': 'adad', 'weight_decay': 0.0006487476955889933}
48 48 16
48 64 32
64 128 32
128 128 64
128 256 64
09/09 04:13:58 PM param size = 0.245898MB
Training size= 48000
09/09 04:13:58 PM WORKER: registered result for job (4, 0, 0) with dispatcher
09/09 04:13:58 PM job (4, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 117, in compute
    lr_scheduler.step()
TypeError: step() missing 1 required positional argument: 'metrics'

09/09 04:13:58 PM WORKER: start processing job (5, 0, 0)
09/09 04:13:58 PM gpu device = cuda:0
09/09 04:13:58 PM config = {'drop_path_prob': 0.2529164161743424, 'grad_clip_value': 4, 'initial_lr': 1.9173311609475522e-05, 'lr_scheduler': 'Cosine', 'n_conv_layers': 3, 'optimizer': 'adam', 'weight_decay': 8.911915225164262e-05}
48 48 16
48 64 32
64 128 64
09/09 04:13:59 PM param size = 0.118122MB
Training size= 48000
09/09 04:13:59 PM epoch 0 lr 1.734242e-05
09/09 04:13:59 PM train 000 2.308738e+00 14.062500 43.750000
09/09 04:14:06 PM train 050 2.287725e+00 14.338235 56.066176
09/09 04:14:14 PM train 100 2.268636e+00 17.961015 61.664604
09/09 04:14:21 PM train 150 2.248341e+00 20.850579 66.038907
09/09 04:14:29 PM train 200 2.229479e+00 22.971082 69.099813
09/09 04:14:36 PM train 250 2.211471e+00 24.508217 71.022161
09/09 04:14:44 PM train 300 2.191407e+00 26.038206 72.975498
09/09 04:14:51 PM train 350 2.169224e+00 27.679843 74.799679
09/09 04:14:58 PM train 400 2.147474e+00 28.923784 76.227400
09/09 04:15:06 PM train 450 2.125006e+00 30.134424 77.439024
09/09 04:15:13 PM train 500 2.099885e+00 31.415294 78.602171
09/09 04:15:21 PM train 550 2.076150e+00 32.509074 79.457804
09/09 04:15:28 PM train 600 2.050542e+00 33.641847 80.223066
09/09 04:15:35 PM train 650 2.024460e+00 34.785426 80.959581
09/09 04:15:43 PM train 700 1.998020e+00 35.812678 81.628923
09/09 04:15:50 PM train_acc 36.772917
09/09 04:15:51 PM valid 000 1.418467e+00 60.937500 93.750000
09/09 04:15:52 PM valid 050 1.561213e+00 51.899510 90.349265
09/09 04:15:53 PM valid 100 1.556771e+00 51.748144 90.965347
09/09 04:15:54 PM valid 150 1.557719e+00 51.955712 91.038907
09/09 04:15:55 PM valid_acc 52.041667
09/09 04:15:55 PM epoch 1 lr 1.254910e-05
09/09 04:15:55 PM train 000 1.598917e+00 45.312500 92.187500
09/09 04:16:03 PM train 050 1.666766e+00 45.128676 89.246324
09/09 04:16:10 PM train 100 1.649932e+00 45.792079 88.969678
09/09 04:16:18 PM train 150 1.640298e+00 46.150662 89.176325
09/09 04:16:26 PM train 200 1.626742e+00 46.548507 89.295709
09/09 04:16:33 PM train 250 1.613905e+00 47.111554 89.436006
09/09 04:16:41 PM train 300 1.603414e+00 47.420058 89.348007
09/09 04:16:48 PM train 350 1.590978e+00 47.894409 89.503205
09/09 04:16:56 PM train 400 1.578862e+00 48.347880 89.608011
09/09 04:17:04 PM train 450 1.563812e+00 49.074972 89.745011
09/09 04:17:11 PM train 500 1.551509e+00 49.532186 89.835953
09/09 04:17:19 PM train 550 1.539619e+00 50.019850 89.950091
09/09 04:17:26 PM train 600 1.527728e+00 50.478369 90.063436
09/09 04:17:34 PM train 650 1.516791e+00 50.900058 90.216974
09/09 04:17:42 PM train 700 1.504192e+00 51.388641 90.341922
09/09 04:17:49 PM train_acc 51.714583
09/09 04:17:49 PM valid 000 1.204147e+00 65.625000 95.312500
09/09 04:17:50 PM valid 050 1.264261e+00 61.427696 92.738971
09/09 04:17:52 PM valid 100 1.266937e+00 61.355198 92.713490
09/09 04:17:53 PM valid 150 1.264892e+00 61.599752 92.953228
09/09 04:17:54 PM valid_acc 61.983333
09/09 04:17:54 PM epoch 2 lr 6.624216e-06
09/09 04:17:54 PM train 000 1.561062e+00 45.312500 92.187500
09/09 04:18:01 PM train 050 1.477888e+00 50.582108 89.828431
09/09 04:18:09 PM train 100 1.451163e+00 52.506188 90.439356
09/09 04:18:17 PM train 150 1.433353e+00 53.321606 90.780215
09/09 04:18:24 PM train 200 1.427429e+00 53.645833 91.021455
09/09 04:18:32 PM train 250 1.420566e+00 53.959163 91.147908
09/09 04:18:39 PM train 300 1.419407e+00 53.981520 91.050664
09/09 04:18:47 PM train 350 1.413969e+00 54.148860 91.038996
09/09 04:18:55 PM train 400 1.406944e+00 54.480985 91.096478
09/09 04:19:03 PM train 450 1.402483e+00 54.753326 91.175859
09/09 04:19:10 PM train 500 1.398837e+00 54.983782 91.170783
09/09 04:19:18 PM train 550 1.395437e+00 55.033462 91.197822
09/09 04:19:25 PM train 600 1.390010e+00 55.246464 91.274958
09/09 04:19:33 PM train 650 1.385934e+00 55.441148 91.330645
09/09 04:19:41 PM train 700 1.382265e+00 55.594686 91.420738
09/09 04:19:48 PM train_acc 55.870833
09/09 04:19:49 PM valid 000 1.045980e+00 73.437500 96.875000
09/09 04:19:50 PM valid 050 1.187653e+00 65.533088 93.719363
09/09 04:19:51 PM valid 100 1.189577e+00 65.779703 93.858292
09/09 04:19:52 PM valid 150 1.187863e+00 66.018212 93.812086
09/09 04:19:53 PM valid_acc 66.025000
09/09 04:19:53 PM epoch 3 lr 1.830888e-06
09/09 04:19:53 PM train 000 1.318004e+00 59.375000 89.062500
09/09 04:20:01 PM train 050 1.420816e+00 53.921569 90.134804
09/09 04:20:08 PM train 100 1.433388e+00 53.728342 89.851485
09/09 04:20:16 PM train 150 1.432686e+00 53.828642 89.973096
09/09 04:20:23 PM train 200 1.426833e+00 54.322139 90.034204
09/09 04:20:31 PM train 250 1.420459e+00 54.413596 90.170568
09/09 04:20:39 PM train 300 1.412190e+00 54.760174 90.266819
09/09 04:20:46 PM train 350 1.409802e+00 54.611823 90.433583
09/09 04:20:54 PM train 400 1.408440e+00 54.668017 90.453554
09/09 04:21:02 PM train 450 1.408834e+00 54.722145 90.392877
09/09 04:21:09 PM train 500 1.408198e+00 54.796657 90.325599
09/09 04:21:17 PM train 550 1.409422e+00 54.713022 90.364111
09/09 04:21:25 PM train 600 1.408263e+00 54.718698 90.396215
09/09 04:21:32 PM train 650 1.407345e+00 54.771505 90.399386
09/09 04:21:40 PM train 700 1.406472e+00 54.823466 90.411020
09/09 04:21:47 PM train_acc 54.943750
09/09 04:21:47 PM valid 000 1.308203e+00 64.062500 87.500000
09/09 04:21:48 PM valid 050 1.217840e+00 64.491422 93.658088
09/09 04:21:50 PM valid 100 1.220067e+00 64.573020 93.394183
09/09 04:21:51 PM valid 150 1.212620e+00 64.807533 93.760348
09/09 04:21:52 PM valid_acc 64.616667
09/09 04:21:52 PM epoch 4 lr 0.000000e+00
09/09 04:21:52 PM train 000 1.425040e+00 50.000000 90.625000
09/09 04:22:00 PM train 050 1.488579e+00 51.011029 89.185049
09/09 04:22:07 PM train 100 1.492796e+00 51.562500 89.000619
09/09 04:22:15 PM train 150 1.492824e+00 51.365894 89.228063
09/09 04:22:22 PM train 200 1.487586e+00 51.484764 89.466729
09/09 04:22:30 PM train 250 1.488702e+00 51.363297 89.411106
09/09 04:22:38 PM train 300 1.492000e+00 51.147218 89.322051
09/09 04:22:45 PM train 350 1.494245e+00 50.992699 89.245014
09/09 04:22:53 PM train 400 1.495262e+00 51.016989 89.210567
09/09 04:23:01 PM train 450 1.497445e+00 50.873060 89.121397
09/09 04:23:08 PM train 500 1.498878e+00 50.854541 89.124875
09/09 04:23:16 PM train 550 1.495673e+00 51.040721 89.190109
09/09 04:23:23 PM train 600 1.497935e+00 50.993136 89.106697
09/09 04:23:31 PM train 650 1.497181e+00 51.010465 89.153706
09/09 04:23:39 PM train 700 1.499402e+00 50.953994 89.118224
09/09 04:23:46 PM train_acc 50.975000
09/09 04:23:46 PM valid 000 1.359565e+00 57.812500 95.312500
09/09 04:23:47 PM valid 050 1.300491e+00 58.302696 92.800245
09/09 04:23:49 PM valid 100 1.307370e+00 58.632426 92.821782
09/09 04:23:50 PM valid 150 1.295790e+00 58.919702 93.056705
09/09 04:23:51 PM valid_acc 58.916667
09/09 04:23:51 PM WORKER: registered result for job (5, 0, 0) with dispatcher
09/09 04:23:51 PM WORKER: start processing job (6, 0, 0)
09/09 04:23:51 PM gpu device = cuda:0
09/09 04:23:51 PM config = {'drop_path_prob': 0.11769446535609998, 'grad_clip_value': 8, 'initial_lr': 0.000708695623243901, 'lr_scheduler': 'Cosine', 'n_conv_layers': 4, 'optimizer': 'sgd', 'weight_decay': 1.8371401495036064e-05, 'nesterov': 'True', 'sgd_momentum': 0.19804375118067463}
48 48 16
48 64 32
64 128 64
128 256 64
09/09 04:23:51 PM param size = 0.213546MB
Training size= 48000
09/09 04:23:51 PM epoch 0 lr 6.410212e-04
09/09 04:23:51 PM train 000 2.259136e+00 9.375000 70.312500
09/09 04:23:59 PM train 050 2.296080e+00 11.734069 56.311275
09/09 04:24:07 PM train 100 2.282846e+00 11.881188 59.003713
09/09 04:24:16 PM train 150 2.274522e+00 12.520695 60.720199
09/09 04:24:24 PM train 200 2.266195e+00 13.502799 62.290112
09/09 04:24:32 PM train 250 2.259186e+00 14.759711 63.527141
09/09 04:24:40 PM train 300 2.253008e+00 15.754776 64.508929
09/09 04:24:48 PM train 350 2.246559e+00 16.902600 65.580484
09/09 04:24:56 PM train 400 2.238962e+00 18.185006 66.478335
09/09 04:25:05 PM train 450 2.232556e+00 19.086059 67.184035
09/09 04:25:13 PM train 500 2.226442e+00 19.872754 67.805015
09/09 04:25:21 PM train 550 2.219505e+00 20.805921 68.358666
09/09 04:25:29 PM train 600 2.213710e+00 21.435628 68.892991
09/09 04:25:38 PM train 650 2.207031e+00 22.129416 69.462846
09/09 04:25:46 PM train 700 2.200860e+00 22.648449 70.069544
09/09 04:25:54 PM train_acc 23.133333
09/09 04:25:54 PM valid 000 2.053559e+00 32.812500 75.000000
09/09 04:25:56 PM valid 050 2.094946e+00 29.871324 77.849265
09/09 04:25:57 PM valid 100 2.092492e+00 30.198020 77.537129
09/09 04:25:59 PM valid 150 2.088761e+00 30.660182 78.187086
09/09 04:26:00 PM valid_acc 30.950000
09/09 04:26:00 PM epoch 1 lr 4.638473e-04
09/09 04:26:00 PM train 000 2.121168e+00 25.000000 73.437500
09/09 04:26:08 PM train 050 2.103388e+00 30.024510 74.877451
09/09 04:26:17 PM train 100 2.096738e+00 30.089728 75.293936
09/09 04:26:25 PM train 150 2.095596e+00 29.408113 75.631209
09/09 04:26:33 PM train 200 2.091829e+00 29.259950 76.026119
09/09 04:26:42 PM train 250 2.086930e+00 29.506972 76.307271
09/09 04:26:50 PM train 300 2.081487e+00 29.687500 76.583264
09/09 04:26:59 PM train 350 2.079282e+00 29.683048 76.664886
09/09 04:27:07 PM train 400 2.074166e+00 30.010910 76.683292
09/09 04:27:16 PM train 450 2.070154e+00 30.273004 76.746120
09/09 04:27:24 PM train 500 2.066802e+00 30.557635 76.824476
09/09 04:27:33 PM train 550 2.062086e+00 30.804787 76.985027
09/09 04:27:41 PM train 600 2.058976e+00 30.821027 77.121464
09/09 04:27:50 PM train 650 2.054264e+00 31.137193 77.397753
09/09 04:27:58 PM train 700 2.051351e+00 31.343616 77.596737
09/09 04:28:06 PM train_acc 31.583333
09/09 04:28:07 PM valid 000 1.864241e+00 54.687500 85.937500
09/09 04:28:09 PM valid 050 1.979865e+00 35.937500 83.180147
09/09 04:28:10 PM valid 100 1.983083e+00 36.277847 82.472153
09/09 04:28:11 PM valid 150 1.979451e+00 36.299669 82.657285
09/09 04:28:13 PM valid_acc 36.575000
09/09 04:28:13 PM epoch 2 lr 2.448483e-04
09/09 04:28:13 PM train 000 2.039755e+00 32.812500 73.437500
09/09 04:28:21 PM train 050 2.003410e+00 33.455882 78.553922
09/09 04:28:30 PM train 100 2.007702e+00 33.338490 78.063119
09/09 04:28:38 PM train 150 2.014525e+00 32.698675 78.052566
09/09 04:28:47 PM train 200 2.016387e+00 32.462687 78.132774
09/09 04:28:56 PM train 250 2.018361e+00 32.196215 78.261952
09/09 04:29:04 PM train 300 2.015983e+00 32.386836 78.540282
09/09 04:29:13 PM train 350 2.013200e+00 32.554309 78.797187
09/09 04:29:21 PM train 400 2.010307e+00 32.773535 78.986128
09/09 04:29:29 PM train 450 2.008283e+00 32.847145 79.032705
09/09 04:29:38 PM train 500 2.005975e+00 32.853044 79.166667
09/09 04:29:46 PM train 550 2.002797e+00 33.056375 79.386910
09/09 04:29:55 PM train 600 2.002343e+00 33.051685 79.417117
09/09 04:30:03 PM train 650 1.998516e+00 33.328533 79.610695
09/09 04:30:12 PM train 700 1.997831e+00 33.376427 79.680813
09/09 04:30:20 PM train_acc 33.397917
09/09 04:30:20 PM valid 000 1.931385e+00 32.812500 82.812500
09/09 04:30:22 PM valid 050 1.978155e+00 34.129902 83.792892
09/09 04:30:23 PM valid 100 1.975846e+00 34.746287 83.323020
09/09 04:30:25 PM valid 150 1.976495e+00 34.499172 82.864238
09/09 04:30:26 PM valid_acc 34.600000
09/09 04:30:26 PM epoch 3 lr 6.767441e-05
09/09 04:30:26 PM train 000 2.016291e+00 28.125000 76.562500
09/09 04:30:34 PM train 050 1.984350e+00 33.241422 80.085784
09/09 04:30:43 PM train 100 1.991100e+00 32.812500 79.207921
09/09 04:30:51 PM train 150 1.995121e+00 32.398593 79.139073
09/09 04:31:00 PM train 200 1.993686e+00 32.641480 79.189988
09/09 04:31:08 PM train 250 1.993843e+00 32.669323 79.152141
09/09 04:31:17 PM train 300 1.996610e+00 32.848837 79.137251
09/09 04:31:25 PM train 350 1.996893e+00 32.919338 79.095442
09/09 04:31:33 PM train 400 1.996075e+00 33.065773 79.153678
09/09 04:31:42 PM train 450 1.996879e+00 32.919900 79.167822
09/09 04:31:50 PM train 500 1.997775e+00 33.008982 79.197854
09/09 04:31:59 PM train 550 1.998778e+00 32.999660 79.174229
09/09 04:32:07 PM train 600 1.998324e+00 33.041285 79.193532
09/09 04:32:16 PM train 650 1.998115e+00 33.014113 79.217070
09/09 04:32:24 PM train 700 1.997156e+00 33.028709 79.281830
09/09 04:32:32 PM train_acc 33.110417
09/09 04:32:33 PM valid 000 2.138303e+00 21.875000 76.562500
09/09 04:32:34 PM valid 050 2.006937e+00 30.085784 80.851716
09/09 04:32:36 PM valid 100 2.010423e+00 30.306312 80.662129
09/09 04:32:37 PM valid 150 2.013161e+00 29.873758 80.784354
09/09 04:32:38 PM valid_acc 29.991667
09/09 04:32:38 PM epoch 4 lr 0.000000e+00
09/09 04:32:38 PM train 000 1.919316e+00 37.500000 76.562500
09/09 04:32:47 PM train 050 2.006395e+00 31.617647 78.370098
09/09 04:32:55 PM train 100 2.010664e+00 31.342822 78.620050
09/09 04:33:04 PM train 150 2.017165e+00 31.394868 78.031871
09/09 04:33:12 PM train 200 2.021858e+00 31.281095 77.557525
09/09 04:33:20 PM train 250 2.019106e+00 31.312251 77.626992
09/09 04:33:29 PM train 300 2.016535e+00 31.457641 77.881022
09/09 04:33:37 PM train 350 2.016917e+00 31.543803 77.853454
09/09 04:33:46 PM train 400 2.017802e+00 31.362999 77.793797
09/09 04:33:54 PM train 450 2.017544e+00 31.298503 77.830516
09/09 04:34:03 PM train 500 2.018112e+00 31.228169 77.863024
09/09 04:34:11 PM train 550 2.018717e+00 31.164927 77.801724
09/09 04:34:20 PM train 600 2.019247e+00 31.088810 77.742824
09/09 04:34:28 PM train 650 2.019051e+00 31.089190 77.748176
09/09 04:34:37 PM train 700 2.019106e+00 31.029333 77.723787
09/09 04:34:45 PM train_acc 31.095833
09/09 04:34:45 PM valid 000 2.099319e+00 21.875000 68.750000
09/09 04:34:47 PM valid 050 2.094549e+00 20.986520 74.816176
09/09 04:34:48 PM valid 100 2.090853e+00 20.621906 74.319307
09/09 04:34:50 PM valid 150 2.090309e+00 20.767798 74.720613
09/09 04:34:51 PM valid_acc 20.816667
09/09 04:34:51 PM WORKER: registered result for job (6, 0, 0) with dispatcher
09/09 04:34:52 PM WORKER: start processing job (7, 0, 0)
09/09 04:34:52 PM gpu device = cuda:0
09/09 04:34:52 PM config = {'drop_path_prob': 0.161506153380091, 'grad_clip_value': 6, 'initial_lr': 0.062385342094074085, 'lr_scheduler': 'Plateau', 'n_conv_layers': 6, 'optimizer': 'adad', 'weight_decay': 2.4153036622088643e-05}
48 48 16
48 64 16
64 64 32
64 128 32
128 128 64
128 256 64
09/09 04:34:52 PM param size = 0.255930MB
Training size= 48000
09/09 04:34:53 PM WORKER: registered result for job (7, 0, 0) with dispatcher
09/09 04:34:53 PM job (7, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 117, in compute
    logging.info('epoch %d lr %e', epoch, lr_scheduler.get_lr()[0])
TypeError: step() missing 1 required positional argument: 'metrics'

09/09 04:34:53 PM WORKER: start processing job (8, 0, 0)
09/09 04:34:53 PM gpu device = cuda:0
09/09 04:34:53 PM config = {'drop_path_prob': 0.17477416436321894, 'grad_clip_value': 6, 'initial_lr': 0.0012742879066109918, 'lr_scheduler': 'Exponential', 'n_conv_layers': 3, 'optimizer': 'adad', 'weight_decay': 1.5011953979774981e-05}
48 48 16
48 64 32
64 128 64
09/09 04:34:53 PM param size = 0.118122MB
09/09 04:34:53 PM WORKER: registered result for job (8, 0, 0) with dispatcher
09/09 04:34:53 PM job (8, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 98, in compute
    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)
AttributeError: module 'torch.optim.lr_scheduler' has no attribute 'ExponnetialLR'

09/09 04:34:54 PM WORKER: start processing job (9, 0, 0)
09/09 04:34:54 PM gpu device = cuda:0
09/09 04:34:54 PM config = {'drop_path_prob': 0.10082702362972706, 'grad_clip_value': 4, 'initial_lr': 1.4318613643824058e-06, 'lr_scheduler': 'Plateau', 'n_conv_layers': 3, 'optimizer': 'sgd', 'weight_decay': 0.0002756579224944817, 'nesterov': 'False', 'sgd_momentum': 0.758508353382604}
48 48 16
48 64 32
64 128 64
09/09 04:34:54 PM param size = 0.118122MB
Training size= 48000
09/09 04:34:54 PM WORKER: registered result for job (9, 0, 0) with dispatcher
09/09 04:34:54 PM job (9, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 117, in compute
    logging.info('epoch %d lr %e', epoch, lr_scheduler.get_lr()[0])
TypeError: step() missing 1 required positional argument: 'metrics'

09/09 04:34:54 PM WORKER: start processing job (10, 0, 0)
09/09 04:34:54 PM gpu device = cuda:0
09/09 04:34:54 PM config = {'drop_path_prob': 0.20899517509206045, 'grad_clip_value': 7, 'initial_lr': 0.00020263525070597483, 'lr_scheduler': 'Exponential', 'n_conv_layers': 4, 'optimizer': 'adad', 'weight_decay': 0.00011920779838577043}
48 48 16
48 64 32
64 128 64
128 256 64
09/09 04:34:54 PM param size = 0.213546MB
09/09 04:34:54 PM WORKER: registered result for job (10, 0, 0) with dispatcher
09/09 04:34:54 PM job (10, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 98, in compute
    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)
AttributeError: module 'torch.optim.lr_scheduler' has no attribute 'ExponnetialLR'

09/09 04:34:54 PM WORKER: start processing job (11, 0, 0)
09/09 04:34:54 PM gpu device = cuda:0
09/09 04:34:54 PM config = {'drop_path_prob': 0.2333013017872667, 'grad_clip_value': 8, 'initial_lr': 0.014613606921945688, 'lr_scheduler': 'Plateau', 'n_conv_layers': 3, 'optimizer': 'adam', 'weight_decay': 3.1375927992817834e-05}
48 48 16
48 64 32
64 128 64
09/09 04:34:54 PM param size = 0.118122MB
Training size= 48000
09/09 04:34:54 PM WORKER: registered result for job (11, 0, 0) with dispatcher
09/09 04:34:54 PM job (11, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 117, in compute
    logging.info('epoch %d lr %e', epoch, lr_scheduler.get_lr()[0])
TypeError: step() missing 1 required positional argument: 'metrics'

09/09 04:34:54 PM WORKER: start processing job (12, 0, 0)
09/09 04:34:54 PM gpu device = cuda:0
09/09 04:34:54 PM config = {'drop_path_prob': 0.3051126040951638, 'grad_clip_value': 5, 'initial_lr': 0.0026138367001593645, 'lr_scheduler': 'Cosine', 'n_conv_layers': 3, 'optimizer': 'sgd', 'weight_decay': 2.524782307324342e-05, 'nesterov': 'False', 'sgd_momentum': 0.1399561937294214}
48 48 16
48 64 32
64 128 64
09/09 04:34:55 PM param size = 0.118122MB
Training size= 48000
09/09 04:34:55 PM epoch 0 lr 2.364238e-03
09/09 04:34:55 PM train 000 2.308738e+00 14.062500 43.750000
09/09 04:35:01 PM train 050 2.287937e+00 13.817402 56.158088
09/09 04:35:07 PM train 100 2.269773e+00 17.264851 60.860149
09/09 04:35:13 PM train 150 2.250281e+00 19.805464 65.066225
09/09 04:35:20 PM train 200 2.232362e+00 21.548507 67.933769
09/09 04:35:26 PM train 250 2.214957e+00 22.864791 69.721116
09/09 04:35:32 PM train 300 2.195939e+00 24.242110 71.553156
09/09 04:35:39 PM train 350 2.175015e+00 25.525285 73.344017
09/09 04:35:45 PM train 400 2.155531e+00 26.433915 74.762313
09/09 04:35:51 PM train 450 2.135639e+00 27.338553 75.980460
09/09 04:35:58 PM train 500 2.114250e+00 28.218563 77.136352
09/09 04:36:04 PM train 550 2.093623e+00 29.077813 78.065449
09/09 04:36:11 PM train 600 2.071704e+00 29.960483 78.915349
09/09 04:36:17 PM train 650 2.049449e+00 30.947581 79.706701
09/09 04:36:23 PM train 700 2.026529e+00 31.860735 80.458720
09/09 04:36:30 PM train_acc 32.827083
09/09 04:36:30 PM valid 000 1.498984e+00 54.687500 96.875000
09/09 04:36:31 PM valid 050 1.637431e+00 48.223039 90.563725
09/09 04:36:32 PM valid 100 1.632083e+00 48.143564 91.135520
09/09 04:36:33 PM valid 150 1.633879e+00 48.137417 91.142384
09/09 04:36:34 PM valid_acc 47.833333
09/09 04:36:34 PM epoch 1 lr 1.710778e-03
09/09 04:36:34 PM train 000 1.723300e+00 39.062500 87.500000
09/09 04:36:41 PM train 050 1.758466e+00 39.185049 87.408088
09/09 04:36:48 PM train 100 1.739979e+00 40.547649 87.670173
09/09 04:36:54 PM train 150 1.729526e+00 41.225166 87.965646
09/09 04:37:01 PM train 200 1.715949e+00 41.814366 88.036381
09/09 04:37:07 PM train 250 1.702254e+00 42.648157 88.259462
09/09 04:37:14 PM train 300 1.692174e+00 42.966154 88.190407
09/09 04:37:21 PM train 350 1.678946e+00 43.549679 88.376959
09/09 04:37:27 PM train 400 1.665803e+00 44.108479 88.474127
09/09 04:37:34 PM train 450 1.649717e+00 44.945261 88.691796
09/09 04:37:41 PM train 500 1.637272e+00 45.381113 88.716317
09/09 04:37:47 PM train 550 1.624256e+00 45.981738 88.849819
09/09 04:37:54 PM train 600 1.611271e+00 46.591618 88.979305
09/09 04:38:00 PM train 650 1.598849e+00 47.059812 89.136905
09/09 04:38:07 PM train 700 1.585480e+00 47.581580 89.296541
09/09 04:38:13 PM train_acc 48.020833
09/09 04:38:14 PM valid 000 1.245575e+00 65.625000 92.187500
09/09 04:38:15 PM valid 050 1.335303e+00 58.057598 92.371324
09/09 04:38:16 PM valid 100 1.339676e+00 57.626856 92.496906
09/09 04:38:17 PM valid 150 1.335694e+00 58.216060 92.653146
09/09 04:38:18 PM valid_acc 58.483333
09/09 04:38:18 PM epoch 2 lr 9.030584e-04
09/09 04:38:18 PM train 000 1.570615e+00 43.750000 90.625000
09/09 04:38:25 PM train 050 1.556387e+00 47.855392 88.082108
09/09 04:38:31 PM train 100 1.541465e+00 48.623144 88.551980
09/09 04:38:38 PM train 150 1.521093e+00 49.492964 88.990066
09/09 04:38:45 PM train 200 1.514116e+00 49.735697 89.148010
09/09 04:38:51 PM train 250 1.509170e+00 50.062251 89.361305
09/09 04:38:58 PM train 300 1.506751e+00 50.186877 89.415490
09/09 04:39:05 PM train 350 1.499844e+00 50.529736 89.516560
09/09 04:39:11 PM train 400 1.493776e+00 50.892300 89.537874
09/09 04:39:18 PM train 450 1.490273e+00 51.056680 89.533675
09/09 04:39:24 PM train 500 1.486827e+00 51.263099 89.514721
09/09 04:39:31 PM train 550 1.482246e+00 51.454741 89.578607
09/09 04:39:37 PM train 600 1.475875e+00 51.666493 89.694260
09/09 04:39:44 PM train 650 1.471083e+00 51.855319 89.784946
09/09 04:39:51 PM train 700 1.467007e+00 52.032810 89.856009
09/09 04:39:57 PM train_acc 52.333333
09/09 04:39:57 PM valid 000 1.162234e+00 68.750000 93.750000
09/09 04:39:58 PM valid 050 1.284002e+00 60.202206 93.229167
09/09 04:40:00 PM valid 100 1.286193e+00 60.024752 93.425124
09/09 04:40:01 PM valid 150 1.284643e+00 60.492550 93.501656
09/09 04:40:02 PM valid_acc 60.558333
09/09 04:40:02 PM epoch 3 lr 2.495992e-04
09/09 04:40:02 PM train 000 1.362891e+00 54.687500 89.062500
09/09 04:40:08 PM train 050 1.524630e+00 48.958333 88.664216
09/09 04:40:15 PM train 100 1.538271e+00 48.777847 88.366337
09/09 04:40:22 PM train 150 1.534678e+00 48.685844 88.576159
09/09 04:40:28 PM train 200 1.529101e+00 48.927239 88.634950
09/09 04:40:35 PM train 250 1.523533e+00 49.140936 88.794821
09/09 04:40:41 PM train 300 1.515130e+00 49.652201 88.911960
09/09 04:40:48 PM train 350 1.513435e+00 49.661681 88.920050
09/09 04:40:54 PM train 400 1.509457e+00 49.875312 88.965087
09/09 04:41:01 PM train 450 1.509232e+00 50.000000 88.916990
09/09 04:41:08 PM train 500 1.507772e+00 50.112275 88.856662
09/09 04:41:14 PM train 550 1.507362e+00 50.110594 88.844147
09/09 04:41:21 PM train 600 1.505833e+00 50.140391 88.968906
09/09 04:41:27 PM train 650 1.503622e+00 50.252016 88.997696
09/09 04:41:34 PM train 700 1.501717e+00 50.349947 89.013463
09/09 04:41:40 PM train_acc 50.518750
09/09 04:41:40 PM valid 000 1.436230e+00 53.125000 85.937500
09/09 04:41:42 PM valid 050 1.337702e+00 57.720588 93.903186
09/09 04:41:43 PM valid 100 1.338698e+00 57.874381 93.471535
09/09 04:41:44 PM valid 150 1.333504e+00 58.060844 93.718957
09/09 04:41:45 PM valid_acc 58.100000
09/09 04:41:45 PM epoch 4 lr 0.000000e+00
09/09 04:41:45 PM train 000 1.586859e+00 53.125000 89.062500
09/09 04:41:52 PM train 050 1.615166e+00 45.281863 87.254902
09/09 04:41:58 PM train 100 1.612644e+00 45.761139 87.004950
09/09 04:42:05 PM train 150 1.614826e+00 45.798841 86.651490
09/09 04:42:12 PM train 200 1.609844e+00 46.012127 86.823694
09/09 04:42:18 PM train 250 1.607119e+00 46.071962 86.970867
09/09 04:42:25 PM train 300 1.610991e+00 45.919850 86.913414
09/09 04:42:31 PM train 350 1.610988e+00 45.811075 86.805556
09/09 04:42:38 PM train 400 1.611915e+00 45.795667 86.786939
09/09 04:42:44 PM train 450 1.612776e+00 45.717849 86.817489
09/09 04:42:51 PM train 500 1.613829e+00 45.621257 86.848179
09/09 04:42:57 PM train 550 1.611627e+00 45.698162 86.876134
09/09 04:43:04 PM train 600 1.613138e+00 45.744072 86.769447
09/09 04:43:10 PM train 650 1.612775e+00 45.778130 86.803955
09/09 04:43:17 PM train 700 1.614629e+00 45.776123 86.773359
09/09 04:43:23 PM train_acc 45.739583
09/09 04:43:24 PM valid 000 1.445406e+00 48.437500 93.750000
09/09 04:43:25 PM valid 050 1.457231e+00 49.080882 92.892157
09/09 04:43:26 PM valid 100 1.466489e+00 49.319307 92.837252
09/09 04:43:27 PM valid 150 1.458094e+00 49.906871 92.973924
09/09 04:43:28 PM valid_acc 50.000000
09/09 04:43:28 PM WORKER: registered result for job (12, 0, 0) with dispatcher
09/09 04:43:28 PM WORKER: start processing job (13, 0, 0)
09/09 04:43:28 PM gpu device = cuda:0
09/09 04:43:28 PM config = {'drop_path_prob': 0.03085821168510905, 'grad_clip_value': 8, 'initial_lr': 0.001888251866465761, 'lr_scheduler': 'Plateau', 'n_conv_layers': 5, 'optimizer': 'adad', 'weight_decay': 5.7422871576394167e-05}
48 48 16
48 64 32
64 128 32
128 128 64
128 256 64
09/09 04:43:29 PM param size = 0.245898MB
Training size= 48000
09/09 04:43:29 PM WORKER: registered result for job (13, 0, 0) with dispatcher
09/09 04:43:29 PM job (13, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 117, in compute
    logging.info('epoch %d lr %e', epoch, lr_scheduler.get_lr()[0])
TypeError: step() missing 1 required positional argument: 'metrics'

09/09 04:43:29 PM WORKER: start processing job (14, 0, 0)
09/09 04:43:29 PM gpu device = cuda:0
09/09 04:43:29 PM config = {'drop_path_prob': 0.13930130505461288, 'grad_clip_value': 4, 'initial_lr': 2.0283129373620743e-05, 'lr_scheduler': 'Exponential', 'n_conv_layers': 3, 'optimizer': 'adam', 'weight_decay': 0.00012476442847934413}
48 48 16
48 64 32
64 128 64
09/09 04:43:29 PM param size = 0.118122MB
09/09 04:43:29 PM WORKER: registered result for job (14, 0, 0) with dispatcher
09/09 04:43:29 PM job (14, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 98, in compute
    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)
AttributeError: module 'torch.optim.lr_scheduler' has no attribute 'ExponnetialLR'

09/09 04:43:29 PM WORKER: start processing job (15, 0, 0)
09/09 04:43:30 PM gpu device = cuda:0
09/09 04:43:30 PM config = {'drop_path_prob': 0.037239827077541456, 'grad_clip_value': 8, 'initial_lr': 0.0010293661003821408, 'lr_scheduler': 'Plateau', 'n_conv_layers': 5, 'optimizer': 'sgd', 'weight_decay': 1.081657640628225e-05, 'nesterov': 'False', 'sgd_momentum': 0.6056437230209422}
48 48 16
48 64 32
64 128 32
128 128 64
128 256 64
09/09 04:43:30 PM param size = 0.245898MB
Training size= 48000
09/09 04:43:30 PM WORKER: registered result for job (15, 0, 0) with dispatcher
09/09 04:43:30 PM job (15, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 117, in compute
    logging.info('epoch %d lr %e', epoch, lr_scheduler.get_lr()[0])
TypeError: step() missing 1 required positional argument: 'metrics'

09/09 04:43:30 PM WORKER: start processing job (16, 0, 0)
09/09 04:43:30 PM gpu device = cuda:0
09/09 04:43:30 PM config = {'drop_path_prob': 0.001355106484079416, 'grad_clip_value': 4, 'initial_lr': 2.6219230261253077e-06, 'lr_scheduler': 'Exponential', 'n_conv_layers': 3, 'optimizer': 'adam', 'weight_decay': 9.647483903722504e-05}
48 48 16
48 64 32
64 128 64
09/09 04:43:30 PM param size = 0.118122MB
09/09 04:43:30 PM WORKER: registered result for job (16, 0, 0) with dispatcher
09/09 04:43:30 PM job (16, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 98, in compute
    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)
AttributeError: module 'torch.optim.lr_scheduler' has no attribute 'ExponnetialLR'

09/09 04:43:30 PM WORKER: start processing job (17, 0, 0)
09/09 04:43:30 PM gpu device = cuda:0
09/09 04:43:30 PM config = {'drop_path_prob': 0.2530211382290676, 'grad_clip_value': 8, 'initial_lr': 0.0021328619408078806, 'lr_scheduler': 'Plateau', 'n_conv_layers': 5, 'optimizer': 'adam', 'weight_decay': 2.767020083278173e-05}
48 48 16
48 64 32
64 128 32
128 128 64
128 256 64
09/09 04:43:30 PM param size = 0.245898MB
Training size= 48000
09/09 04:43:30 PM WORKER: registered result for job (17, 0, 0) with dispatcher
09/09 04:43:30 PM job (17, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 117, in compute
    logging.info('epoch %d lr %e', epoch, lr_scheduler.get_lr()[0])
TypeError: step() missing 1 required positional argument: 'metrics'

09/09 04:43:30 PM WORKER: start processing job (18, 0, 0)
09/09 04:43:30 PM gpu device = cuda:0
09/09 04:43:30 PM config = {'drop_path_prob': 0.309615240756197, 'grad_clip_value': 6, 'initial_lr': 0.02416761349801617, 'lr_scheduler': 'Exponential', 'n_conv_layers': 4, 'optimizer': 'sgd', 'weight_decay': 1.2782094181829203e-05, 'nesterov': 'False', 'sgd_momentum': 0.7502913421421702}
48 48 16
48 64 32
64 128 64
128 256 64
09/09 04:43:31 PM param size = 0.213546MB
09/09 04:43:31 PM WORKER: registered result for job (18, 0, 0) with dispatcher
09/09 04:43:31 PM job (18, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 98, in compute
    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)
AttributeError: module 'torch.optim.lr_scheduler' has no attribute 'ExponnetialLR'

09/09 04:43:31 PM WORKER: start processing job (19, 0, 0)
09/09 04:43:31 PM gpu device = cuda:0
09/09 04:43:31 PM config = {'drop_path_prob': 0.05240146673210697, 'grad_clip_value': 7, 'initial_lr': 0.007729681423331825, 'lr_scheduler': 'Exponential', 'n_conv_layers': 4, 'optimizer': 'adad', 'weight_decay': 1.9529174066872524e-05}
48 48 16
48 64 32
64 128 64
128 256 64
09/09 04:43:31 PM param size = 0.213546MB
09/09 04:43:31 PM WORKER: registered result for job (19, 0, 0) with dispatcher
09/09 04:43:31 PM job (19, 0, 0) failed with exception
Traceback (most recent call last):
  File "/home/rkohli/anaconda3/lib/python3.7/site-packages/hpbandster/core/worker.py", line 206, in start_computation
    result = {'result': self.compute(*args, config_id=id, **kwargs),
  File "darts-bohb.py", line 98, in compute
    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)
AttributeError: module 'torch.optim.lr_scheduler' has no attribute 'ExponnetialLR'

09/09 04:43:31 PM DISPATCHER: Dispatcher shutting down
09/09 04:43:31 PM DISPATCHER: shut down complete
Best run id:(1, 0, 0), 
 Config:{'drop_path_prob': 0.1845860078956414, 'grad_clip_value': 5, 'initial_lr': 0.005120593129238132, 'lr_scheduler': 'Cosine', 'n_conv_layers': 3, 'optimizer': 'sgd', 'weight_decay': 4.241376263065748e-05, 'nesterov': 'False', 'sgd_momentum': 0.9352732812800111}
